.. slide::
RÃ©sumÃ© des concepts clÃ©s du chapitre 5
================

.. slide::

ğŸ“– 1. MLP vs CNN : pourquoi les convolutions ?
-----------------

**ProblÃ¨mes des MLP pour les images** :

- Trop de paramÃ¨tres (77M pour une image $$224Ã—224$$ RGB)
- Perte de structure spatiale lors de l'aplatissement
- Pas d'invariance par translation

**Avantages des CNN** :

- **Partage de poids** : mÃªme filtre appliquÃ© partout â†’ rÃ©duction drastique des paramÃ¨tres
- **Invariance par translation** : dÃ©tecte les motifs quelle que soit leur position
- **PrÃ©servation de la structure spatiale** : traite les rÃ©gions locales

**Les filtres de convolution** :

- Petites matrices apprenables ($$3Ã—3$$, $$5Ã—5$$, $$7Ã—7$$)
- Apprennent automatiquement : contours, formes, objets complexes, etc.

.. slide::

ğŸ“– 2. Couches de convolution
-------------------

**ParamÃ¨tres clÃ©s de ``conv2d``** :

- ``in_channels`` : nombre de canaux en entrÃ©e (3 pour RGB)
- ``out_channels`` : nombre de filtres Ã  apprendre
- ``kernel_size`` : taille du filtre (3Ã—3, 5Ã—5, etc.)
- ``stride`` : pas de dÃ©placement (1 par dÃ©faut)
- ``padding`` : zÃ©ros ajoutÃ©s autour (prÃ©serve la taille si =1)

**Calcul de la taille de sortie** : $$H_{out} = \left\lfloor \frac{H_{in} + 2 \times \text{padding} - \text{kernel_size}}{\text{stride}} \right\rfloor + 1$$

**Le padding** : essentiel pour ne pas perdre d'information sur les bords.

.. slide::

ğŸ“– 3. Pooling
-------------------

**Max Pooling** (le plus utilisÃ©) :

- Prend le maximum dans chaque rÃ©gion (kernel $$2Ã—2$$ typiquement)
- Divise la taille spatiale par 2

**Avantages** :

- Diminue le nombre de paramÃ¨tres et le temps de calcul
- Apporte une invariance aux petites translations
- Augmente le champ rÃ©ceptif

.. slide::

ğŸ“– 4. Mini-batchs
-------------------

**Trois approches** :

1. **Batch Gradient Descent** : tout le dataset (lent mais stable)
2. **SGD** : un exemple Ã  la fois (rapide mais bruitÃ©)
3. **Mini-Batch** : compromis idÃ©al (32 ou 64 exemples) âœ“

**Avantages** : exploite le GPU, estime bien le gradient, rÃ©gularisation naturelle.

.. slide::

ğŸ“– 5. Datasets et DataLoaders
-------------------

**Dataset** : classe pour organiser vos donnÃ©es

- Doit implÃ©menter ``__len__`` et ``__getitem__``
- Peut charger des images depuis le disque et appliquer des transformations si nÃ©cessaire.

**DataLoader** : automatise le chargement

- DÃ©coupage en mini-batchs
- MÃ©lange des donnÃ©es (``shuffle=True`` pour train, ``False`` pour val/test)
- Chargement parallÃ¨le (``num_workers``)

âœ“ **Bonnes pratiques** : Toujours utiliser ``Dataset`` et ``DataLoader`` pour gÃ©rer les donnÃ©es

.. slide::

ğŸ“– 6. Train/Val/Test
-------------------

**Proportions recommandÃ©es** :

- **Train** (70-80%) : entraÃ®nement du modÃ¨le
- **Validation** (10-15%) : surveillance et sÃ©lection du meilleur modÃ¨le pendant l'entraÃ®nement
- **Test** (10-15%) : Ã©valuation finale uniquement

âš ï¸ **RÃ¨gle d'or** : Ne JAMAIS utiliser le test set pendant l'entraÃ®nement !

âœ“ **Bonnes pratiques** : Utiliser ``random_split`` pour diviser automatiquement et toujours sÃ©parer les trois ensembles

.. slide::

ğŸ“– 7. Transformations d'images
-------------------

**PrÃ©traitement (toujours nÃ©cessaire)** :

- ``ToTensor()`` : convertit en tenseur PyTorch
- ``Normalize(mean, std)`` : centre les valeurs autour de 0

**Augmentation (train uniquement)** :

- ``RandomHorizontalFlip()`` : retourne horizontalement
- ``RandomRotation()`` : rotation alÃ©atoire
- ``ColorJitter()`` : modifie luminositÃ©/contraste

ğŸ’¡ **Pourquoi pas d'augmentation pour val/test ?** On veut Ã©valuer sur les vraies images.

âœ“ **Bonnes pratiques** : Augmentation uniquement pour l'entraÃ®nement, jamais pour validation/test

.. slide::

ğŸ“– 8. Sauvegarde de modÃ¨les
-------------------

**Trois mÃ©thodes** :

1. **Tout le modÃ¨le** : ``torch.save(model, 'model.pth')`` (Ã©viter si possible)

2. **Poids uniquement** (recommandÃ© âœ“) : ``torch.save(model.state_dict(), 'weights.pth')``

3. **Ã‰tat complet** (pour reprendre l'entraÃ®nement) :

   - Sauvegarde : epoch, model_state_dict, optimizer_state_dict, loss, mÃ©triques
   - Permet de reprendre exactement oÃ¹ on s'est arrÃªtÃ©

âœ“ **Bonnes pratiques** : PrÃ©fÃ©rer ``state_dict()`` au modÃ¨le complet, sauvegarder le meilleur modÃ¨le basÃ© sur validation loss, inclure epoch et optimizer dans les checkpoints






