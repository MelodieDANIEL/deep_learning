.. slide::

Chapitre 5 ‚Äî Classification d'images avec CNN
================

üéØ Objectifs du Chapitre
----------------------


.. important::

   √Ä la fin de ce chapitre, vous saurez : 

   - Comprendre la diff√©rence entre un MLP et les r√©seaux convolutifs (CNN).
   - Utiliser les couches de convolution pour le traitement d'images.
   - Appliquer les techniques de pooling pour r√©duire la dimensionnalit√©.
   - G√©rer les mini-batchs pour un entra√Ænement efficace.
   - Sauvegarder et charger les poids d'un mod√®le entra√Æn√©.
   - Utiliser les datasets PyTorch pour organiser vos donn√©es.

.. slide::

üìñ 1. MLP vs Convolutions : pourquoi les CNN ?
----------------------

Dans les chapitres pr√©c√©dents, nous avons utilis√© des perceptrons multi-couches (MLP) pour r√©soudre divers probl√®mes. Cependant, lorsqu'on travaille avec des images, les MLP pr√©sentent plusieurs limitations importantes.

1.1. Limitations des MLP pour les images
~~~~~~~~~~~~~~~~~~~

Imaginons une image en couleur de taille $$224√ó224$$ pixels. Si on "aplatit" (avec ``flatten`` ou ``view`` par exemple) cette image pour la donner √† un MLP :

- Chaque pixel RGB ‚Üí 3 valeurs
- Total d'entr√©es : $$224 \times 224 \times 3 = 150528$$ valeurs

Si la premi√®re couche cach√©e a 512 neurones :

- Nombre de poids : $$150528 \times 512 = 77070336$$ param√®tres

**Probl√®mes** :

1. **Trop de param√®tres** : le mod√®le devient √©norme, difficile √† entra√Æner et tr√®s gourmand en m√©moire.
2. **Perte de structure spatiale** : en aplatissant l'image, on perd l'information sur la proximit√© des pixels. Or, dans une image, les pixels voisins sont fortement corr√©l√©s.
3. **Pas de g√©n√©ralisation spatiale** : un MLP doit r√©apprendre le m√™me motif s'il appara√Æt √† des positions diff√©rentes dans l'image.

.. slide::

1.2. Solution : les r√©seaux convolutifs (CNN)
~~~~~~~~~~~~~~~~~~~

Les r√©seaux de neurones convolutifs (CNN, de Convolutional Neural Networks en anglais) r√©solvent ces probl√®mes en utilisant des convolutions au lieu de couches enti√®rement connect√©es.

.. slide::

1.3. Qu'est-ce qu'un filtre (ou noyau de convolution) ?
~~~~~~~~~~~~~~~~~~~

Un filtre (aussi appel√© *kernel* ou *noyau*) est une petite matrice de poids apprenables qui sert √† d√©tecter des motifs dans l'image.

- **Taille typique** : $$3√ó3$$, $$5√ó5$$, ou $$7√ó7$$ pixels
- **Fonctionnement** : le filtre "glisse" sur toute l'image (comme un tampon qu'on d√©placerait)
- **D√©tection** : √† chaque position, il calcule une somme pond√©r√©e des pixels qu'il couvre
- **Apprentissage** : les poids du filtre sont appris automatiquement pendant l'entra√Ænement

üí° **Intuition** : imaginez que vous cherchez des visages dans une photo. Vos yeux scannent l'image en cherchant des motifs caract√©ristiques (deux yeux, un nez, une bouche). Les filtres font exactement la m√™me chose, mais de mani√®re automatique et sur des milliers de motifs diff√©rents !

.. slide::

1.4. √Ä quoi servent les filtres ?
~~~~~~~~~~~~~~~~~~~

Chaque filtre est sp√©cialis√© dans la d√©tection d'un type de motif :

- **Contours** : verticaux, horizontaux, diagonaux
- **Textures** : lignes, points, motifs r√©p√©t√©s
- **Formes** : coins, courbes, angles
- **Caract√©ristiques complexes** : yeux, roues, fen√™tres (dans les couches profondes)

Les filtres s'organisent de mani√®re hi√©rarchique :

- **Premi√®res couches** : d√©tectent des caract√©ristiques simples (bords, couleurs)
- **Couches interm√©diaires** : combinent ces caract√©ristiques pour d√©tecter des formes
- **Couches profondes** : d√©tectent des objets complexes (visages, voitures, animaux)

.. slide::

1.5. Qu'est-ce qui d√©termine quel filtre fait quoi ?
~~~~~~~~~~~~~~~~~~~

C'est l'entra√Ænement qui d√©termine la sp√©cialisation de chaque filtre ! Voici comment :

1. **Initialisation al√©atoire** : au d√©part, les poids des filtres sont initialis√©s al√©atoirement (petites valeurs proches de 0).

2. **Apprentissage automatique** : pendant l'entra√Ænement, l'algorithme de descente de gradient ajuste progressivement les poids de chaque filtre pour minimiser l'erreur du r√©seau.

3. **Sp√©cialisation √©mergente** : chaque filtre "apprend" naturellement √† d√©tecter les motifs les plus utiles pour la t√¢che. Par exemple :
   
   - Si le r√©seau doit reconna√Ætre des chats, certains filtres apprendront √† d√©tecter des oreilles pointues
   - Si c'est pour des voitures, d'autres d√©tecteront des roues ou des phares

4. **Pas de programmation manuelle** : on ne dit jamais explicitement √† un filtre "tu dois d√©tecter les contours verticaux". C'est le r√©seau qui d√©couvre lui-m√™me quels motifs sont importants !

üí° **Analogie** : c'est comme apprendre √† reconna√Ætre des champignons comestibles. Au d√©but, vous ne savez pas quoi regarder. Apr√®s avoir vu des centaines d'exemples, votre cerveau apprend automatiquement √† rep√©rer les indices pertinents (couleur du chapeau, forme du pied, pr√©sence d'un anneau, etc.). Les filtres font exactement pareil !

.. slide::

1.6. Avantages des convolutions
~~~~~~~~~~~~~~~~~~~

1. **Partage de poids** : le m√™me filtre est appliqu√© sur toute l'image, r√©duisant drastiquement le nombre de param√®tres.
2. **Invariance par translation** : un motif appris √† un endroit peut √™tre d√©tect√© ailleurs dans l'image (un visage reste un visage, qu'il soit en haut √† gauche ou en bas √† droite).
3. **Pr√©servation de la structure spatiale** : les convolutions traitent des r√©gions locales, pr√©servant les relations entre pixels voisins.

**Exemple de gain en param√®tres** :

- Un filtre $$3√ó3$$ sur une image RGB ‚Üí $$3 \times 3 \times 3 = 27$$ poids par filtre
- Avec 64 filtres diff√©rents ‚Üí $$64 \times 27 = 1728$$ param√®tres au total

Compar√© aux 77 millions de param√®tres du MLP, c'est une r√©duction spectaculaire !

.. slide::

üìñ 2. Les couches de convolution dans PyTorch
----------------------

Comme nous l'avons vu au chapitre 4, une convolution 2D applique un filtre sur une image en le faisant glisser sur toute la surface. PyTorch fournit ``nn.Conv2d`` pour cr√©er ces couches convolutives.

2.1. Syntaxe de base
~~~~~~~~~~~~~~~~~~~

.. code-block:: python

   import torch
   import torch.nn as nn

   # Cr√©er une couche de convolution
   # Note : stride=1 et padding=0 sont les valeurs par d√©faut, on ne les √©crit que si on veut une autre valeur
   conv = nn.Conv2d(
       in_channels=3,      # nombre de canaux en entr√©e (1 pour niveaux de gris, 3 pour RGB, 4 pour RGBA)
       out_channels=64,    # nombre de filtres √† apprendre (64 d√©tecteurs de motifs diff√©rents)
       kernel_size=3,      # taille du filtre 3√ó3 pixels (valeurs courantes : 3, 5, 7, etc.)
       stride=1,           # pas de d√©placement du filtre (un stride de 1 d√©place d'1 pixel √† chaque fois et un stride de 2 divise la taille spatiale par 2)
       padding=1           # ajoute 1 pixel de z√©ros autour de l'image pour conserver la taille spatiale
   )

   # Exemple d'utilisation
   x = torch.randn(1, 3, 224, 224)  # batch_size=1, canaux=3, Height=224, Width=224
   y = conv(x)
   print(y.shape)  # torch.Size([1, 64, 224, 224])

.. slide::

2.2. Calcul de la taille de sortie
~~~~~~~~~~~~~~~~~~~

.. math::

   H_{out} = \left\lfloor \frac{H_{in} + 2 \times \text{padding} - \text{kernel_size}}{\text{stride}} \right\rfloor + 1

**Exemple avec padding=1, kernel_size=3, stride=1 sur une image 224√ó224** :

.. math::

   H_{out} = \left\lfloor \frac{224 + 2 - 3}{1} \right\rfloor + 1 = 224

La taille spatiale est pr√©serv√©e.

.. slide::

2.3 Padding
~~~~~~~~~~~~~

La visualisation ci-dessous montre ce qui se passe avec et sans padding :

.. image:: images/convolution_padding_explanation.png
   :width: 100%
   :align: center
   :alt: Explication visuelle du padding dans les convolutions

.. slide::

Avec un filtre $$3√ó3$$ et **padding=0** : le filtre ne peut pas se centrer sur les pixels des bords (il d√©borderait de l'image). C'est √† dire que le filtre ne peut se centrer ni sur toute la ligne du haut, ni sur toute la ligne du bas, ni sur toute la colonne de gauche, ni sur toute la colonne de droite.

**Exemple concret : image $$5√ó5$$ avec filtre $$3√ó3$$**

Pour comprendre ce qui se passe, regardons le filtre $$3√ó3$$ qui doit se centrer sur chaque pixel :

.. code-block:: text

   Image 5√ó5 :
   ‚îå‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îê
   ‚îÇ X ‚îÇ X ‚îÇ X ‚îÇ X ‚îÇ X ‚îÇ  ‚Üê Ligne du haut : impossible (5 pixels)
   ‚îú‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚î§
   ‚îÇ X ‚îÇ ‚úì ‚îÇ ‚úì ‚îÇ ‚úì ‚îÇ X ‚îÇ  ‚Üê Ligne 2 : coins impossibles, centre OK
   ‚îú‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚î§
   ‚îÇ X ‚îÇ ‚úì ‚îÇ ‚úì ‚îÇ ‚úì ‚îÇ X ‚îÇ  ‚Üê Ligne 3 : coins impossibles, centre OK
   ‚îú‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚î§
   ‚îÇ X ‚îÇ ‚úì ‚îÇ ‚úì ‚îÇ ‚úì ‚îÇ X ‚îÇ  ‚Üê Ligne 4 : coins impossibles, centre OK
   ‚îú‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚î§
   ‚îÇ X ‚îÇ X ‚îÇ X ‚îÇ X ‚îÇ X ‚îÇ  ‚Üê Ligne du bas : impossible (5 pixels)
   ‚îî‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îò
     ‚Üë               ‚Üë
   Colonne         Colonne
   gauche          droite
   impossible      impossible
   (5 pixels)      (5 pixels)

**‚ö†Ô∏è Attention √† ne pas confondre deux choses diff√©rentes :**

1. **Nombre de positions impossibles** = 16 pixels (toute la bordure marqu√©e X)
   
   - Ligne du haut : 5 pixels
   - Ligne du bas : 5 pixels
   - Colonne gauche : 5 pixels (dont 2 d√©j√† compt√©s dans les lignes)
   - Colonne droite : 5 pixels (dont 2 d√©j√† compt√©s dans les lignes)
   - **Total : 5 + 5 + 3 + 3 = 16 positions impossibles**

2. **R√©duction des dimensions** = passer de $$5√ó5$$ √† $$3√ó3$$
   
   - **En hauteur** : ligne du haut + ligne du bas impossibles ‚Üí **on perd 2 lignes**
   - **En largeur** : colonne gauche + colonne droite impossibles ‚Üí **on perd 2 colonnes**
   - **R√©sultat** : $$5√ó5$$ devient $$3√ó3$$ (image de sortie r√©duite)

**üí° En r√©sum√©** : on a 16 pixels de bordure o√π le filtre ne peut pas se positionner, mais cela se traduit par une **r√©duction de 2 en hauteur** (5‚Üí3) et **2 en largeur** (5‚Üí3), pas par une r√©duction de 16 pixels au total !

.. slide::
**Formule g√©n√©rale** : 

Pour un filtre de taille $$k√ók$$, on perd :

- **En hauteur** : (kernel_size - 1) lignes au total
  
  - $$\frac{kernel\_size - 1}{2}$$ lignes en haut
  - $$\frac{kernel\_size - 1}{2}$$ lignes en bas

- **En largeur** : (kernel_size - 1) colonnes au total
  
  - $$\frac{kernel\_size - 1}{2}$$ colonnes √† gauche
  - $$\frac{kernel\_size - 1}{2}$$ colonnes √† droite

**Exemples d√©taill√©s** :

1. **Filtre $$3√ó3$$ sur image $$5√ó5$$** :
   
   - Perte : (3-1) = 2 en hauteur, (3-1) = 2 en largeur
   - D√©tail : 1 ligne en haut + 1 ligne en bas, 1 colonne √† gauche + 1 colonne √† droite
   - **R√©sultat** : $$5√ó5$$ ‚Üí $$3√ó3$$
   - Calcul : hauteur = 5 - 2 = 3, largeur = 5 - 2 = 3

2. **Filtre $$5√ó5$$ sur image $$7√ó7$$** :
   
   - Perte : (5-1) = 4 en hauteur, (5-1) = 4 en largeur
   - D√©tail : 2 lignes en haut + 2 lignes en bas, 2 colonnes √† gauche + 2 colonnes √† droite
   - **R√©sultat** : $$7√ó7$$ ‚Üí $$3√ó3$$
   - Calcul : hauteur = 7 - 4 = 3, largeur = 7 - 4 = 3

3. **Filtre $$5√ó5$$ sur image $$5√ó5$$** :
   
   - Perte : (5-1) = 4 en hauteur, (5-1) = 4 en largeur
   - D√©tail : 2 lignes en haut + 2 lignes en bas, 2 colonnes √† gauche + 2 colonnes √† droite
   - **R√©sultat** : $$5√ó5$$ ‚Üí $$1√ó1$$ (un seul pixel central valide !)
   - Calcul : hauteur = 5 - 4 = 1, largeur = 5 - 4 = 1

.. warning::
   **üí° R√®gle simple** : Dimensions de sortie = Dimensions d'entr√©e - (kernel_size - 1)

   **Solution pour ne rien perdre** : le padding qui ajoute des z√©ros autour pour que le filtre puisse se centrer partout !


.. slide::

üìñ 3. Pooling : r√©duire la dimensionnalit√©
----------------------

Les couches de pooling permettent de r√©duire progressivement la taille spatiale des repr√©sentations, ce qui :

- **Diminue le nombre de param√®tres et le temps de calcul** : en r√©duisant la taille spatiale (par exemple de 224√ó224 √† 112√ó112), on divise par 4 le nombre de valeurs √† traiter dans les couches suivantes ce qui implique moins de param√®tres et un entra√Ænement plus rapide.
- **Apporte une invariance aux petites translations** : si un motif (par exemple un ≈ìil) se d√©place l√©g√®rement dans l'image (de quelques pixels), le max pooling va quand m√™me d√©tecter la m√™me valeur maximale dans la r√©gion. Cela rend le r√©seau plus robuste aux petits d√©placements des objets
- **Augmente le champ r√©ceptif** : apr√®s un pooling, chaque neurone "voit" une r√©gion plus grande de l'image d'origine, ce qui lui permet de capturer des motifs plus globaux

3.1. Max Pooling
~~~~~~~~~~~~~~~~~~~

Le max pooling prend le maximum dans chaque r√©gion. C'est le type de pooling le plus utilis√© car il pr√©serve mieux les caract√©ristiques importantes (contours, textures).

.. code-block:: python

   import torch.nn.functional as F

   # Exemple : matrice 4√ó4
   x = torch.tensor([[[[1., 2., 3., 4.],
                       [5., 6., 7., 8.],
                       [9., 10., 11., 12.],
                       [13., 14., 15., 16.]]]])  # [batch=1, canaux=1, height=4, width=4]

   # Max pooling avec kernel 2√ó2 et stride 2
   # kernel_size=2 : on regarde des fen√™tres de 2√ó2 pixels
   # stride=2 : on d√©place la fen√™tre de 2 pixels √† chaque fois (pas de chevauchement)
   y = F.max_pool2d(x, kernel_size=2, stride=2)
   print(y)
   # tensor([[[[ 6.,  8.],
   #           [14., 16.]]]])  # [1, 1, 2, 2] - taille divis√©e par 2

**Explication d√©taill√©e** : 

Le max pooling divise l'image en r√©gions de $$2√ó2$$ pixels et garde seulement le maximum de chaque r√©gion.

**Visualisation des 4 r√©gions** :

.. code-block:: text

   Image d'origine 4√ó4 :
   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îê
   ‚îÇ 1  ‚îÇ 2  ‚îÇ 3  ‚îÇ 4  ‚îÇ
   ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚î§
   ‚îÇ 5  ‚îÇ 6  ‚îÇ 7  ‚îÇ 8  ‚îÇ
   ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚î§
   ‚îÇ 9  ‚îÇ 10 ‚îÇ 11 ‚îÇ 12 ‚îÇ
   ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚î§
   ‚îÇ 13 ‚îÇ 14 ‚îÇ 15 ‚îÇ 16 ‚îÇ
   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îò

   D√©coupage en 4 r√©gions 2√ó2 :
   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
   ‚îÇ  1   2  ‚îÇ  3   4  ‚îÇ  ‚Üê r√©gion 1 (haut-gauche) : max([1,2,5,6]) = 6
   ‚îÇ  5   6  ‚îÇ  7   8  ‚îÇ  ‚Üê r√©gion 2 (haut-droite) : max([3,4,7,8]) = 8
   ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
   ‚îÇ  9  10  ‚îÇ 11  12  ‚îÇ  ‚Üê r√©gion 3 (bas-gauche) : max([9,10,13,14]) = 14
   ‚îÇ 13  14  ‚îÇ 15  16  ‚îÇ  ‚Üê r√©gion 4 (bas-droite) : max([11,12,15,16]) = 16
   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

   R√©sultat apr√®s max pooling 2√ó2 :
   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îê
   ‚îÇ 6  ‚îÇ 8  ‚îÇ
   ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚î§
   ‚îÇ 14 ‚îÇ 16 ‚îÇ
   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îò
   
   Taille : 4√ó4 ‚Üí 2√ó2 (divis√©e par 2 en hauteur et en largeur)

.. slide::

3.2. Average Pooling
~~~~~~~~~~~~~~~~~~~

L'average pooling calcule la moyenne de chaque r√©gion.

.. code-block:: python

   y = F.avg_pool2d(x, kernel_size=2, stride=2)
   print(y)
   # tensor([[[[ 3.5,  5.5],
   #           [11.5, 13.5]]]])

**Explication** :

- [1,2,5,6] ‚Üí (1+2+5+6)/4 = 3.5
- [3,4,7,8] ‚Üí 5.5
- etc.

.. slide::

3.3. Exemple d'un CNN qui peut-√™tre utilis√© pour la classification d'images
~~~~~~~~~~~~~~~~~~~

Maintenant que nous avons vu les convolutions et le pooling, voici un exemple complet de CNN pour la classification d'images RGB de taille $$224√ó224$$ pixels en 10 classes :

.. code-block:: python

   class CNNWithPooling(nn.Module):
       def __init__(self, num_classes=10):
           super(CNNWithPooling, self).__init__()
           
           # Premi√®re couche convolutive : 3 canaux ‚Üí 32 filtres
           self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)
           
           # Deuxi√®me couche convolutive : 32 canaux ‚Üí 64 filtres
           self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)
           
           # Couches fully-connected pour la classification
           self.fc1 = nn.Linear(64 * 56 * 56, 128)
           self.fc2 = nn.Linear(128, num_classes)
       
       def forward(self, x):
           # x: [batch_size, 3, 224, 224] - image RGB d'entr√©e
           
           # Bloc 1 : Convolution + ReLU + Max Pooling
           x = F.relu(self.conv1(x))           # [batch, 32, 224, 224] - applique 32 filtres
           x = F.max_pool2d(x, kernel_size=2)  # [batch, 32, 112, 112] - divise la taille par 2
           # Quand on ne pr√©cise pas le stride, PyTorch utilise par d√©faut la m√™me valeur que kernel_size, donc ici stride=2 √©galement.
           
           # Bloc 2 : Convolution + ReLU + Max Pooling
           x = F.relu(self.conv2(x))           # [batch, 64, 112, 112] - applique 64 filtres
           x = F.max_pool2d(x, kernel_size=2)  # [batch, 64, 56, 56] - divise encore par 2
           
           # Aplatir les features maps pour les couches fully-connected
           # Note : on peut utiliser view() plut√¥t que flatten() pour plus de contr√¥le
           # - x.view(x.size(0), -1) : pr√©serve la dimension du batch, aplatit le reste
           # - x.flatten(1) : √©quivalent mais moins explicite (le 1 signifie "√† partir de la dimension 1")
           # - Le -1 signifie "calcule automatiquement cette dimension"
           x = x.view(x.size(0), -1)       # [batch, 64*56*56] = [batch, 200704]
           
           # Classification avec couches fully-connected
           x = F.relu(self.fc1(x))         # [batch, 128]
           x = self.fc2(x)                 # [batch, num_classes] - scores pour chaque classe
           
           return x

.. slide::

3.4. Cr√©ation et test du mod√®le
~~~~~~~~~~~~~~~~~

.. code-block:: python

   # Cr√©er et tester le mod√®le
   model = CNNWithPooling(num_classes=10)
   
   # Afficher l'architecture
   print(model)
   
   # Test avec un batch d'images
   x = torch.randn(4, 3, 224, 224)  # batch de 4 images RGB 224√ó224
   output = model(x)
   print(f"Input shape: {x.shape}")
   print(f"Output shape: {output.shape}")  # torch.Size([4, 10])

.. warning::

   ‚ö†Ô∏è **Adaptation n√©cessaire selon vos donn√©es**
   
   Ce mod√®le est con√ßu pour des **images RGB de taille 224√ó224 pixels**. 
   
   Si vos images ont une **taille diff√©rente**, vous devez adapter la premi√®re couche fully-connected :
   
   - Utilisez la formule : ``nn.Linear(nombre_de_filtres * (H_final) * (W_final), ...)``
   - O√π ``H_final`` et ``W_final`` sont les dimensions spatiales apr√®s toutes les convolutions et poolings
   
   üí° **Astuce** : Pour conna√Ætre la taille exacte, ajoutez ``print(x.shape)`` juste avant ``x.view()`` dans la m√©thode ``forward()``.

.. slide::

üìñ 4. Mini-batchs : entra√Ænement efficace
----------------------

L'entra√Ænement par mini-batchs est une technique fondamentale en deep learning qui combine les avantages de deux approches extr√™mes.

4.1. Trois approches d'entra√Ænement
~~~~~~~~~~~~~~~~~~~

**1. Batch Gradient Descent (tout le dataset)** :

- Calcule le gradient sur toutes les donn√©es
- Mise √† jour stable mais tr√®s lente
- N√©cessite beaucoup de m√©moire

**2. Stochastic Gradient Descent (SGD, un exemple √† la fois)** :

- Calcule le gradient sur un seul exemple
- Tr√®s rapide mais gradient bruit√©
- Converge de mani√®re erratique

**3. Mini-Batch Gradient Descent** :

- Calcule le gradient sur un petit groupe d'exemples (typiquement 32, 64, 128)
- **Compromis id√©al** : rapide et gradient raisonnablement stable
- Exploite efficacement le parall√©lisme du GPU

.. slide::

4.2. Pourquoi les mini-batchs ?
~~~~~~~~~~~~~~~~~~~

**Avantages** :

1. **Efficacit√© GPU** : les GPUs sont optimis√©s pour traiter plusieurs donn√©es en parall√®le
2. **Estimation du gradient** : le gradient calcul√© sur un mini-batch est une bonne approximation du gradient sur tout le dataset
3. **R√©gularisation** : le bruit dans les mini-batchs peut aider √† √©viter les minima locaux
4. **Gestion m√©moire** : on ne charge qu'une partie du dataset en m√©moire √† la fois

**Choix de la taille** :

- Petits batchs (16-32) : gradient plus bruit√©, convergence plus exploratrice
- Grands batchs (128-256) : gradient plus stable, convergence plus directe
- Compromis courant : 32 ou 64

.. slide::

4.3. Mini-batchs dans PyTorch
~~~~~~~~~~~~~~~~~~~

En PyTorch, tous les tenseurs ont une dimension de batch en premi√®re position :

.. code-block:: python

   # Format attendu : [batch_size, channels, height, width]
   images = torch.randn(32, 3, 224, 224)  # batch de 32 images RGB 224√ó224

   # Les op√©rations sont automatiquement appliqu√©es sur tout le batch
   # Exemple : Convolution SANS padding (padding=0 par d√©faut)
   conv = nn.Conv2d(3, 64, kernel_size=3)
   output = conv(images)  # [32, 64, 222, 222] -> la taille diminue !

**Exemple d'entra√Ænement avec mini-batchs** :

.. code-block:: python

   # Supposons qu'on a des donn√©es et un mod√®le
   model = CNNWithPooling()
   optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
   criterion = nn.CrossEntropyLoss()

   # Donn√©es factices
   images = torch.randn(100, 3, 224, 224) # dataset de 100 images
   labels = torch.randint(0, 10, (100,))

   # Param√®tres
   batch_size = 32
   num_batches = len(images) // batch_size

   # Entra√Ænement par mini-batchs
   for epoch in range(5): # 5 √©poques
       for i in range(num_batches):
           # Extraire un mini-batch d'images et labels dans en suivant l'ordre du dataset
           # Attention en pratique on tire les mini-batchs de mani√®re al√©atoire
           start_idx = i * batch_size 
           end_idx = start_idx + batch_size
           
           batch_images = images[start_idx:end_idx]
           batch_labels = labels[start_idx:end_idx]
           
           # Forward pass
           outputs = model(batch_images)
           loss = criterion(outputs, batch_labels)
           
           # Backward pass et optimisation
           optimizer.zero_grad()
           loss.backward()
           optimizer.step()
       
       print(f"Epoch {epoch+1}, Loss: {loss.item():.4f}")

.. slide::

üìñ 5. Datasets et DataLoaders PyTorch
----------------------

G√©rer manuellement les mini-batchs comme ci-dessus devient rapidement fastidieux. PyTorch fournit ``Dataset`` et ``DataLoader`` pour automatiser ce processus.

5.1. La classe Dataset
~~~~~~~~~~~~~~~~~~~

``Dataset`` est une classe abstraite qui repr√©sente votre jeu de donn√©es. Il existe deux approches :

**Approche 1 : Utiliser TensorDataset (recommand√© pour des tenseurs simples)**

Si vos donn√©es sont d√©j√† sous forme de tenseurs PyTorch, utilisez directement ``TensorDataset`` :

.. code-block:: python

   from torch.utils.data import TensorDataset

   # Cr√©er des donn√©es factices
   num_samples = 1000
   images = torch.randn(num_samples, 3, 64, 64)  # 1000 images RGB 64√ó64
   labels = torch.randint(0, 10, (num_samples,))  # labels de 0 √† 9
   
   # Cr√©er un dataset avec TensorDataset (une seule ligne !)
   dataset = TensorDataset(images, labels)
   
   print(f"Nombre d'exemples : {len(dataset)}")  # 1000
   
   # Acc√©der √† un exemple
   image, label = dataset[0]
   print(f"Shape de l'image : {image.shape}")  # torch.Size([3, 64, 64])
   print(f"Label : {label}")  # tensor(X) avec X entre 0 et 9

üí° **Avantage** : Simple et direct, pas besoin de cr√©er une classe personnalis√©e.

.. slide::

**Approche 2 : Cr√©er une classe Dataset personnalis√©e avec transformations**

Exemple complet avec chargement depuis des fichiers et application de transformations :

.. code-block:: python

   from torch.utils.data import Dataset
   from torchvision import transforms
   from PIL import Image
   import os

   class ImageFolderDataset(Dataset):
       def __init__(self, image_paths, labels, transform=None):
           """
           Args:
               image_paths: Liste des chemins vers les images
               labels: Liste des labels correspondants
               transform: Transformations √† appliquer (optionnel)
           """
           self.image_paths = image_paths
           self.labels = labels
           self.transform = transform
       
       def __len__(self):
           return len(self.image_paths)
       
       def __getitem__(self, idx):
           # Charger l'image depuis le disque
           img_path = self.image_paths[idx]
           image = Image.open(img_path).convert('RGB')
           label = self.labels[idx]
           
           # Appliquer les transformations si sp√©cifi√©es
           if self.transform:
               image = self.transform(image)
           
           return image, label

   # Exemple d'utilisation avec transformations
   train_paths = ['img1.jpg', 'img2.jpg', 'img3.jpg']  # Chemins vers vos images
   train_labels = [0, 1, 0]  # Labels correspondants

   # D√©finir les transformations pour l'entra√Ænement
   train_transform = transforms.Compose([
       transforms.Resize((224, 224)),        # Redimensionner
       transforms.RandomHorizontalFlip(),    # Augmentation
       transforms.ToTensor(),                # Convertir en tenseur
       transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])
   ])

   # Cr√©er le dataset en passant les transformations
   train_dataset = ImageFolderDataset(train_paths, train_labels, transform=train_transform)

   # Utiliser le dataset
   image, label = train_dataset[0]
   print(image.shape)  # torch.Size([3, 224, 224])
   
   # Pour la validation/test, cr√©er des transformations SANS augmentation
   val_paths = ['val_img1.jpg', 'val_img2.jpg']  # Chemins vers vos images de validation
   val_labels = [1, 0]  # Labels correspondants
   
   val_transform = transforms.Compose([
       transforms.Resize((224, 224)),         # Redimensionner (pas d'augmentation !)
       transforms.ToTensor(),                 # Convertir en tenseur
       transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])
   ])
   val_dataset = ImageFolderDataset(val_paths, val_labels, transform=val_transform)

.. slide::
**√Ä propos des transformations** :

Les transformations permettent de modifier les images avant de les donner au r√©seau. Elles ont deux r√¥les :

1. **Pr√©traitement (toujours n√©cessaire)** : 
   
   - ``ToTensor()`` : convertit une image PIL ou numpy en tenseur PyTorch
   - ``Normalize(mean, std)`` : centre les valeurs autour de 0 pour faciliter l'apprentissage

2. **Augmentation de donn√©es (uniquement pour l'entra√Ænement)** :
   
   - ``RandomHorizontalFlip()`` : retourne l'image horizontalement de mani√®re al√©atoire
   - ``RandomRotation()`` : fait pivoter l'image d'un angle al√©atoire
   - ``ColorJitter()`` : modifie la luminosit√©, le contraste, etc.

üí° **Pourquoi pas d'augmentation pour validation/test ?** On veut √©valuer le mod√®le sur les vraies images, pas sur des versions modifi√©es artificiellement.

.. slide::

5.2. La classe DataLoader
~~~~~~~~~~~~~~~~~~~

``DataLoader`` encapsule un ``Dataset`` et fournit :

- Le d√©coupage automatique en mini-batchs
- Le m√©lange des donn√©es (shuffle)
- Le chargement parall√®le (multiprocessing)
- La gestion du dernier batch incomplet

.. code-block:: python

   from torch.utils.data import DataLoader

   # Cr√©er le dataset
   ...

   # Cr√©er le dataloader
   dataloader = DataLoader(
       dataset,
       batch_size=32,        # taille des batchs
       shuffle=True,         # m√©langer les donn√©es √† chaque epoch (recommand√© pour l'entra√Ænement)
       num_workers=4,        # nombre de processus parall√®les pour charger les donn√©es (0 = chargement dans le processus principal, >0 = chargement en parall√®le pour acc√©l√©rer)
       drop_last=True       # si True, ignore le dernier batch s'il est incomplet (utile quand la taille du batch doit √™tre fixe, par exemple pour le batch normalization)
   )

   # It√©ration sur les batchs
   for batch_idx, (images, labels) in enumerate(dataloader):
       print(f"Batch {batch_idx}: images shape = {images.shape}, labels shape = {labels.shape}")
       # Batch 0: images shape = torch.Size([32, 3, 64, 64]), labels shape = torch.Size([32])

.. slide::

5.3. Diviser en ensembles d'entra√Ænement et de validation
~~~~~~~~~~~~~~~~~~~

Avant de cr√©er des DataLoaders, il est essentiel de bien diviser vos donn√©es en trois ensembles distincts : **train**, **validation** et **test**.

PyTorch fournit ``random_split`` qui divise automatiquement un dataset et m√©lange les donn√©es :

.. code-block:: python

   from torch.utils.data import TensorDataset, random_split
   
   # 1. Cr√©er ou charger toutes les donn√©es
   all_images = torch.randn(1000, 3, 64, 64)
   all_labels = torch.randint(0, 10, (1000,))
   
   # 2. Cr√©er un dataset avec toutes les donn√©es
   full_dataset = TensorDataset(all_images, all_labels)
   
   # 3. D√©finir les tailles de chaque ensemble (70% train, 15% val, 15% test)
   train_size = int(0.70 * len(full_dataset))  # 700
   val_size = int(0.15 * len(full_dataset))     # 150
   test_size = len(full_dataset) - train_size - val_size  # 150
   
   # 4. Diviser le dataset automatiquement (avec m√©lange al√©atoire)
   train_dataset, val_dataset, test_dataset = random_split(
       full_dataset,
       [train_size, val_size, test_size]
   )
   
   # 5. Cr√©er les DataLoaders
   # shuffle=True pour train : m√©langer les donn√©es √† chaque epoch √©vite que le mod√®le apprenne l'ordre des exemples
   # shuffle=False pour val/test : l'ordre n'a pas d'importance pour l'√©valuation, et garder le m√™me ordre permet de reproduire les r√©sultats
   train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
   val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)
   test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)
   
   print(f"Train: {len(train_dataset)} exemples, {len(train_loader)} batches")
   print(f"Validation: {len(val_dataset)} exemples, {len(val_loader)} batches")
   print(f"Test: {len(test_dataset)} exemples, {len(test_loader)} batches")

üí° **Avantages** : ``random_split`` m√©lange automatiquement les donn√©es et cr√©e des sous-ensembles du dataset original sans dupliquer les donn√©es en m√©moire.

.. slide::

**√Ä quoi servent ces trois ensembles ?**

1. **Train (70-80%)** : Utilis√© pour entra√Æner le mod√®le
   
   - Calcul du gradient et mise √† jour des poids
   - Apprentissage des patterns dans les donn√©es

2. **Validation (10-15%)** : Utilis√© pendant l'entra√Ænement pour :
   
   - Surveiller les performances sur des donn√©es non vues
   - D√©tecter le surapprentissage (overfitting)
   - Choisir les meilleurs hyperparam√®tres
   - D√©cider quand arr√™ter l'entra√Ænement
   - Sauvegarder le meilleur mod√®le

3. **Test (10-15%)** : Utilis√© **uniquement √† la fin** pour :
   
   - √âvaluer les performances finales du mod√®le
   - Obtenir des m√©triques non biais√©es
   - Tester sur des donn√©es compl√®tement nouvelles

.. warning::

   ‚ö†Ô∏è **Ne JAMAIS utiliser le test set pendant l'entra√Ænement !**
   
   Le test set doit rester totalement invisible jusqu'√† l'√©valuation finale, sinon vous risquez de sur-optimiser votre mod√®le sur ces donn√©es (data leakage).

.. slide::

5.4. Datasets PyTorch int√©gr√©s
~~~~~~~~~~~~~~~~~~~

PyTorch fournit de nombreux datasets pr√™ts √† l'emploi dans ``torchvision.datasets`` :

.. code-block:: python

   from torchvision import datasets, transforms

   # MNIST (chiffres manuscrits 0-9 en noir et blanc, images 28√ó28)
   mnist_train = datasets.MNIST(
       root='./data',
       train=True,
       download=True,
       transform=transforms.ToTensor()
   )

   # CIFAR-10 (images naturelles en couleur 32√ó32, 10 classes : avion, voiture, oiseau, chat, cerf, chien, grenouille, cheval, bateau, camion)
   cifar_train = datasets.CIFAR10(
       root='./data',
       train=True,
       download=True,
       transform=transforms.ToTensor()
   )

   # Cr√©er un DataLoader
   train_loader = DataLoader(mnist_train, batch_size=64, shuffle=True)

   # Utilisation
   for images, labels in train_loader:
       print(images.shape)  # torch.Size([64, 1, 28, 28]) pour MNIST
       break

.. slide::

üìñ 6. Sauvegarder et charger les poids d'un mod√®le
----------------------

Apr√®s avoir entra√Æn√© un mod√®le pendant des heures, il est essentiel de pouvoir sauvegarder son √©tat pour le r√©utiliser plus tard sans avoir √† tout r√©-entra√Æner. Il est aussi possible de sauvegarder p√©riodiquement pendant l'entra√Ænement pour √©viter de tout perdre en cas d'interruption. Aussi on peut reprendre l'entra√Ænement plus tard. 

6.1. Sauvegarder un mod√®le complet
~~~~~~~~~~~~~~~~~~~

PyTorch offre deux approches pour sauvegarder un mod√®le :

**M√©thode 1 : Sauvegarder tout le mod√®le**

.. code-block:: python

   import torch

   # Entra√Ænement du mod√®le
   model = CNNWithPooling(num_classes=10)
   # ... entra√Ænement ...

   # Sauvegarder le mod√®le complet
   torch.save(model, 'model_complet.pth')

   # Charger le mod√®le complet
   model_charge = torch.load('model_complet.pth')
   model_charge.eval()  # passer en mode √©valuation

**‚ö†Ô∏è Attention** : cette m√©thode sauvegarde toute la structure du mod√®le. Si vous modifiez la d√©finition de la classe, le chargement peut √©chouer.

.. slide::

6.2. Sauvegarder uniquement les poids (m√©thode recommand√©e)
~~~~~~~~~~~~~~~~~~~

**M√©thode 2 : Sauvegarder uniquement les param√®tres (state_dict)**

.. code-block:: python

   # Sauvegarder uniquement les poids
   torch.save(model.state_dict(), 'model_weights.pth')

   # Charger les poids
   model = CNNWithPooling(num_classes=10)  # cr√©er d'abord une instance du mod√®le
   model.load_state_dict(torch.load('model_weights.pth'))
   model.eval()

**üí° Avantages** :

- Plus flexible : on peut modifier l√©g√®rement l'architecture
- Fichier plus l√©ger
- Meilleure pratique recommand√©e par PyTorch

.. code-block:: python

   # Exemple : charger des poids dans un mod√®le avec architecture modifi√©e
   torch.save(model.state_dict(), 'model_10classes.pth')  # mod√®le avec 10 classes
   
   new_model = CNNWithPooling(num_classes=5)  # nouveau mod√®le avec 5 classes
   state_dict = torch.load('model_10classes.pth')
   del state_dict['fc2.weight'], state_dict['fc2.bias']  # supprimer les poids incompatibles
   new_model.load_state_dict(state_dict, strict=False)  # charger en ignorant les couches manquantes

.. warning::
   **‚ö†Ô∏è Attention** : modifier l'architecture et charger partiellement les poids avec ``strict=False`` est dangereux ! Vous risquez de cr√©er des incoh√©rences dans le mod√®le. √Ä √©viter sauf si vous savez exactement ce que vous faites.

.. slide::

6.3. Sauvegarder l'√©tat complet de l'entra√Ænement
~~~~~~~~~~~~~~~~~~~

Pour reprendre l'entra√Ænement exactement o√π vous l'aviez arr√™t√©, sauvegardez √©galement l'optimiseur et l'epoch :

.. code-block:: python

   # Sauvegarder tout l'√©tat d'entra√Ænement
   checkpoint = {
       'epoch': epoch,
       'model_state_dict': model.state_dict(),
       'optimizer_state_dict': optimizer.state_dict(),
       'loss': loss,
   }
   torch.save(checkpoint, 'checkpoint.pth')

   # Charger et reprendre l'entra√Ænement
   model = CNNWithPooling(num_classes=10)
   optimizer = torch.optim.Adam(model.parameters())

   checkpoint = torch.load('checkpoint.pth')
   model.load_state_dict(checkpoint['model_state_dict'])
   optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
   start_epoch = checkpoint['epoch']
   loss = checkpoint['loss']

   model.train()  # reprendre l'entra√Ænement

**Variante : Sauvegarder √† chaque epoch**

.. code-block:: python

   import os
   os.makedirs('checkpoints', exist_ok=True)
   
   # Boucle d'entra√Ænement avec sauvegarde √† chaque epoch
   for epoch in range(num_epochs):
       model.train()
       # ... entra√Ænement ...
       
       # Sauvegarder √† chaque epoch
       checkpoint = {
           'epoch': epoch,
           'model_state_dict': model.state_dict(),
           'optimizer_state_dict': optimizer.state_dict(),
           'train_loss': train_loss,
           'val_loss': val_loss,
       }
       torch.save(checkpoint, f'checkpoints/checkpoint_epoch_{epoch}.pth')
   
   # Reprendre depuis un epoch sp√©cifique (par exemple epoch 5)
   checkpoint = torch.load('checkpoints/checkpoint_epoch_5.pth')
   model.load_state_dict(checkpoint['model_state_dict'])
   optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
   start_epoch = checkpoint['epoch'] + 1  # reprendre √† l'epoch suivant
   
   # Continuer l'entra√Ænement
   for epoch in range(start_epoch, num_epochs):
       # ... suite de l'entra√Ænement ...
       pass

.. slide::

üìñ 7. R√©capitulatif 
----------------------

7.1. Pipeline complet d'entra√Ænement
~~~~~~~~~~~~~~~~~~~

Voici le pipeline standard pour entra√Æner un CNN avec toutes les techniques vues :

.. code-block:: python

   import torch
   import torch.nn as nn
   import torch.optim as optim
   from torch.utils.data import Dataset, DataLoader
   from torchvision import transforms
   from PIL import Image
   import os
   from torch.utils.data import random_split

   # 1. D√©finir le Dataset
   class CustomDataset(Dataset):
       def __init__(self, image_paths, labels, transform=None):
           """
           Args:
               image_paths: Liste des chemins vers les images
               labels: Liste des labels correspondants
               transform: Transformations √† appliquer (optionnel)
           """
           self.image_paths = image_paths
           self.labels = labels
           self.transform = transform
       
       def __len__(self):
           return len(self.image_paths)
       
       def __getitem__(self, idx):
           # Charger l'image depuis le disque
           img_path = self.image_paths[idx]
           image = Image.open(img_path).convert('RGB')
           label = self.labels[idx]
           
           # Appliquer les transformations si sp√©cifi√©es
           if self.transform:
               image = self.transform(image)
           
           return image, label

   # 2. D√©finir le mod√®le avec convolutions et pooling
   class CNN(nn.Module):
       def __init__(self, num_classes):
           super(CNN, self).__init__()
           self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)
           self.pool = nn.MaxPool2d(2, 2)
           self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)
           self.fc = nn.Linear(64 * 16 * 16, num_classes)
       
       def forward(self, x):
           x = self.pool(torch.relu(self.conv1(x)))
           x = self.pool(torch.relu(self.conv2(x)))
           x = x.view(x.size(0), -1)
           x = self.fc(x)
           return x

   # 3. Pr√©parer les donn√©es avec DataLoader
   # D√©finir les transformations
   train_transform = transforms.Compose([
       transforms.Resize((64, 64)),
       transforms.RandomHorizontalFlip(),
       transforms.ToTensor(),
       transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])
   ])
   
   val_transform = transforms.Compose([
       transforms.Resize((64, 64)),
       transforms.ToTensor(),
       transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])
   ])
   
   # Charger toutes les donn√©es (√† adapter selon votre cas)
   all_paths = None # Il faut sp√©cifier le chemin, par exemple : ['path/img/image1.jpg', 'path/img/image2.jpg', ...].
   all_labels = None # Il  faut sp√©cifier le chemin, par exemple : [0, 1, 2, ...].

   # Cr√©er le dataset complet
   full_dataset = CustomDataset(all_paths, all_labels, transform=None)
   
   # Diviser le dataset, par exemple, en train (70%), validation (15%) et test (15%)
   
   train_size = int(0.70 * len(full_dataset))
   val_size = int(0.15 * len(full_dataset))
   test_size = len(full_dataset) - train_size - val_size
   
   train_dataset, val_dataset, test_dataset = random_split(
       full_dataset,
       [train_size, val_size, test_size]
   )
   
   # Appliquer les transformations appropri√©es √† chaque subset
   # Note: random_split cr√©e des Subset qui utilisent le transform du dataset parent
   # Pour des transformations diff√©rentes, on doit cr√©er les datasets s√©par√©ment:
   train_dataset = CustomDataset(
       all_paths[:train_size], 
       all_labels[:train_size], 
       transform=train_transform
   )
   val_dataset = CustomDataset(
       all_paths[train_size:train_size+val_size],
       all_labels[train_size:train_size+val_size],
       transform=val_transform
   )
   test_dataset = CustomDataset(
       all_paths[train_size+val_size:],
       all_labels[train_size+val_size:],
       transform=val_transform  # pas d'augmentation pour test comme pour val
   )
   
   # Cr√©er les DataLoaders
   train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4)
   val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)
   test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)

   # 4. Initialiser le mod√®le, la loss et l'optimiseur
   device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
   model = CNN(num_classes=10).to(device)
   criterion = nn.CrossEntropyLoss()
   optimizer = optim.Adam(model.parameters(), lr=0.001)

   # 5. Cr√©er un dossier pour les sauvegardes
   os.makedirs('checkpoints', exist_ok=True)
   best_val_loss = float('inf')

   # 6. Boucle d'entra√Ænement
   num_epochs = None # il faut mettre un nombre d'epoch, par exemple : 50
   
   for epoch in range(num_epochs):
       # PHASE D'ENTRA√éNEMENT
       model.train()
       train_loss = 0.0
       
       for batch_idx, (images, labels) in enumerate(train_loader):
           images, labels = images.to(device), labels.to(device)
           
           # Forward pass
           outputs = model(images)
           loss = criterion(outputs, labels)
           
           # Backward pass et optimisation
           optimizer.zero_grad()
           loss.backward()
           optimizer.step()
           
           train_loss += loss.item()
       
       # PHASE DE VALIDATION
       model.eval()
       val_loss = 0.0
       correct = 0
       total = 0
       
       with torch.no_grad():
           for images, labels in val_loader:
               images, labels = images.to(device), labels.to(device)
               outputs = model(images)
               loss = criterion(outputs, labels)
               val_loss += loss.item()
               
               _, predicted = torch.max(outputs, 1)
               total += labels.size(0)
               correct += (predicted == labels).sum().item()
       
       # Calcul des moyennes
       train_loss /= len(train_loader)
       val_loss /= len(val_loader)
       val_acc = 100 * correct / total
       
       print(f"Epoch [{epoch+1}/{num_epochs}]")
       print(f"  Train Loss: {train_loss:.4f}")
       print(f"  Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%")
       
       # Sauvegarder le meilleur mod√®le
       if val_loss < best_val_loss:
           best_val_loss = val_loss
           torch.save({
               'epoch': epoch,
               'model_state_dict': model.state_dict(),
               'optimizer_state_dict': optimizer.state_dict(),
               'val_loss': val_loss,
               'val_acc': val_acc,
           }, 'checkpoints/best_model.pth')
           print(f"  ‚úì Meilleur mod√®le sauvegard√©!")

   print("Entra√Ænement termin√©!")

.. slide::

7.2. Pipeline complet d'inf√©rence (test final)
~~~~~~~~~~~

Apr√®s l'entra√Ænement, √©valuez le mod√®le sur le test set pour obtenir les performances finales :

.. code-block:: python

   # 7. PHASE D'INF√âRENCE - √âvaluation finale sur le test set
   
   # Charger le meilleur mod√®le sauvegard√©
   checkpoint = torch.load('checkpoints/best_model.pth')
   model.load_state_dict(checkpoint['model_state_dict'])
   print(f"Meilleur mod√®le charg√© (epoch {checkpoint['epoch']}, val_loss: {checkpoint['val_loss']:.4f})")
   
   # Passer en mode √©valuation
   model.eval()
   
   # √âvaluation sur le test set
   test_loss = 0.0
   correct = 0
   total = 0
   all_predictions = []
   all_labels = []
   
   with torch.no_grad():
       for images, labels in test_loader:
           images, labels = images.to(device), labels.to(device)
           
           # Forward pass
           outputs = model(images)
           loss = criterion(outputs, labels)
           test_loss += loss.item()
           
           # Pr√©dictions
           _, predicted = torch.max(outputs, 1)
           total += labels.size(0)
           correct += (predicted == labels).sum().item()
           
           # Sauvegarder pour analyse d√©taill√©e (optionnel)
           all_predictions.extend(predicted.cpu().numpy())
           all_labels.extend(labels.cpu().numpy())
   
   # Calcul des m√©triques finales
   test_loss /= len(test_loader)
   test_acc = 100 * correct / total
   
   print("\n" + "="*50)
   print("R√âSULTATS FINAUX SUR LE TEST SET")
   print("="*50)
   print(f"Test Loss: {test_loss:.4f}")
   print(f"Test Accuracy: {test_acc:.2f}%")
   print(f"Erreurs: {total - correct}/{total}")
   print("="*50)
   
   # Optionnel : Matrice de confusion et rapport de classification
   from sklearn.metrics import classification_report, confusion_matrix
   import numpy as np
   
   print("\nRapport de classification:")
   print(classification_report(all_labels, all_predictions))
   
   print("\nMatrice de confusion:")
   cm = confusion_matrix(all_labels, all_predictions)
   print(cm)

.. slide::

7.3. Bonnes pratiques
~~~~~~~~~~~~~~~~~~~

**Organisation des donn√©es** :

1. Toujours s√©parer train/validation/test
2. Utiliser ``Dataset`` et ``DataLoader`` pour g√©rer les donn√©es
3. Appliquer les transformations (normalisation, augmentation) dans le ``Dataset``

**Architecture du mod√®le** :

1. Utiliser des convolutions pour les images (pas de MLP)
2. Alterner convolutions et pooling pour r√©duire progressivement la taille
3. Ajouter du batch normalization pour stabiliser l'entra√Ænement
4. Utiliser ReLU comme activation dans les couches cach√©es

**Entra√Ænement** :

1. Utiliser des mini-batchs (taille typique : 32-64)
2. Shuffler les donn√©es d'entra√Ænement (``shuffle=True``)
3. Ne PAS shuffler les donn√©es de validation/test
4. Utiliser ``model.train()`` pour l'entra√Ænement et ``model.eval()`` pour l'√©valuation
5. Utiliser ``torch.no_grad()`` pendant la validation pour √©conomiser la m√©moire

**Sauvegarde** :

1. Sauvegarder le meilleur mod√®le bas√© sur la validation loss
2. Sauvegarder r√©guli√®rement des checkpoints pour pouvoir reprendre
3. Pr√©f√©rer ``state_dict()`` √† sauvegarder le mod√®le entier

.. slide::

üèãÔ∏è Travaux Pratiques 5
--------------------

.. toctree::

    TP_chap5
