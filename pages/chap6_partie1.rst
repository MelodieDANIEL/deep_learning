.. slide::

Chapitre 6 ‚Äî D√©tection d'objets avec des bo√Ætes englobantes
================

üéØ Objectifs du Chapitre
----------------------

.. important::

   √Ä la fin de ce chapitre, vous saurez : 

   - Comprendre la diff√©rence entre classification et d√©tection d'objets.
   - Extraire des images depuis une vid√©o.
   - Utiliser Label Studio pour annoter des objets avec des bo√Ætes englobantes de mani√®re collaborative.
   - Comprendre et manipuler les formats d'annotations.
   - Cr√©er un dataset PyTorch pour la d√©tection d'objets.
   - Entra√Æner un d√©tecteur custom.
   - Comparer avec YOLO et choisir le bon mod√®le selon le contexte.
   - Effectuer l'inf√©rence sur des images en temps r√©el.

.. slide::

üìñ 1. Classification vs D√©tection : comprendre la diff√©rence
----------------------

1.1. Classification d'images (chapitres pr√©c√©dents)
~~~~~~~~~~~~~~~~~~~

Dans les chapitres pr√©c√©dents, nous avons travaill√© sur la **classification d'images** : le mod√®le devait r√©pondre √† la question *"Qu'est-ce qu'il y a dans cette image ?"*

**Exemple** : 

- Entr√©e : une image $$224√ó224$$ pixels
- Sortie : une classe parmi N possibles (ex : "chat", "chien", "voiture")
- Une seule pr√©diction par image

.. code-block:: python

   # Classification : une image ‚Üí une classe
   output = model(image)  # Shape: [batch_size, num_classes]
   predicted_class = torch.argmax(output, dim=1)
   print(f"Cette image contient : {classes[predicted_class]}")

.. slide::

1.2. D√©tection d'objets : localiser ET classifier
~~~~~~~~~~~~~~~~~~~

La **d√©tection d'objets** va plus loin : le mod√®le doit r√©pondre √† *"Qu'est-ce qu'il y a dans cette image ET o√π se trouve chaque objet ?"*

**Pour chaque objet d√©tect√©, le mod√®le doit fournir** :

1. **La classe** de l'objet (ex : "personne", "voiture", "chien")
2. **La bo√Æte englobante** (bounding box en anglais) : 4 coordonn√©es d√©finissant un rectangle autour de l'objet
3. **Un score de confiance** : probabilit√© que la d√©tection soit correcte (0 √† 1)

**Exemple de sortie** :

.. code-block:: python

   # D√©tection : une image ‚Üí plusieurs objets localis√©s
   outputs = model(image)
   # outputs[0]['boxes']: tensor([[x1, y1, x2, y2], [x1, y1, x2, y2], ...])
   # outputs[0]['labels']: tensor([1, 3, 1, ...])  # IDs des classes
   # outputs[0]['scores']: tensor([0.95, 0.87, 0.76, ...])  # Confiances

üí° **Intuition** : imaginez que vous regardez une photo de famille. La classification dirait "photo de groupe", tandis que la d√©tection indiquerait "3 personnes aux positions (x1,y1,x2,y2), (x3,y3,x4,y4), (x5,y5,x6,y6)".

.. slide::

1.3. Qu'est-ce qu'une bo√Æte englobante ?
~~~~~~~~~~~~~~~~~~~

Une **bo√Æte englobante** est un rectangle d√©fini par 4 valeurs. Il existe plusieurs fa√ßons de repr√©senter ces coordonn√©es :

**Format 1 : (x1, y1, x2, y2)** ‚Äî> coins de la bo√Æte (PyTorch/torchvision)

- ``x1, y1`` : coordonn√©es du coin sup√©rieur gauche
- ``x2, y2`` : coordonn√©es du coin inf√©rieur droit

**Format 2 : (x, y, w, h)** ‚Äî> coin + dimensions (Label Studio format standard)

- ``x, y`` : coordonn√©es du coin sup√©rieur gauche (en % )
- ``w`` : largeur de la bo√Æte
- ``h`` : hauteur de la bo√Æte

**Format 3 : (x_center, y_center, w, h) normalis√©** (Label Studio format utilis√© pour YOLO)

- ``x_center, y_center`` : coordonn√©es du centre (normalis√©es entre 0 et 1)
- ``w, h`` : largeur et hauteur (normalis√©es entre 0 et 1)

.. code-block:: text

   Exemple d'une image $$640√ó480$$ pixels avec un objet :
   
   Format PyTorch : [100, 50, 300, 250]
   ‚Üí Rectangle du pixel (100,50) au pixel (300,250)
   
   Format Label Studio : [15.625, 10.417, 31.25, 41.67]
   ‚Üí Coin en (15.625%, 10.417%), taille 31.25%√ó41.67% de l'image
   
   Format YOLO : [0.3125, 0.3125, 0.3125, 0.4167]
   ‚Üí Centre √† 31.25% de la largeur/hauteur, bo√Æte de 31.25%√ó41.67% de l'image

.. slide::

1.4. Applications concr√®tes de la d√©tection
~~~~~~~~~~~~~~~~~~~

La d√©tection d'objets est au c≈ìur de nombreuses applications :

- **V√©hicules autonomes** : d√©tecter pi√©tons, voitures, panneaux
- **Surveillance vid√©o** : compter les personnes, d√©tecter des comportements suspects
- **Commerce** : compter les produits en rayon, d√©tecter les vols
- **M√©dical** : localiser des tumeurs, anomalies sur des radiographies
- **R√©alit√© augment√©e** : d√©tecter des objets pour y superposer des informations

üí° Dans ce chapitre, nous allons apprendre √† cr√©er notre propre d√©tecteur d'objets personnalis√©, de A √† Z !

.. slide::

üìñ 2. Pr√©parer les donn√©es : de la vid√©o aux images annot√©es
----------------------

Le pipeline complet pour cr√©er un dataset de d√©tection :

1. **Capturer une vid√©o** de l'objet √† d√©tecter
2. **Extraire des images** (frames) depuis la vid√©o
3. **Annoter** les objets sur chaque image
4. **Exporter** les annotations dans un format standard
5. **Organiser** le dataset pour l'entra√Ænement

Voyons chaque √©tape en d√©tail.

.. slide::

2.1. Capturer une vid√©o
~~~~~~~~~~~~~~~~~~~

**Objectif** : filmer l'objet que vous voulez d√©tecter sous diff√©rents angles et conditions.

**Conseils pratiques** :

- Dur√©e : 30 secondes √† 2 minutes suffisent
- Vari√©t√© : filmez l'objet sous diff√©rents angles, distances, √©clairages
- Stabilit√© : √©vitez les mouvements trop brusques
- Qualit√© : r√©solution HD ($$1280√ó720$$ ou $$1920√ó1080$$) recommand√©e

üí° **Astuce** : plus vous capturez de vari√©t√©, meilleur sera votre d√©tecteur !

.. note::

   **üí° Vous pouvez commencer avec beaucoup moins !**
   
   - ~50-100 photos extraites d'une vid√©o faite avec un smartphone suffisent pour d√©buter
   - R√©solution modeste ($$640√ó480$$ ou $$224√ó224$$) acceptable pour un prototype
   - M√™me avec peu de vari√©t√©, vous obtiendrez d√©j√† des r√©sultats !

.. slide::

2.2. Installation d'OpenCV
~~~~~~~~~~~~~~~~~~~

**OpenCV (cv2)** est une biblioth√®que Python tr√®s puissante pour manipuler des vid√©os. Elle s'utilise directement en Python sans installer d'outils externes.

.. code-block:: bash

   # Installer OpenCV dans votre environnement virtuel
   pip install opencv-python

.. slide::

2.3. Script d'extraction de base
~~~~~~~~~~~~~~~~~~~

Voici un script complet pour extraire toutes les frames d'une vid√©o :

.. code-block:: python

   import cv2
   import os

   def extraire_frames(video_path, output_dir):
       """
       Extrait toutes les frames d'une vid√©o.
       
       Args:
           video_path: chemin vers la vid√©o
           output_dir: dossier o√π sauvegarder les images
       """
       # Cr√©er le dossier de sortie (exist_ok=True √©vite l'erreur si le dossier existe d√©j√†)
       os.makedirs(output_dir, exist_ok=True)
       
       # Ouvrir la vid√©o
       cap = cv2.VideoCapture(video_path)
       
       # V√©rifier que la vid√©o s'ouvre correctement
       if not cap.isOpened():
           print(f"‚ùå Erreur : impossible d'ouvrir {video_path}")
           return

       # Obtenir les propri√©t√©s de la vid√©o (fps, nombre total de frames)
       fps = cap.get(cv2.CAP_PROP_FPS)
       total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
       
       print(f"üìπ Vid√©o : {total_frames} frames √† {fps:.2f} fps")
       
       frame_count = 0
       
       while True:
           # Lire la frame suivante
           ret, frame = cap.read()
           
           # Si plus de frames, sortir de la boucle
           if not ret:
               break
           
           # Sauvegarder la frame en jpg pour compression
           output_path = os.path.join(output_dir, f'frame_{frame_count:05d}.jpg')
           cv2.imwrite(output_path, frame)
           
           frame_count += 1
       
       # Lib√©rer les ressources
       cap.release()
       
       print(f"‚úì {frame_count} frames extraites dans {output_dir}")

   # Utilisation
   extraire_frames('ma_video.mp4', 'frames/')

.. warning::

   ‚ö†Ô∏è **Attention √† la quantit√© !**
   
   Une vid√©o de 30 secondes √† 30 fps g√©n√®re **900 images**. C'est souvent trop pour annoter manuellement !

.. slide::

2.4. Extraction intelligente (sous-√©chantillonnage)
~~~~~~~~~~~~~~~~~~~

Pour r√©duire le nombre d'images √† annoter, on extrait seulement certaines frames :

.. code-block:: python

   import cv2
   import os

   def extraire_frames_espacees(video_path, output_dir, intervalle=10):
       """
       Extrait 1 frame tous les N frames.
       
       Args:
           video_path: chemin vers la vid√©o
           output_dir: dossier de sortie
           intervalle: extraire 1 frame tous les N frames (ex: 10)
       """
       os.makedirs(output_dir, exist_ok=True)
       
       cap = cv2.VideoCapture(video_path)
       
       if not cap.isOpened():
           print(f"‚ùå Erreur : impossible d'ouvrir {video_path}")
           return
       
       fps = cap.get(cv2.CAP_PROP_FPS)
       total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
       
       print(f"üìπ Extraction de 1 frame tous les {intervalle} frames")
       print(f"   Total attendu : ~{total_frames // intervalle} images")
       
       frame_count = 0
       saved_count = 0
       
       while True:
           ret, frame = cap.read()
           if not ret:
               break
           
           # Sauvegarder seulement toutes les N frames
           if frame_count % intervalle == 0:
               output_path = os.path.join(output_dir, f'frame_{saved_count:05d}.jpg')
               cv2.imwrite(output_path, frame)
               saved_count += 1
           
           frame_count += 1
       
       cap.release()
       
       print(f"‚úì {saved_count} frames extraites sur {frame_count} totales")

   # Exemple : extraire 1 frame toutes les 10 frames
   extraire_frames_espacees('ma_video.mp4', 'frames/', intervalle=10)

**Recommandation pratique** : pour d√©buter, extraire 50-200 images est un bon compromis entre travail d'annotation et qualit√© du mod√®le.

**R√®gles de calcul de l'intervalle** :

- Vid√©o √† 30 fps, 1 frame/seconde ‚Üí ``intervalle=30``
- Vid√©o √† 30 fps, 1 frame toutes les 10 frames ‚Üí ``intervalle=10``
- Extraire ~100 images d'une vid√©o de 900 frames ‚Üí ``intervalle=9``

.. slide::

2.5. Redimensionner les images √† l'extraction
~~~~~~~~~~~~~~~~~~~

Pour √©conomiser l'espace disque et acc√©l√©rer le traitement, on peut redimensionner directement.

.. warning::

   ‚ö†Ô∏è **Attention √† la d√©formation !**
   
   Si votre vid√©o n'est **pas carr√©e** (ex : $$1920√ó1080$$) et que vous redimensionnez en **carr√©** (ex : $$224√ó224$$), l'image sera **d√©form√©e** (√©cras√©e ou √©tir√©e).
   
   **Deux solutions** :
   
   1. **Crop au centre** (RECOMMAND√â) : d√©couper un carr√© au centre avant de redimensionner
   2. **Padding** : ajouter des bordures noires pour garder le ratio

.. slide::

Voici les deux approches :

**Approche 1 : Crop au centre (recommand√©e - pas de d√©formation)**

.. code-block:: python

   import cv2
   import os

   def extraire_frames_crop_redimensionner(video_path, output_dir, intervalle=10, 
                                           target_size=224):
       """
       Extrait, crop au centre en carr√©, puis redimensionne.
       √âVITE la d√©formation en d√©coupant l'image.
       
       Args:
           video_path: chemin vers la vid√©o
           output_dir: dossier de sortie
           intervalle: extraire 1 frame tous les N frames
           target_size: taille finale du carr√© (ex: 224 pour 224√ó224)
       """
       os.makedirs(output_dir, exist_ok=True)
       
       cap = cv2.VideoCapture(video_path)
       
       if not cap.isOpened():
           print(f"‚ùå Erreur : impossible d'ouvrir {video_path}")
           return
       
       # Obtenir les dimensions originales
       original_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
       original_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
       
       print(f"üìπ R√©solution originale : {original_width}√ó{original_height}")
       print(f"üìê Nouvelle r√©solution : {target_size}√ó{target_size} (carr√©)")
       print(f"‚úÇÔ∏è  M√©thode : Crop au centre (pas de d√©formation)")
       
       frame_count = 0
       saved_count = 0
       
       while True:
           ret, frame = cap.read()
           if not ret:
               break
           
           if frame_count % intervalle == 0:
               # √âTAPE 1 : Crop au centre pour obtenir un carr√©
               h, w = frame.shape[:2]
               size = min(h, w)  # Prendre la plus petite dimension
               
               # Calculer les coordonn√©es du crop au centre
               start_y = (h - size) // 2
               start_x = (w - size) // 2
               
               # D√©couper le carr√© au centre
               cropped = frame[start_y:start_y+size, start_x:start_x+size]
               
               # √âTAPE 2 : Redimensionner le carr√© √† la taille souhait√©e
               resized = cv2.resize(cropped, (target_size, target_size))
               
               # Sauvegarder
               output_path = os.path.join(output_dir, f'frame_{saved_count:05d}.jpg')
               cv2.imwrite(output_path, resized)
               saved_count += 1
           
           frame_count += 1
       
       cap.release()
       
       print(f"‚úì {saved_count} frames extraites, cropp√©es et redimensionn√©es")

   # Exemple : extraire 1 frame/seconde en 224√ó224 (format standard CNN)
   extraire_frames_crop_redimensionner('ma_video.mp4', 'frames/', 
                                       intervalle=30, target_size=224)


.. slide::

**Approche 2 : Redimensionnement direct (D√âCONSEILL√â si ratio diff√©rent)**

.. code-block:: python

   def extraire_frames_redimensionner_simple(video_path, output_dir, intervalle=10, 
                                             target_width=640, target_height=480):
       """
       Redimensionne directement sans crop.
       ‚ö†Ô∏è ATTENTION : d√©forme l'image si le ratio change !
       """
       os.makedirs(output_dir, exist_ok=True)
       
       cap = cv2.VideoCapture(video_path)
       
       if not cap.isOpened():
           print(f"‚ùå Erreur : impossible d'ouvrir {video_path}")
           return
       
       original_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
       original_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
       
       print(f"üìπ R√©solution originale : {original_width}√ó{original_height}")
       print(f"üìê Nouvelle r√©solution : {target_width}√ó{target_height}")
       
       # V√©rifier si le ratio va changer
       original_ratio = original_width / original_height
       target_ratio = target_width / target_height
       
       if abs(original_ratio - target_ratio) > 0.01:
           print(f"‚ö†Ô∏è  ATTENTION : Le ratio va changer !")
           print(f"    Original : {original_ratio:.2f}")
           print(f"    Cible : {target_ratio:.2f}")
           print(f"    ‚Üí L'image sera d√©form√©e !")
       
       frame_count = 0
       saved_count = 0
       
       while True:
           ret, frame = cap.read()
           if not ret:
               break
           
           if frame_count % intervalle == 0:
               # Redimensionner directement (PEUT D√âFORMER !)
               resized = cv2.resize(frame, (target_width, target_height))
               
               output_path = os.path.join(output_dir, f'frame_{saved_count:05d}.jpg')
               cv2.imwrite(output_path, resized)
               saved_count += 1
           
           frame_count += 1
       
       cap.release()
       
       print(f"‚úì {saved_count} frames extraites et redimensionn√©es")

   # Exemple : ‚ö†Ô∏è Vid√©o 16:9 ‚Üí carr√© = D√âFORMATION !
   # extraire_frames_redimensionner_simple('ma_video.mp4', 'frames/', 
   #                                        intervalle=30, 
   #                                        target_width=224, target_height=224)

üí° **Recommandations** :

- **Pour la d√©tection d'objets** : utilisez le crop au centre pour √©viter les d√©formations.
- **Pour la classification** : le crop au centre est aussi pr√©f√©rable.
- **R√©solutions recommand√©es** : $$224√ó224$$ (standard CNN), $$640√ó480$$ (compromis vitesse/qualit√©), $$800√ó600$$ (bonne qualit√©).

.. slide::

üìñ 3. Annotation avec Label Studio
----------------------

**Label Studio** est un outil open-source d'annotation collaborative qui permet de cr√©er des bo√Ætes englobantes, de g√©rer plusieurs annotateurs et d'exporter dans diff√©rents formats.

3.1. Installation et premier lancement
~~~~~~~~~~~~~~~~~~~

.. code-block:: bash

   # Installer Label Studio (dans votre environnement virtuel)
   pip install label-studio
   
   # Lancer Label Studio
   label-studio start
   
   # L'interface web s'ouvre automatiquement sur http://localhost:8080

**Premier lancement : cr√©ation du compte**

Au premier lancement, Label Studio vous demande de cr√©er un compte.

.. note::

   üí° **Travail collaboratif**
   
   Si vous souhaitez travailler en √©quipe, vous pourrez inviter vos coll√®gues via **"Invite People"** (voir section 3.5). Label Studio leur enverra automatiquement un email d'invitation.

**Si vous avez d√©j√† un compte** : entrez simplement votre email et mot de passe pour vous connecter.

**En cas de probl√®me** : si Label Studio ne s'ouvre pas automatiquement, ouvrez manuellement votre navigateur et allez sur ``http://localhost:8080``

.. slide::

3.2. Cr√©er un projet d'annotation
~~~~~~~~~~~~~~~~~~~

**√âtapes dans l'interface web** :

1. Cliquer sur "Create Project"

2. Donner un nom au projet (ex : "Detection_Bouteille")

3. **Import des donn√©es** :
   
   - Onglet "Data Import"
   - S√©lectionner tous les fichiers du dossier ``frames/`` (ou le dossier o√π vous avez extrait les images)
   - Cliquer sur "Import"

4. **Configuration de l'annotation** :
   
   - Cliquez sur votre projet pour l'ouvrir
   - Cliquez sur "Settings" (en haut √† droite ou dans le menu du projet)
   - Allez dans l'onglet "Labeling Interface"
   - Cliquez sur "Browse Templates"
   - S√©lectionnez "Computer Vision"
   - Choisissez "Object Detection with Bounding Boxes"
   - Une page s'ouvre pour d√©finir les labels (classes d'objets)

.. slide::

3.3. D√©finir les classes d'objets
~~~~~~~~~~~~~~~~~~~

Apr√®s avoir choisi le template, une interface appara√Æt o√π vous pouvez d√©finir vos labels (classes d'objets).

**M√©thode simple : ajouter des labels via l'interface**

1. Dans le champ "Add Label Name", entrez le nom de votre premi√®re classe (ex : "bouteille")
2. Cliquez sur "Add" ou appuyez sur Entr√©e
3. R√©p√©tez pour chaque classe d'objet √† d√©tecter
4. Cliquez sur "Save" pour valider

**Si vous pr√©f√©rez √©diter le code XML directement**, vous pouvez voir/modifier le code de configuration :

**Exemple pour d√©tecter des bouteilles et des gobelets** :

.. code-block:: xml

   <View>
     <Image name="image" value="$image"/>
     <RectangleLabels name="label" toName="image">
       <Label value="bouteille" background="green"/>
       <Label value="gobelet" background="blue"/>
     </RectangleLabels>
   </View>

üí° **Astuce** : commencez avec une seule classe pour simplifier. Vous pourrez toujours ajouter des classes plus tard.

.. slide::

3.4. Annoter les images
~~~~~~~~~~~~~~~~~~~

**Pour cr√©er une annotation** :

1. Cliquer sur une t√¢che (image) dans la liste
2. S√©lectionner la classe dans le panneau en bas de l'image (ex : "bouteille")
3. Dessiner un rectangle autour de l'objet :
   
   - Cliquer et maintenir le bouton de la souris
   - D√©placer pour cr√©er le rectangle
   - Rel√¢cher quand l'objet est bien encadr√©

4. R√©p√©ter pour tous les objets de l'image
5. Cliquer sur "Submit" pour valider l'annotation

**Modifier une annotation existante** :

- **Double-cliquer** sur un rectangle pour le s√©lectionner
- Vous pouvez alors :
  
  - **D√©placer** le rectangle en le faisant glisser
  - **Redimensionner** en tirant sur les coins ou les bords
  - **Changer la classe** dans le panneau de droite
  - **Supprimer** avec la touche ``Suppr``

**Bonnes pratiques d'annotation** :

- La bo√Æte doit englober **tout l'objet** visible (pas trop serr√©e, pas trop large)
- Si un objet est **partiellement visible** (coup√© par le bord), l'annoter quand m√™me
- Si un objet est **tr√®s petit** (<10 pixels), c'est optionnel (difficiles √† d√©tecter)
- **Coh√©rence** : gardez le m√™me style d'annotation d'une image √† l'autre

.. slide::

3.5. Annotation collaborative : inviter des personnes
~~~~~~~~~~~~~~~~~~~

Pour travailler en √©quipe sur l'annotation, suivez ces √©tapes :

**√âtape 1 : Inviter des personnes**

1. Dans Label Studio, cliquez sur l'ic√¥ne **Organization** (en haut √† droite, ic√¥ne avec plusieurs personnes)

2. Allez dans l'onglet **"People"**

3. Cliquez sur le bouton **"Invite People"**

4. Entrez les adresses email de vos coll√®gues (ex : ``marie.dubois@exemple.com``, ``paul.martin@exemple.com``)

5. Choisissez le r√¥le pour chaque personne :
   
   - **Annotator** : peut uniquement annoter les images
   - **Reviewer** : peut annoter ET valider/corriger les annotations des autres
   - **Manager** : peut g√©rer les projets et les param√®tres

6. Cliquez sur **"Send Invitations"**

7. Vos coll√®gues recevront un email avec un lien pour cr√©er leur compte

.. slide::

**√âtape 2 : Ajouter les membres √† votre projet**

Une fois les invitations accept√©es :

1. Ouvrez votre projet d'annotation
2. Allez dans **"Settings"** ‚Üí **"Members"**
3. Cliquez sur **"Add Member"**
4. S√©lectionnez les personnes dans la liste
5. Assignez-leur le r√¥le appropri√© pour ce projet

**√âtape 3 : R√©partir le travail (optionnel mais recommand√©)**

Pour √©viter que deux personnes annotent les m√™mes images :

1. Dans le projet, onglet **"Tasks"** (liste des images)
2. S√©lectionnez un groupe d'images (ex : images 1-50)
3. Menu **"Actions"** ‚Üí **"Assign Annotators"**
4. Choisissez la personne
5. R√©p√©tez pour les autres groupes d'images

**Exemple de workflow collaboratif** :

- **Marie** (Annotator) : images 1-50
- **Paul** (Annotator) : images 51-100
- **Sophie** (Reviewer) : v√©rifie et corrige toutes les annotations
- **Vous** (Manager) : supervise et exporte les donn√©es finales

üí° **Astuce qualit√©** : faites annoter 10 images par deux personnes diff√©rentes et comparez. Un IoU (Intersection over Union) > 0.7 indique une bonne coh√©rence entre annotateurs.

.. slide::

3.6. Raccourcis clavier utiles
~~~~~~~~~~~~~~~~~~~

Pour acc√©l√©rer l'annotation :

- ``1, 2, 3...`` : s√©lectionner la classe 1, 2, 3...
- ``Ctrl + Enter`` ou ``Cmd + Enter`` : soumettre et passer √† l'image suivante
- ``Ctrl + Z`` : annuler la derni√®re action
- ``Suppr`` : supprimer la bo√Æte s√©lectionn√©e
- ``Fl√®ches`` : ajuster finement la position d'une bo√Æte

.. slide::

üìñ 4. Formats d'annotations : Label Studio JSON et YOLO
----------------------

Apr√®s l'annotation dans Label Studio, nous allons utiliser deux formats diff√©rents dans ce chapitre :

1. **Format JSON** : pour cr√©er notre d√©tecteur CNN custom (sections 5-7)
2. **Format YOLO** : pour entra√Æner YOLO sur notre dataset (section 9)

.. slide::

4.1. Format Label Studio JSON
~~~~~~~~~~~~~~~~~~~

Label Studio utilise son propre format JSON qui stocke toutes les annotations dans un seul fichier.

**Caract√©ristiques principales** :

- **Un fichier JSON** contenant toutes les images et leurs annotations
- **Coordonn√©es en pourcentages** : ``x, y, width, height`` exprim√©s en % de la taille de l'image (0-100)
- ``x, y`` : position du coin sup√©rieur gauche en %
- ``width, height`` : dimensions de la bo√Æte en %
- ``rectanglelabels`` : liste des classes d√©tect√©es

**Avantages** :

- Format complet avec m√©tadonn√©es (id, date, utilisateur...)
- Un seul fichier pour tout le dataset (facile √† manipuler)
- Structure Python-friendly (facile √† parser)

**Inconv√©nients** :

- Fichier unique qui peut devenir volumineux
- N√©cessite de cr√©er un Dataset PyTorch custom

.. note::

   üí° La structure d√©taill√©e du JSON sera pr√©sent√©e en **section 5** avec des exemples de parsing Python

.. slide::

4.2. Format YOLO (TXT) 
~~~~~~~~~~~~~~~~~~~

**YOLO** utilise un format ultra-simple : un fichier texte par image.

**Exemple de fichier** ``frame_00001.txt`` :

.. code-block:: text

   0 0.3125 0.2604 0.3125 0.3125
   1 0.6250 0.5417 0.1562 0.2500

**Format d'une ligne** : ``class_id x_center y_center width height``

**Toutes les valeurs sont normalis√©es entre 0 et 1** :

- ``class_id`` : entier (0, 1, 2...) correspondant √† l'index de la classe
- ``x_center`` : position X du centre / largeur de l'image
- ``y_center`` : position Y du centre / hauteur de l'image
- ``width`` : largeur de la bo√Æte / largeur de l'image
- ``height`` : hauteur de la bo√Æte / hauteur de l'image

**Exemple de calcul** (image $$640√ó480$$, objet de 100,50 √† 300,200) :

.. code-block:: python

   # Coordonn√©es en pixels
   x1, y1, x2, y2 = 100, 50, 300, 200
   img_width, img_height = 640, 480
   
   # Calcul des valeurs YOLO
   x_center = ((x1 + x2) / 2) / img_width  # (100+300)/2 / 640 = 0.3125
   y_center = ((y1 + y2) / 2) / img_height  # (50+200)/2 / 480 = 0.2604
   width = (x2 - x1) / img_width  # (300-100) / 640 = 0.3125
   height = (y2 - y1) / img_height  # (200-50) / 480 = 0.3125
   
   # Ligne YOLO : "0 0.3125 0.2604 0.3125 0.3125"

.. slide::

**Avantages du format YOLO** :

- Format ultra-compact et rapide √† parser
- Un fichier par image (facile √† parall√©liser)
- Coordonn√©es normalis√©es (insensible √† la r√©solution)
- Utilis√© nativement par tous les mod√®les YOLO

**Structure compl√®te d'un dataset YOLO** :

.. code-block:: text

   dataset_yolo/
   ‚îú‚îÄ‚îÄ images/           # Toutes les images
   ‚îÇ   ‚îú‚îÄ‚îÄ img1.jpg
   ‚îÇ   ‚îî‚îÄ‚îÄ img2.jpg
   ‚îú‚îÄ‚îÄ labels/           # Fichiers .txt (m√™mes noms que les images)
   ‚îÇ   ‚îú‚îÄ‚îÄ img1.txt
   ‚îÇ   ‚îî‚îÄ‚îÄ img2.txt
   ‚îî‚îÄ‚îÄ classes.txt       # Liste des noms de classes (1 par ligne)

.. slide::

4.3. Exporter depuis Label Studio
~~~~~~~~~~~~~~~~~~~

Label Studio peut exporter dans plusieurs formats. **Dans ce chapitre, nous allons utiliser :**

1. **Le format JSON natif de Label Studio** pour cr√©er un **d√©tecteur CNN custom** (sections 5-7)
2. **Le format YOLO** pour entra√Æner YOLO sur notre dataset custom (section 9)

**√âtapes pour exporter** :

1. Ouvrez votre projet dans Label Studio
2. Cliquez sur "Export" en haut de la liste des t√¢ches
3. Choisir le format :
   
   - **"JSON"** ‚Üí format natif Label Studio (pour sections 5-7)
   - **"YOLO"** ‚Üí fichiers .txt au format YOLO (pour section 9)

4. T√©l√©charger le fichier

.. slide::

üìñ 5. Comprendre le format JSON de Label Studio
----------------------

Nous allons utiliser le **format JSON natif de Label Studio** pour entra√Æner notre d√©tecteur custom. Voyons d'abord sa structure.

5.1. Structure du fichier JSON export√©
~~~~~~~~~~~~~~~~~~~

Apr√®s avoir cliqu√© sur "Export" ‚Üí "JSON" dans Label Studio, vous obtenez un fichier avec cette structure :

.. code-block:: json

   [
     {
       "id": 1,
       "annotations": [
         {
           "id": 1,
           "completed_by": 1,
           "result": [
             {
               "original_width": 224,
               "original_height": 224,
               "value": {
                 "x": 24.67,
                 "y": 45.99,
                 "width": 52.41,
                 "height": 54.01,
                 "rotation": 0,
                 "rectanglelabels": ["cube"]
               },
               "type": "rectanglelabels",
               "from_name": "label",
               "to_name": "image"
             }
           ]
         }
       ],
       "file_upload": "ad2a7904-frame_000000.jpg",
       "data": {
         "image": "/data/upload/1/ad2a7904-frame_000000.jpg"
       }
     },
     {
       "id": 2,
       "annotations": [...],
       "file_upload": "caed06ef-frame_000001.jpg",
       "data": {
         "image": "/data/upload/1/caed06ef-frame_000001.jpg"
       }
     }
   ]

.. slide::

**Points importants** :

- Chaque √©l√©ment du tableau JSON repr√©sente **une image**
- ``file_upload`` : nom original du fichier image
- ``data.image`` : chemin dans Label Studio (√† ignorer, on utilise ``file_upload``)
- ``annotations[0].result`` : liste des bo√Ætes englobantes
- ``value.x, y, width, height`` : **coordonn√©es en pourcentage** (0-100) de l'image
- ``value.rectanglelabels`` : liste des labels (ici un seul)
- ``original_width`` et ``original_height`` : dimensions de l'image (utile pour v√©rifier)

.. slide::

5.2. Extraire le nom de fichier depuis le JSON
~~~~~~~~~~~~~~~~~~~

Le champ ``file_upload`` contient le nom du fichier tel que stock√© par Label Studio (avec un pr√©fixe UUID ajout√© automatiquement). Voici comment l'utiliser :

.. code-block:: python

   import json

   # Charger le JSON
   with open('project-1-annotations.json', 'r', encoding='utf-8') as f: # ADAPTEZ : le nom de fichier
       data = json.load(f)

   # Examiner la premi√®re image
   first_item = data[0]
   
   # Le champ file_upload contient le nom avec le pr√©fixe UUID
   image_name = first_item['file_upload']
   print(f"Nom du fichier : {image_name}")
   # Exemple : "ad2a7904-frame_000000.jpg"
   
   # Vous pouvez aussi l'extraire depuis data.image (m√™me r√©sultat)
   import os
   image_path = first_item['data']['image']
   image_name_alt = os.path.basename(image_path)
   print(f"Nom du fichier (depuis path) : {image_name_alt}")
   # Exemple : "ad2a7904-frame_000000.jpg"
   
   # V√©rifier les dimensions de l'image dans les annotations
   result = first_item['annotations'][0]['result'][0]
   print(f"Dimensions : {result['original_width']}x{result['original_height']}")
   # Exemple : "Dimensions : 224x224"

.. warning::

   ‚ö†Ô∏è **Attention au pr√©fixe UUID !**
   
   Label Studio ajoute automatiquement un pr√©fixe UUID lors de l'upload (ex : ``ad2a7904-frame_000000.jpg``). 
   
   **Si vos fichiers images ont les noms originaux** (``frame_000000.jpg``), vous devrez extraire la partie originale du nom.

.. slide::

**Script complet : nettoyer le JSON et renommer les images** :

.. code-block:: python

   import json
   import os
   import shutil

   def clean_labelstudio_dataset(json_path, images_dir, output_json_path=None):
       """
       Nettoie compl√®tement un dataset Label Studio :
       - Enl√®ve les pr√©fixes UUID du JSON
       - Renomme les fichiers images correspondants
       
       Args:
           json_path: chemin vers le JSON Label Studio
           images_dir: dossier contenant les images
           output_json_path: chemin JSON de sortie (None = √©crase l'original)
       """
       
       def remove_prefix(filename):
           """Enl√®ve le pr√©fixe UUID (8 caract√®res hexa + tiret)."""
           if '-' in filename:
               parts = filename.split('-', 1)
               if len(parts[0]) == 8 and all(c in '0123456789abcdef' for c in parts[0].lower()):
                   return parts[1]
           return filename
       
       # 1. NETTOYER LE JSON
       print("üìÑ Nettoyage du JSON...")
       with open(json_path, 'r', encoding='utf-8') as f:
           data = json.load(f)
       
       json_changes = 0
       for item in data:
           # Nettoyer file_upload
           if 'file_upload' in item:
               original = remove_prefix(item['file_upload'])
               if original != item['file_upload']:
                   print(f"  ‚úì {item['file_upload']} ‚Üí {original}")
                   item['file_upload'] = original
                   json_changes += 1
           
           # Nettoyer data.image
           if 'data' in item and 'image' in item['data']:
               path = item['data']['image']
               filename = os.path.basename(path)
               cleaned = remove_prefix(filename)
               if '/' in path:
                   item['data']['image'] = path.rsplit('/', 1)[0] + '/' + cleaned
               else:
                   item['data']['image'] = cleaned
       
       # Sauvegarder le JSON nettoy√©
       output_path = output_json_path or json_path
       with open(output_path, 'w', encoding='utf-8') as f:
           json.dump(data, f, indent=2, ensure_ascii=False)
       
       print(f"  ‚úì {json_changes} noms nettoy√©s dans le JSON")
       print(f"  ‚úì JSON sauvegard√© : {output_path}\n")
       
       # 2. RENOMMER LES IMAGES
       print("üñºÔ∏è  Renommage des images...")
       if not os.path.exists(images_dir):
           print(f"  ‚ö†Ô∏è  Dossier introuvable : {images_dir}")
           return
       
       image_changes = 0
       for filename in os.listdir(images_dir):
           if not filename.lower().endswith(('.jpg', '.jpeg', '.png')):
               continue
           
           original = remove_prefix(filename)
           if original != filename:
               old_path = os.path.join(images_dir, filename)
               new_path = os.path.join(images_dir, original)
               
               if os.path.exists(new_path):
                   print(f"  ‚ö†Ô∏è  {original} existe d√©j√†, ignor√©")
                   continue
               
               shutil.move(old_path, new_path)
               print(f"  ‚úì {filename} ‚Üí {original}")
               image_changes += 1
       
       print(f"\n‚úÖ Termin√© ! {image_changes} images renomm√©es")

   # üéØ UTILISATION
   clean_labelstudio_dataset(
       json_path='project-1-annotations.json',
       images_dir='data/images/',
       output_json_path='project-1-annotations-clean.json'  # Ou None pour √©craser
   )

üí° **Astuce** : le format peut varier selon la configuration de Label Studio. Utilisez ``file_upload`` si disponible, sinon extrayez depuis ``data.image``.

.. slide::

5.3. V√©rifier que tout fonctionne
~~~~~~~~~~~~~~~~~~~

Apr√®s avoir nettoy√© le JSON, v√©rifiez que les donn√©es sont correctes :

.. code-block:: python

   import json
   import os
   import cv2

   def verify_labelstudio_dataset(json_path, images_dir, num_samples=5):
       """
       V√©rifie que le JSON et les images correspondent.
       Affiche les statistiques et dessine quelques exemples.
       
       Args:
           json_path: JSON Label Studio (nettoy√©)
           images_dir: dossier des images
           num_samples: nombre d'images √† visualiser
       """
       
       # Charger le JSON
       with open(json_path, 'r', encoding='utf-8') as f:
           data = json.load(f)
       
       print(f"üìä STATISTIQUES DU DATASET")
       print(f"   Nombre d'images : {len(data)}")
       
       # Compter les objets et classes
       total_objects = 0
       classes_count = {}
       missing_images = []
       
       for item in data:
           image_name = item['file_upload']
           full_path = os.path.join(images_dir, image_name)
           
           # V√©rifier que l'image existe
           if not os.path.exists(full_path):
               missing_images.append(image_name)
               continue
           
           # Compter les objets
           annotations = item.get('annotations', [])
           if annotations:
               result = annotations[-1].get('result', [])
               for ann in result:
                   if ann.get('type') == 'rectanglelabels':
                       total_objects += 1
                       label = ann['value']['rectanglelabels'][0]
                       classes_count[label] = classes_count.get(label, 0) + 1
       
       print(f"   Objets annot√©s : {total_objects}")
       print(f"   Classes : {list(classes_count.keys())}")
       for cls, count in classes_count.items():
           print(f"      - {cls}: {count} objets")
       
       if missing_images:
           print(f"\n‚ö†Ô∏è  {len(missing_images)} images manquantes :")
           for img in missing_images[:5]:
               print(f"      - {img}")
       else:
           print(f"\n‚úÖ Toutes les images sont pr√©sentes !")
       
       # Visualiser quelques exemples
       print(f"\nüñºÔ∏è  VISUALISATION DE {num_samples} EXEMPLES")
       os.makedirs('verification', exist_ok=True)
       
       for idx, item in enumerate(data[:num_samples]):
           image_name = item['file_upload']
           full_path = os.path.join(images_dir, image_name)
           
           if not os.path.exists(full_path):
               continue
           
           # Charger l'image
           img = cv2.imread(full_path)
           h, w = img.shape[:2]
           
           # Dessiner les bo√Ætes
           annotations = item.get('annotations', [])
           if annotations:
               result = annotations[-1].get('result', [])
               for ann in result:
                   if ann.get('type') != 'rectanglelabels':
                       continue
                   
                   value = ann['value']
                   label = value['rectanglelabels'][0]
                   
                   # Convertir % ‚Üí pixels
                   x1 = int(value['x'] * w / 100)
                   y1 = int(value['y'] * h / 100)
                   x2 = int((value['x'] + value['width']) * w / 100)
                   y2 = int((value['y'] + value['height']) * h / 100)
                   
                   # Dessiner
                   cv2.rectangle(img, (x1, y1), (x2, y2), (0, 255, 0), 2)
                   cv2.putText(img, label, (x1, y1-10),
                              cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)
           
           # Sauvegarder
           output_path = f'verification/check_{idx:02d}_{image_name}'
           cv2.imwrite(output_path, img)
           print(f"   ‚úì {output_path}")
       
       print(f"\n‚úÖ V√©rification termin√©e ! Consultez le dossier 'verification/'")

   # üéØ UTILISATION
   verify_labelstudio_dataset(
       json_path='project-1-annotations-clean.json',
       images_dir='data/images/',
       num_samples=5
   )

üí° **Conseil** : v√©rifiez toujours vos donn√©es avant de lancer l'entra√Ænement !

.. slide::

üìñ 6. Cr√©er un Dataset PyTorch pour la d√©tection
----------------------

Maintenant que nos annotations sont pr√™tes, cr√©ons un Dataset PyTorch personnalis√© qui charge directement le JSON de Label Studio.

6.1. Structure de dossiers recommand√©e
~~~~~~~~~~~~~~~~~~~

Organisez vos fichiers ainsi :

.. code-block:: text

   mon_projet_detection/
   ‚îú‚îÄ‚îÄ data/
   ‚îÇ   ‚îú‚îÄ‚îÄ images/           # Toutes les images
   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ frame_00001.jpg
   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ frame_00002.jpg
   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ...
   ‚îÇ   ‚îú‚îÄ‚îÄ annotations.json  # Export Label Studio
   ‚îÇ   ‚îî‚îÄ‚îÄ splits.json       # Split train/val/test (optionnel)
   ‚îî‚îÄ‚îÄ train.py              # Script d'entra√Ænement

**Fichier** ``splits.json`` **(optionnel)** : pour s√©parer train/val/test

.. code-block:: json

   {
     "train": ["frame_00001.jpg", "frame_00002.jpg", ...],
     "val": ["frame_00151.jpg", "frame_00152.jpg", ...],
     "test": ["frame_00181.jpg", "frame_00182.jpg", ...]
   }

.. note::

   üí° **Split automatique avec random_split**
   
   Pas besoin de cr√©er ``splits.json`` ! Vous pouvez s√©parer train/val/test directement dans le code avec ``random_split`` comme au chapitre 5.

.. slide::

6.2. Classe DetectionDataset compl√®te
~~~~~~~~~~~~~~~~~~~

Voici une impl√©mentation qui charge directement le JSON de Label Studio :

.. warning::

   ‚ö†Ô∏è **Pr√©requis : Nettoyage des UUID**
   
   Avant d'utiliser ce Dataset, assurez-vous d'avoir :
   
   1. ‚úÖ Nettoy√© le JSON avec ``clean_labelstudio_dataset()`` (section 5.2)
   2. ‚úÖ Renomm√© les images pour enlever les pr√©fixes UUID (section 5.2)
   3. ‚úÖ V√©rifi√© avec ``verify_labelstudio_dataset()`` (section 5.3)
   
   **Sinon**, le Dataset ne trouvera pas les images (erreur ``FileNotFoundError``) car les noms dans le JSON ne correspondront pas aux noms des fichiers sur le disque.

.. code-block:: python

   import torch
   from torch.utils.data import Dataset
   from PIL import Image
   import json
   import os
   from torchvision.transforms import functional as F

   class LabelStudioDetectionDataset(Dataset):
       """
       Dataset PyTorch qui charge directement les annotations Label Studio.
       """
       
       def __init__(self, json_path, images_dir, split_images=None, transforms=None):
           """
           Args:
               json_path: chemin vers le JSON export√© de Label Studio
               images_dir: dossier contenant les images
               split_images: liste de noms d'images √† utiliser (None = toutes)
               transforms: transformations √† appliquer (optionnel)
           """
           self.images_dir = images_dir
           self.transforms = transforms
           
           # Charger le JSON
           with open(json_path, 'r', encoding='utf-8') as f:
               all_data = json.load(f)
           
           # Filtrer selon split_images si fourni
           if split_images:
               split_set = set(split_images)
               self.data = [
                   item for item in all_data
                   if os.path.basename(item['data']['image']) in split_set
               ]
           else:
               self.data = all_data
           
           # Extraire les noms de classes uniques
           classes_set = set()
           for item in self.data:
               annotations = item.get('annotations', [])
               if annotations:
                   result = annotations[-1].get('result', [])
                   for ann in result:
                       if ann.get('type') == 'rectanglelabels':
                           labels = ann['value'].get('rectanglelabels', [])
                           classes_set.update(labels)
           
           self.classes = sorted(list(classes_set))
           self.class_to_idx = {cls: idx+1 for idx, cls in enumerate(self.classes)}
           
           print(f"Dataset initialis√© : {len(self.data)} images, "
                 f"{len(self.classes)} classes : {self.classes}")
       
       def __len__(self):
           return len(self.data)
       
       def __getitem__(self, idx):
           """
           Charge une image et ses annotations.
           
           Returns:
               img: tensor [3, H, W]
               target: dict avec 'boxes', 'labels', 'image_id'
           """
           item = self.data[idx]
           
           # Extraire le nom de l'image et la charger
           image_path_str = item['data']['image']
           image_name = os.path.basename(image_path_str)
           full_path = os.path.join(self.images_dir, image_name)
           
           img = Image.open(full_path).convert('RGB')
           img_width, img_height = img.size
           
           # R√©cup√©rer les annotations
           boxes = []
           labels = []
           
           annotations = item.get('annotations', [])
           if annotations:
               # Prendre la derni√®re version (plus r√©cente)
               result = annotations[-1].get('result', [])
               
               for ann in result:
                   if ann.get('type') != 'rectanglelabels':
                       continue
                   
                   value = ann['value']
                   
                   # Label Studio donne les coordonn√©es en pourcentages (0-100)
                   x_percent = value['x']
                   y_percent = value['y']
                   w_percent = value['width']
                   h_percent = value['height']
                   
                   # Convertir en pixels [x1, y1, x2, y2]
                   x1 = (x_percent / 100.0) * img_width
                   y1 = (y_percent / 100.0) * img_height
                   x2 = ((x_percent + w_percent) / 100.0) * img_width
                   y2 = ((y_percent + h_percent) / 100.0) * img_height
                   
                   boxes.append([x1, y1, x2, y2])
                   
                   # R√©cup√©rer la classe
                   class_name = value['rectanglelabels'][0]
                   class_idx = self.class_to_idx[class_name]
                   labels.append(class_idx)
           
           # Convertir en tenseurs
           boxes = torch.as_tensor(boxes, dtype=torch.float32)
           labels = torch.as_tensor(labels, dtype=torch.int64)
           
           # Cr√©er le dictionnaire target
           target = {}
           target['boxes'] = boxes
           target['labels'] = labels
           target['image_id'] = torch.tensor([idx])
           
           # Si aucune bo√Æte, cr√©er des tenseurs vides
           if len(boxes) == 0:
               target['boxes'] = torch.zeros((0, 4), dtype=torch.float32)
               target['labels'] = torch.zeros((0,), dtype=torch.int64)
           
           # Appliquer les transformations
           if self.transforms:
               img = self.transforms(img)
           else:
               img = F.to_tensor(img)
           
           return img, target
       
       def get_class_name(self, class_id):
           """Retourne le nom d'une classe depuis son ID."""
           return self.classes[class_id - 1]

.. note::

   üí° **Gestion des IDs de classes**
   
   - Les classes sont automatiquement extraites du JSON
   - Les IDs commencent √† **1** (0 est r√©serv√© au background dans torchvision)
   - ``class_to_idx`` : dictionnaire ``{'bouteille': 1, 'gobelet': 2}``

.. slide::

6.3. Cr√©er les DataLoaders avec split automatique
~~~~~~~~~~~~~~~~~~~

Si vous n'avez pas de fichier ``splits.json``, utilisez ``random_split`` comme au chapitre 5 :

.. code-block:: python

   from torch.utils.data import DataLoader, random_split

   # Charger le dataset complet
   full_dataset = LabelStudioDetectionDataset(
       json_path='data/annotations.json',
       images_dir='data/images/'
   )

   # Split : 70% train, 15% val, 15% test
   total_size = len(full_dataset)
   train_size = int(0.70 * total_size)
   val_size = int(0.15 * total_size)
   test_size = total_size - train_size - val_size

   train_dataset, val_dataset, test_dataset = random_split(
   full_dataset, 
   [train_size, val_size, test_size],
   generator=torch.Generator().manual_seed(42)
   )

   print(f"Train : {len(train_dataset)} images")
   print(f"Val   : {len(val_dataset)} images")
   print(f"Test  : {len(test_dataset)} images")

   # Cr√©er les dataloaders
   def collate_fn(batch):
       """Fonction n√©cessaire car chaque image a un nombre diff√©rent d'objets."""
       return tuple(zip(*batch))

   train_loader = DataLoader(
       train_dataset,
       batch_size=4,
       shuffle=True,
       num_workers=4,
       collate_fn=collate_fn
   )

   val_loader = DataLoader(
       val_dataset,
       batch_size=4,
       shuffle=False,
       num_workers=4,
       collate_fn=collate_fn
   )

üí° **Avantage** : tout en un ! Pas besoin de g√©rer des listes de noms de fichiers s√©par√©es.

.. slide::

6.5. Tester le chargement des donn√©es
~~~~~~~~~~~~~~~~~~~

Toujours v√©rifier que le Dataset charge correctement :

.. code-block:: python

   # Charger un exemple
   img, target = train_dataset[0]

   print(f"Image shape: {img.shape}")
   print(f"Nombre d'objets: {len(target['boxes'])}")
   print(f"Boxes:\n{target['boxes']}")
   print(f"Labels: {target['labels']}")

   # Visualiser quelques exemples
   import matplotlib.pyplot as plt
   import matplotlib.patches as patches

   def visualize_sample(dataset, idx):
       img, target = dataset[idx]
       
       # Convertir le tensor en numpy pour l'affichage
       img_np = img.permute(1, 2, 0).numpy()
       
       fig, ax = plt.subplots(1, figsize=(12, 8))
       ax.imshow(img_np)
       
       # Dessiner chaque bo√Æte
       for box, label in zip(target['boxes'], target['labels']):
           x1, y1, x2, y2 = box.tolist()
           width = x2 - x1
           height = y2 - y1
           
           rect = patches.Rectangle(
               (x1, y1), width, height,
               linewidth=2, edgecolor='r', facecolor='none'
           )
           ax.add_patch(rect)
           
           # Ajouter le label
           class_name = dataset.get_class_name(label.item())
           ax.text(x1, y1-5, class_name, 
                  bbox=dict(facecolor='red', alpha=0.5),
                  fontsize=12, color='white')
       
       plt.axis('off')
       plt.tight_layout()
       plt.savefig(f'check_sample_{idx}.png')
       print(f"‚úì Visualisation sauvegard√©e : check_sample_{idx}.png")

   # V√©rifier les 5 premiers exemples
   for i in range(5):
       visualize_sample(train_dataset, i)

