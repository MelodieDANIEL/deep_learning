.. slide::

Chapitre 6 ‚Äî D√©tection d'objets avec des bo√Ætes englobantes (partie 1)
================

üéØ Objectifs du Chapitre
----------------------

.. important::

   √Ä la fin de ce chapitre, vous saurez : 

   - Comprendre la diff√©rence entre classification et d√©tection d'objets.
   - Extraire des images depuis une vid√©o.
   - Utiliser Label Studio pour annoter des objets avec des bo√Ætes englobantes de mani√®re collaborative.
   - Comprendre et manipuler les formats d'annotations.
   - Cr√©er un dataset PyTorch pour la d√©tection d'objets.
   - Entra√Æner un d√©tecteur custom.
   - Comparer avec YOLO et choisir le bon mod√®le selon le contexte.
   - Effectuer l'inf√©rence sur des images en temps r√©el.

.. slide::

üìñ 1. Classification vs D√©tection : comprendre la diff√©rence
----------------------

1.1. Classification d'images (chapitres pr√©c√©dents)
~~~~~~~~~~~~~~~~~~~

Dans les chapitres pr√©c√©dents, nous avons travaill√© sur la **classification d'images** : le mod√®le devait r√©pondre √† la question *"Qu'est-ce qu'il y a dans cette image ?"*

**Exemple** : 

- Entr√©e : une image $$224√ó224$$ pixels
- Sortie : une classe parmi N possibles (ex : "chat", "chien", "voiture")
- Une seule pr√©diction par image

.. code-block:: python

   # Classification : une image ‚Üí une classe
   output = model(image)  # Shape: [batch_size, num_classes]
   predicted_class = torch.argmax(output, dim=1)
   print(f"Cette image contient : {classes[predicted_class]}")

.. slide::

1.2. D√©tection d'objets : localiser ET classifier
~~~~~~~~~~~~~~~~~~~

La **d√©tection d'objets** va plus loin : le mod√®le doit r√©pondre √† *"Qu'est-ce qu'il y a dans cette image ET o√π se trouve chaque objet ?"*

**Pour chaque objet d√©tect√©, le mod√®le doit fournir** :

1. **La classe** de l'objet (ex : "personne", "voiture", "chien")
2. **La bo√Æte englobante** (bounding box en anglais) : 4 coordonn√©es d√©finissant un rectangle autour de l'objet
3. **Un score de confiance** : probabilit√© que la d√©tection soit correcte (0 √† 1)

**Exemple de sortie** :

.. code-block:: python

   # D√©tection : une image ‚Üí plusieurs objets localis√©s
   outputs = model(image)
   # outputs[0]['boxes']: tensor([[x1, y1, x2, y2], [x1, y1, x2, y2], ...])
   # outputs[0]['labels']: tensor([1, 3, 1, ...])  # IDs des classes
   # outputs[0]['scores']: tensor([0.95, 0.87, 0.76, ...])  # Confiances

üí° **Intuition** : imaginez que vous regardez une photo de famille. La classification dirait "photo de groupe", tandis que la d√©tection indiquerait "3 personnes aux positions (x1,y1,x2,y2), (x3,y3,x4,y4), (x5,y5,x6,y6)".

.. slide::

1.3. Qu'est-ce qu'une bo√Æte englobante ?
~~~~~~~~~~~~~~~~~~~

Une **bo√Æte englobante** est un rectangle d√©fini par 4 valeurs. Il existe plusieurs fa√ßons de repr√©senter ces coordonn√©es :

**Format 1 : (x1, y1, x2, y2)** ‚Äî> coins de la bo√Æte (PyTorch/torchvision)

- ``x1, y1`` : coordonn√©es du coin sup√©rieur gauche
- ``x2, y2`` : coordonn√©es du coin inf√©rieur droit

**Format 2 : (x, y, w, h)** ‚Äî> coin + dimensions (Label Studio format standard)

- ``x, y`` : coordonn√©es du coin sup√©rieur gauche (en % )
- ``w`` : largeur de la bo√Æte
- ``h`` : hauteur de la bo√Æte

**Format 3 : (x_center, y_center, w, h) normalis√©** (Label Studio format utilis√© pour YOLO)

- ``x_center, y_center`` : coordonn√©es du centre (normalis√©es entre 0 et 1)
- ``w, h`` : largeur et hauteur (normalis√©es entre 0 et 1)

.. code-block:: text

   Exemple d'une image $$640√ó480$$ pixels avec un objet :
   
   Format PyTorch : [100, 50, 300, 250]
   ‚Üí Rectangle du pixel (100,50) au pixel (300,250)
   
   Format Label Studio : [15.625, 10.417, 31.25, 41.67]
   ‚Üí Coin en (15.625%, 10.417%), taille 31.25%√ó41.67% de l'image
   
   Format YOLO : [0.3125, 0.3125, 0.3125, 0.4167]
   ‚Üí Centre √† 31.25% de la largeur/hauteur, bo√Æte de 31.25%√ó41.67% de l'image

.. warning::

   ‚ö†Ô∏è **Format utilis√© dans la suite du TP**
   
   Dans la suite de ce chapitre, nous utiliserons le **Format 3 (YOLO)** avec des coordonn√©es normalis√©es. C'est le format standard pour la d√©tection d'objets, compatible avec YOLO et la plupart des frameworks modernes.

.. slide:

1.4. Applications concr√®tes de la d√©tection
~~~~~~~~~~~~~~~~~~~

La d√©tection d'objets est au c≈ìur de nombreuses applications :

- **V√©hicules autonomes** : d√©tecter pi√©tons, voitures, panneaux
- **Surveillance vid√©o** : compter les personnes, d√©tecter des comportements suspects
- **Commerce** : compter les produits en rayon, d√©tecter les vols
- **M√©dical** : localiser des tumeurs, anomalies sur des radiographies
- **R√©alit√© augment√©e** : d√©tecter des objets pour y superposer des informations

üí° Dans ce chapitre, nous allons apprendre √† cr√©er notre propre d√©tecteur d'objets personnalis√©, de A √† Z !

.. slide::

üìñ 2. Pr√©parer les donn√©es : de la vid√©o aux images annot√©es
----------------------

Le pipeline complet pour cr√©er un dataset de d√©tection :

1. **Capturer une vid√©o** de l'objet √† d√©tecter
2. **Extraire des images** (frames) depuis la vid√©o
3. **Annoter** les objets sur chaque image
4. **Exporter** les annotations dans un format standard
5. **Organiser** le dataset pour l'entra√Ænement

Voyons chaque √©tape en d√©tail.

.. slide::

2.1. Capturer une vid√©o
~~~~~~~~~~~~~~~~~~~

**Objectif** : filmer l'objet que vous voulez d√©tecter sous diff√©rents angles et conditions.

**Conseils pratiques** :

- Dur√©e : 30 secondes √† 2 minutes suffisent
- Vari√©t√© : filmez l'objet sous diff√©rents angles, distances, √©clairages
- Stabilit√© : √©vitez les mouvements trop brusques
- Qualit√© : r√©solution HD ($$1280√ó720$$ ou $$1920√ó1080$$) recommand√©e

üí° **Astuce** : plus vous capturez de vari√©t√©, meilleur sera votre d√©tecteur !

.. note::

   **üí° Vous pouvez commencer avec beaucoup moins !**
   
   - ~50-100 photos extraites d'une vid√©o faite avec un smartphone suffisent pour d√©buter
   - R√©solution modeste ($$640√ó480$$ ou $$224√ó224$$) acceptable pour un prototype
   - M√™me avec peu de vari√©t√©, vous obtiendrez d√©j√† des r√©sultats !

.. slide::

2.2. Installation d'OpenCV
~~~~~~~~~~~~~~~~~~~

**OpenCV (cv2)** est une biblioth√®que Python tr√®s puissante pour manipuler des vid√©os. Elle s'utilise directement en Python sans installer d'outils externes.

.. code-block:: bash

   # Installer OpenCV dans votre environnement virtuel
   pip install opencv-python

.. slide::

2.3. Script d'extraction de base
~~~~~~~~~~~~~~~~~~~

Voici un script complet pour extraire toutes les frames d'une vid√©o :

.. code-block:: python

   import cv2
   import os

   def extraire_frames(video_path, output_dir):
       """
       Extrait toutes les frames d'une vid√©o.
       
       Args:
           video_path: chemin vers la vid√©o
           output_dir: dossier o√π sauvegarder les images
       """
       # Cr√©er le dossier de sortie (exist_ok=True √©vite l'erreur si le dossier existe d√©j√†)
       os.makedirs(output_dir, exist_ok=True)
       
       # Ouvrir la vid√©o
       cap = cv2.VideoCapture(video_path)
       
       # V√©rifier que la vid√©o s'ouvre correctement
       if not cap.isOpened():
           print(f"‚ùå Erreur : impossible d'ouvrir {video_path}")
           return

       # Obtenir les propri√©t√©s de la vid√©o (fps, nombre total de frames)
       fps = cap.get(cv2.CAP_PROP_FPS)
       total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
       
       print(f"üìπ Vid√©o : {total_frames} frames √† {fps:.2f} fps")
       
       frame_count = 0
       
       while True:
           # Lire la frame suivante
           ret, frame = cap.read()
           
           # Si plus de frames, sortir de la boucle
           if not ret:
               break
           
           # Sauvegarder la frame en jpg pour compression
           output_path = os.path.join(output_dir, f'frame_{frame_count:05d}.jpg')
           cv2.imwrite(output_path, frame)
           
           frame_count += 1
       
       # Lib√©rer les ressources
       cap.release()
       
       print(f"‚úì {frame_count} frames extraites dans {output_dir}")

   # Utilisation
   extraire_frames('ma_video.mp4', 'frames/')

.. warning::

   ‚ö†Ô∏è **Attention √† la quantit√© !**
   
   Une vid√©o de 30 secondes √† 30 fps g√©n√®re **900 images**. C'est souvent trop pour annoter manuellement !

.. slide::

2.4. Extraction intelligente (sous-√©chantillonnage)
~~~~~~~~~~~~~~~~~~~

Pour r√©duire le nombre d'images √† annoter, on extrait seulement certaines frames :

.. code-block:: python

   import cv2
   import os

   def extraire_frames_espacees(video_path, output_dir, intervalle=10):
       """
       Extrait 1 frame tous les N frames.
       
       Args:
           video_path: chemin vers la vid√©o
           output_dir: dossier de sortie
           intervalle: extraire 1 frame tous les N frames (ex: 10)
       """
       os.makedirs(output_dir, exist_ok=True)
       
       cap = cv2.VideoCapture(video_path)
       
       if not cap.isOpened():
           print(f"‚ùå Erreur : impossible d'ouvrir {video_path}")
           return
       
       fps = cap.get(cv2.CAP_PROP_FPS)
       total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
       
       print(f"üìπ Extraction de 1 frame tous les {intervalle} frames")
       print(f"   Total attendu : ~{total_frames // intervalle} images")
       
       frame_count = 0
       saved_count = 0
       
       while True:
           ret, frame = cap.read()
           if not ret:
               break
           
           # Sauvegarder seulement toutes les N frames
           if frame_count % intervalle == 0:
               output_path = os.path.join(output_dir, f'frame_{saved_count:05d}.jpg')
               cv2.imwrite(output_path, frame)
               saved_count += 1
           
           frame_count += 1
       
       cap.release()
       
       print(f"‚úì {saved_count} frames extraites sur {frame_count} totales")

   # Exemple : extraire 1 frame toutes les 10 frames
   extraire_frames_espacees('ma_video.mp4', 'frames/', intervalle=10)

**Recommandation pratique** : pour d√©buter, extraire 50-200 images est un bon compromis entre travail d'annotation et qualit√© du mod√®le.

**R√®gles de calcul de l'intervalle** :

- Vid√©o √† 30 fps, 1 frame/seconde ‚Üí ``intervalle=30``
- Vid√©o √† 30 fps, 1 frame toutes les 10 frames ‚Üí ``intervalle=10``
- Extraire ~100 images d'une vid√©o de 900 frames ‚Üí ``intervalle=9``

.. slide::

2.5. Redimensionner les images √† l'extraction
~~~~~~~~~~~~~~~~~~~

Pour √©conomiser l'espace disque et acc√©l√©rer le traitement, on peut redimensionner directement.

.. warning::

   ‚ö†Ô∏è **Attention √† la d√©formation !**
   
   Si votre vid√©o n'est **pas carr√©e** (ex : $$1920√ó1080$$) et que vous redimensionnez en **carr√©** (ex : $$224√ó224$$), l'image sera **d√©form√©e** (√©cras√©e ou √©tir√©e).
   
   **Deux solutions** :
   
   1. **Crop au centre** (RECOMMAND√â) : d√©couper un carr√© au centre avant de redimensionner
   2. **Padding** : ajouter des bordures noires pour garder le ratio

.. slide::

Voici les deux approches :

**Approche 1 : Crop au centre (recommand√©e - pas de d√©formation)**

.. warning::

   ‚ö†Ô∏è **Code utilis√© dans la suite du TP**
   
   C'est **cette fonction** (``extraire_frames_crop_redimensionner``) que nous utiliserons dans tous les exercices du chapitre. Elle √©vite les d√©formations en d√©coupant un carr√© au centre de l'image avant de redimensionner.

.. code-block:: python

   import cv2
   import os

   def extraire_frames_crop_redimensionner(video_path, output_dir, intervalle=10, 
                                           target_size=224):
       """
       Extrait, crop au centre en carr√©, puis redimensionne.
       √âVITE la d√©formation en d√©coupant l'image.
       
       Args:
           video_path: chemin vers la vid√©o
           output_dir: dossier de sortie
           intervalle: extraire 1 frame tous les N frames
           target_size: taille finale du carr√© (ex: 224 pour 224√ó224)
       """
       os.makedirs(output_dir, exist_ok=True)
       
       cap = cv2.VideoCapture(video_path)
       
       if not cap.isOpened():
           print(f"‚ùå Erreur : impossible d'ouvrir {video_path}")
           return
       
       # Obtenir les dimensions originales
       original_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
       original_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
       
       print(f"üìπ R√©solution originale : {original_width}√ó{original_height}")
       print(f"üìê Nouvelle r√©solution : {target_size}√ó{target_size} (carr√©)")
       print(f"‚úÇÔ∏è  M√©thode : Crop au centre (pas de d√©formation)")
       
       frame_count = 0
       saved_count = 0
       
       while True:
           ret, frame = cap.read()
           if not ret:
               break
           
           if frame_count % intervalle == 0:
               # √âTAPE 1 : Crop au centre pour obtenir un carr√©
               h, w = frame.shape[:2]
               size = min(h, w)  # Prendre la plus petite dimension
               
               # Calculer les coordonn√©es du crop au centre
               start_y = (h - size) // 2
               start_x = (w - size) // 2
               
               # D√©couper le carr√© au centre
               cropped = frame[start_y:start_y+size, start_x:start_x+size]
               
               # √âTAPE 2 : Redimensionner le carr√© √† la taille souhait√©e
               resized = cv2.resize(cropped, (target_size, target_size))
               
               # Sauvegarder
               output_path = os.path.join(output_dir, f'frame_{saved_count:05d}.jpg')
               cv2.imwrite(output_path, resized)
               saved_count += 1
           
           frame_count += 1
       
       cap.release()
       
       print(f"‚úì {saved_count} frames extraites, cropp√©es et redimensionn√©es")

   # Exemple : extraire 1 frame/seconde en 224√ó224 (format standard CNN)
   extraire_frames_crop_redimensionner('ma_video.mp4', 'frames/', 
                                       intervalle=30, target_size=224)


.. slide::

**Approche 2 : Redimensionnement direct (D√âCONSEILL√â si ratio diff√©rent)**

.. code-block:: python

   def extraire_frames_redimensionner_simple(video_path, output_dir, intervalle=10, 
                                             target_width=640, target_height=480):
       """
       Redimensionne directement sans crop.
       ‚ö†Ô∏è ATTENTION : d√©forme l'image si le ratio change !
       """
       os.makedirs(output_dir, exist_ok=True)
       
       cap = cv2.VideoCapture(video_path)
       
       if not cap.isOpened():
           print(f"‚ùå Erreur : impossible d'ouvrir {video_path}")
           return
       
       original_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
       original_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
       
       print(f"üìπ R√©solution originale : {original_width}√ó{original_height}")
       print(f"üìê Nouvelle r√©solution : {target_width}√ó{target_height}")
       
       # V√©rifier si le ratio va changer
       original_ratio = original_width / original_height
       target_ratio = target_width / target_height
       
       if abs(original_ratio - target_ratio) > 0.01:
           print(f"‚ö†Ô∏è  ATTENTION : Le ratio va changer !")
           print(f"    Original : {original_ratio:.2f}")
           print(f"    Cible : {target_ratio:.2f}")
           print(f"    ‚Üí L'image sera d√©form√©e !")
       
       frame_count = 0
       saved_count = 0
       
       while True:
           ret, frame = cap.read()
           if not ret:
               break
           
           if frame_count % intervalle == 0:
               # Redimensionner directement (PEUT D√âFORMER !)
               resized = cv2.resize(frame, (target_width, target_height))
               
               output_path = os.path.join(output_dir, f'frame_{saved_count:05d}.jpg')
               cv2.imwrite(output_path, resized)
               saved_count += 1
           
           frame_count += 1
       
       cap.release()
       
       print(f"‚úì {saved_count} frames extraites et redimensionn√©es")

   # Exemple : ‚ö†Ô∏è Vid√©o 16:9 ‚Üí carr√© = D√âFORMATION !
   # extraire_frames_redimensionner_simple('ma_video.mp4', 'frames/', 
   #                                        intervalle=30, 
   #                                        target_width=224, target_height=224)

üí° **Recommandations** :

- **Pour la d√©tection d'objets** : utilisez le crop au centre pour √©viter les d√©formations.
- **Pour la classification** : le crop au centre est aussi pr√©f√©rable.
- **R√©solutions recommand√©es** : $$224√ó224$$ (standard CNN), $$640√ó480$$ (compromis vitesse/qualit√©), $$800√ó600$$ (bonne qualit√©).

.. slide::

üìñ 3. Annotation avec Label Studio
----------------------

**Label Studio** est un outil open-source d'annotation collaborative qui permet de cr√©er des bo√Ætes englobantes, de g√©rer plusieurs annotateurs et d'exporter dans diff√©rents formats.

3.1. Installation et premier lancement
~~~~~~~~~~~~~~~~~~~

.. code-block:: bash

   # Installer Label Studio (dans votre environnement virtuel)
   pip install label-studio
   
   # Lancer Label Studio
   label-studio start
   
   # L'interface web s'ouvre automatiquement sur http://localhost:8080

**Premier lancement : cr√©ation du compte**

Au premier lancement, Label Studio vous demande de cr√©er un compte.

.. note::

   üí° **Travail collaboratif**
   
   Si vous souhaitez travailler en √©quipe, vous pourrez inviter vos coll√®gues via **"Invite People"** (voir section 3.5). Label Studio leur enverra automatiquement un email d'invitation.

**Si vous avez d√©j√† un compte** : entrez simplement votre email et mot de passe pour vous connecter.

**En cas de probl√®me** : si Label Studio ne s'ouvre pas automatiquement, ouvrez manuellement votre navigateur et allez sur ``http://localhost:8080``

.. slide::

3.2. Cr√©er un projet d'annotation
~~~~~~~~~~~~~~~~~~~

**√âtapes dans l'interface web** :

1. Cliquer sur "Create Project"

2. Donner un nom au projet (ex : "Detection_Bouteille")

3. **Import des donn√©es** :
   
   - Onglet "Data Import"
   - S√©lectionner tous les fichiers du dossier ``frames/`` (ou le dossier o√π vous avez extrait les images)
   - Cliquer sur "Import"

4. **Configuration de l'annotation** :
   
   - Cliquez sur votre projet pour l'ouvrir
   - Cliquez sur "Settings" (en haut √† droite ou dans le menu du projet)
   - Allez dans l'onglet "Labeling Interface"
   - Cliquez sur "Browse Templates"
   - S√©lectionnez "Computer Vision"
   - Choisissez "Object Detection with Bounding Boxes"
   - Une page s'ouvre pour d√©finir les labels (classes d'objets)

.. slide::

3.3. D√©finir les classes d'objets
~~~~~~~~~~~~~~~~~~~

Apr√®s avoir choisi le template, une interface appara√Æt o√π vous pouvez d√©finir vos labels (classes d'objets).

**M√©thode simple : ajouter des labels via l'interface**

1. Dans le champ "Add Label Name", entrez le nom de votre premi√®re classe (ex : "bouteille")
2. Cliquez sur "Add" ou appuyez sur Entr√©e
3. R√©p√©tez pour chaque classe d'objet √† d√©tecter
4. Cliquez sur "Save" pour valider

**Si vous pr√©f√©rez √©diter le code XML directement**, vous pouvez voir/modifier le code de configuration :

**Exemple pour d√©tecter des bouteilles et des gobelets** :

.. code-block:: xml

   <View>
     <Image name="image" value="$image"/>
     <RectangleLabels name="label" toName="image">
       <Label value="bouteille" background="green"/>
       <Label value="gobelet" background="blue"/>
     </RectangleLabels>
   </View>

üí° **Astuce** : commencez avec une seule classe pour simplifier. Vous pourrez toujours ajouter des classes plus tard.

.. slide::

3.4. Annoter les images
~~~~~~~~~~~~~~~~~~~

**Pour cr√©er une annotation** :

1. Cliquer sur une t√¢che (image) dans la liste
2. S√©lectionner la classe dans le panneau en bas de l'image (ex : "bouteille")
3. Dessiner un rectangle autour de l'objet :
   
   - Cliquer et maintenir le bouton de la souris
   - D√©placer pour cr√©er le rectangle
   - Rel√¢cher quand l'objet est bien encadr√©

4. R√©p√©ter pour tous les objets de l'image
5. Cliquer sur "Submit" pour valider l'annotation

**Modifier une annotation existante** :

- **Double-cliquer** sur un rectangle pour le s√©lectionner
- Vous pouvez alors :
  
  - **D√©placer** le rectangle en le faisant glisser
  - **Redimensionner** en tirant sur les coins ou les bords
  - **Changer la classe** dans le panneau de droite
  - **Supprimer** avec la touche ``Suppr``

**Bonnes pratiques d'annotation** :

- La bo√Æte doit englober **tout l'objet** visible (pas trop serr√©e, pas trop large)
- Si un objet est **partiellement visible** (coup√© par le bord), l'annoter quand m√™me
- Si un objet est **tr√®s petit** (<10 pixels), c'est optionnel (difficiles √† d√©tecter)
- **Coh√©rence** : gardez le m√™me style d'annotation d'une image √† l'autre

.. slide::

3.5. Annotation collaborative : inviter des personnes
~~~~~~~~~~~~~~~~~~~

Pour travailler en √©quipe sur l'annotation, suivez ces √©tapes :

**√âtape 1 : Inviter des personnes**

1. Dans Label Studio, cliquez sur l'ic√¥ne **Organization** (en haut √† droite, ic√¥ne avec plusieurs personnes)

2. Allez dans l'onglet **"People"**

3. Cliquez sur le bouton **"Invite People"**

4. Entrez les adresses email de vos coll√®gues (ex : ``marie.dubois@exemple.com``, ``paul.martin@exemple.com``)

5. Choisissez le r√¥le pour chaque personne :
   
   - **Annotator** : peut uniquement annoter les images
   - **Reviewer** : peut annoter ET valider/corriger les annotations des autres
   - **Manager** : peut g√©rer les projets et les param√®tres

6. Cliquez sur **"Send Invitations"**

7. Vos coll√®gues recevront un email avec un lien pour cr√©er leur compte

.. slide::

**√âtape 2 : Ajouter les membres √† votre projet**

Une fois les invitations accept√©es :

1. Ouvrez votre projet d'annotation
2. Allez dans **"Settings"** ‚Üí **"Members"**
3. Cliquez sur **"Add Member"**
4. S√©lectionnez les personnes dans la liste
5. Assignez-leur le r√¥le appropri√© pour ce projet

**√âtape 3 : R√©partir le travail (optionnel mais recommand√©)**

Pour √©viter que deux personnes annotent les m√™mes images :

1. Dans le projet, onglet **"Tasks"** (liste des images)
2. S√©lectionnez un groupe d'images (ex : images 1-50)
3. Menu **"Actions"** ‚Üí **"Assign Annotators"**
4. Choisissez la personne
5. R√©p√©tez pour les autres groupes d'images

**Exemple de workflow collaboratif** :

- **Marie** (Annotator) : images 1-50
- **Paul** (Annotator) : images 51-100
- **Sophie** (Reviewer) : v√©rifie et corrige toutes les annotations
- **Vous** (Manager) : supervise et exporte les donn√©es finales

üí° **Astuce qualit√©** : faites annoter 10 images par deux personnes diff√©rentes et comparez. Un IoU (Intersection over Union) > 0.7 indique une bonne coh√©rence entre annotateurs.

.. slide::

3.6. Raccourcis clavier utiles
~~~~~~~~~~~~~~~~~~~

Pour acc√©l√©rer l'annotation :

- ``1, 2, 3...`` : s√©lectionner la classe 1, 2, 3...
- ``Ctrl + Enter`` ou ``Cmd + Enter`` : soumettre et passer √† l'image suivante
- ``Ctrl + Z`` : annuler la derni√®re action
- ``Suppr`` : supprimer la bo√Æte s√©lectionn√©e
- ``Fl√®ches`` : ajuster finement la position d'une bo√Æte

.. slide::

üìñ 4. Exporter depuis Label Studio au format YOLO
----------------------

Apr√®s avoir termin√© l'annotation de vos images dans Label Studio, vous devez exporter les donn√©es pour l'entra√Ænement. Dans ce chapitre, nous allons utiliser le format **"YOLO with Images"**.

.. slide::

4.1. Qu'est-ce que le format "YOLO with Images" ?
~~~~~~~~~~~~~~~~~~~

Le format **"YOLO with Images"** est un export complet propos√© par Label Studio qui contient :

1. **Un dossier ``images/``** : toutes vos images annot√©es
2. **Un dossier ``labels/``** : un fichier texte ``.txt`` par image contenant les annotations
3. **Un fichier ``classes.txt``** : la liste des noms de classes (un par ligne)
4. **Un fichier ``notes.json``** : m√©tadonn√©es sur l'export

**C'est le format id√©al** car il regroupe tout ce dont vous avez besoin pour l'entra√Ænement dans une seule archive ZIP.

.. slide::

4.2. Structure du format YOLO
~~~~~~~~~~~~~~~~~~~

**Format d'annotation YOLO** : un fichier texte par image avec des coordonn√©es normalis√©es.

**Exemple de fichier** ``labels/frame_00001.txt`` :

.. code-block:: text

   0 0.3125 0.3125 0.3125 0.4167
   1 0.6250 0.5417 0.1562 0.2500

**Format d'une ligne** : ``class_id x_center y_center width height``

**Toutes les valeurs sont normalis√©es entre 0 et 1** :

- ``class_id`` : entier (0, 1, 2...) correspondant √† l'index de la classe
- ``x_center`` : position X du centre de la bo√Æte / largeur de l'image
- ``y_center`` : position Y du centre de la bo√Æte / hauteur de l'image
- ``width`` : largeur de la bo√Æte / largeur de l'image
- ``height`` : hauteur de la bo√Æte / hauteur de l'image

.. slide::

**Exemple concret** (image $$640√ó480$$, objet de 100,50 √† 300,200) :

.. code-block:: python

   # Coordonn√©es en pixels (format classique)
   x1, y1, x2, y2 = 100, 50, 300, 200
   img_width, img_height = 640, 480
   
   # Conversion en format YOLO
   x_center = ((x1 + x2) / 2) / img_width   # Centre X : (100+300)/2 / 640 = 0.3125
   y_center = ((y1 + y2) / 2) / img_height  # Centre Y : (50+200)/2 / 480 = 0.2604
   width = (x2 - x1) / img_width            # Largeur : (300-100) / 640 = 0.3125
   height = (y2 - y1) / img_height          # Hauteur : (200-50) / 480 = 0.3125
   
   # R√©sultat : "0 0.3125 0.2604 0.3125 0.3125"

**Structure compl√®te apr√®s export** :

.. code-block:: text

   dataset_yolo/
   ‚îú‚îÄ‚îÄ images/              # Toutes vos images annot√©es
   ‚îÇ   ‚îú‚îÄ‚îÄ frame_00001.jpg
   ‚îÇ   ‚îú‚îÄ‚îÄ frame_00002.jpg
   ‚îÇ   ‚îî‚îÄ‚îÄ ...
   ‚îú‚îÄ‚îÄ labels/              # Fichiers .txt (m√™me nom que l'image)
   ‚îÇ   ‚îú‚îÄ‚îÄ frame_00001.txt
   ‚îÇ   ‚îú‚îÄ‚îÄ frame_00002.txt
   ‚îÇ   ‚îî‚îÄ‚îÄ ...
   ‚îú‚îÄ‚îÄ classes.txt          # Liste des classes : une par ligne
   ‚îî‚îÄ‚îÄ notes.json           # M√©tadonn√©es (optionnel)

**Fichier** ``classes.txt`` **exemple** :

.. code-block:: text

   cube
   bouteille
   gobelet

üí° **L'ordre des classes dans** ``classes.txt`` **d√©finit les IDs** : cube=0, bouteille=1, gobelet=2.

.. slide::

4.3. √âtapes pour exporter depuis Label Studio
~~~~~~~~~~~~~~~~~~~

**1. Acc√©der √† l'export**

- Ouvrez votre projet dans Label Studio
- En haut de la page, cliquez sur le bouton **"Export"**

**2. Choisir le format**

- Dans la liste des formats d'export, s√©lectionnez : **"YOLO"**
- Label Studio g√©n√®re automatiquement l'archive

**3. T√©l√©charger l'archive**

- Cliquez sur **"Export"** pour t√©l√©charger le fichier ZIP
- Le fichier se nomme g√©n√©ralement ``project-X-at-YYYY-MM-DD-HH-MM-XX.zip``

**4. Extraire l'archive**

.. code-block:: bash

   # D√©compresser l'archive
   unzip project-1-at-2024-01-15-14-30-00.zip -d dataset_yolo/
   
   # V√©rifier le contenu
   ls -R dataset_yolo/
   
   # Vous devriez voir :
   # dataset_yolo/
   # ‚îú‚îÄ‚îÄ images/
   # ‚îú‚îÄ‚îÄ labels/
   # ‚îú‚îÄ‚îÄ classes.txt
   # ‚îî‚îÄ‚îÄ notes.json

.. slide::

4.4. V√©rifier l'export
~~~~~~~~~~~~~~~~~~~

Avant de commencer l'entra√Ænement, v√©rifiez toujours que l'export est correct :

.. code-block:: python

   import os

   def verify_yolo_export(dataset_path):
       """V√©rifie la structure d'un export YOLO."""
       
       images_dir = os.path.join(dataset_path, 'images')
       labels_dir = os.path.join(dataset_path, 'labels')
       classes_file = os.path.join(dataset_path, 'classes.txt')
       
       # V√©rifier que les dossiers existent
       assert os.path.exists(images_dir), "‚ùå Dossier 'images/' manquant"
       assert os.path.exists(labels_dir), "‚ùå Dossier 'labels/' manquant"
       assert os.path.exists(classes_file), "‚ùå Fichier 'classes.txt' manquant"
       
       # Compter les fichiers
       images = [f for f in os.listdir(images_dir) if f.endswith(('.jpg', '.png'))]
       labels = [f for f in os.listdir(labels_dir) if f.endswith('.txt')]
       
       print(f"‚úÖ Structure YOLO valide")
       print(f"   üìÅ Images : {len(images)}")
       print(f"   üìÅ Labels : {len(labels)}")
       
       # Charger les classes
       with open(classes_file, 'r') as f:
           classes = [line.strip() for line in f.readlines()]
       print(f"   üìã Classes ({len(classes)}) : {classes}")
       
       # V√©rifier la correspondance images/labels
       missing_labels = []
       for img in images:
           label_name = os.path.splitext(img)[0] + '.txt'
           if label_name not in labels:
               missing_labels.append(img)
       
       if missing_labels:
           print(f"\n‚ö†Ô∏è  {len(missing_labels)} images sans annotation :")
           for img in missing_labels[:5]:
               print(f"      - {img}")
       else:
           print(f"\n‚úÖ Toutes les images ont leurs annotations")
       
       # V√©rifier un fichier d'annotation
       if labels:
           sample_label = os.path.join(labels_dir, labels[0])
           with open(sample_label, 'r') as f:
               lines = f.readlines()
           print(f"\nüìÑ Exemple d'annotation ({labels[0]}) :")
           for line in lines[:3]:
               print(f"      {line.strip()}")

   # üéØ UTILISATION
   verify_yolo_export('dataset_yolo/')

.. slide::

üìñ 5. Cr√©er un Dataset PyTorch pour le format YOLO
----------------------

Maintenant que vous avez export√© votre dataset au format YOLO, cr√©ons un Dataset PyTorch personnalis√© pour le charger.

5.1. Structure de dossiers YOLO
~~~~~~~~~~~~~~~~~~~

Apr√®s extraction de l'archive ZIP, votre dataset doit avoir cette structure :

.. code-block:: text

   dataset_yolo/
   ‚îú‚îÄ‚îÄ images/              # Toutes vos images annot√©es
   ‚îÇ   ‚îú‚îÄ‚îÄ frame_00001.jpg
   ‚îÇ   ‚îú‚îÄ‚îÄ frame_00002.jpg
   ‚îÇ   ‚îî‚îÄ‚îÄ ...
   ‚îú‚îÄ‚îÄ labels/              # Fichiers .txt (m√™me nom que l'image)
   ‚îÇ   ‚îú‚îÄ‚îÄ frame_00001.txt
   ‚îÇ   ‚îú‚îÄ‚îÄ frame_00002.txt
   ‚îÇ   ‚îî‚îÄ‚îÄ ...
   ‚îî‚îÄ‚îÄ classes.txt          # Liste des classes (une par ligne)

.. slide::

5.2. Classe YOLODetectionDataset
~~~~~~~~~~~~~~~~~~~

Voici une impl√©mentation compl√®te qui charge le format YOLO et g√®re intelligemment le redimensionnement :

.. code-block:: python

   import torch
   from torch.utils.data import Dataset
   from PIL import Image
   import os
   from torchvision import transforms

   class YOLODetectionDataset(Dataset):
       """
       Dataset PyTorch pour le format YOLO (images + labels .txt).
       Compatible avec l'export YOLO de Label Studio.
       """
       
       def __init__(self, images_dir, labels_dir, classes_file, img_size=224, custom_transforms=None):
           """
           Args:
               images_dir: dossier contenant les images
               labels_dir: dossier contenant les labels .txt au format YOLO
               classes_file: chemin vers classes.txt
               img_size: taille de redimensionnement (d√©faut: 224x224)
               custom_transforms: transformations √† appliquer (optionnel)
           """
           self.images_dir = images_dir
           self.labels_dir = labels_dir
           self.img_size = img_size
           self.custom_transforms = custom_transforms
           
           # Pas de transformation automatique - on g√©rera le resize manuellement
           # pour ajuster les coordonn√©es des bounding boxes en cons√©quence
           self.to_tensor = transforms.ToTensor()
           
           # Charger les noms de classes
           with open(classes_file, 'r') as f:
               self.classes = [line.strip() for line in f.readlines()]
           
           # Liste des images (on suppose que chaque image a son label correspondant)
           self.image_files = sorted([f for f in os.listdir(images_dir) 
                                      if f.endswith(('.jpg', '.jpeg', '.png'))])
           
           print(f"‚úÖ Dataset YOLO initialis√©:")
           print(f"   - {len(self.image_files)} images")
           print(f"   - {len(self.classes)} classes : {self.classes}")
       
       def __len__(self):
           return len(self.image_files)
       
       def __getitem__(self, idx):
           """
           Charge une image et ses annotations au format YOLO.
           
           Returns:
               img: tensor [3, H, W]
               target: dict avec 'boxes' (format [x1, y1, x2, y2] en pixels), 
                       'labels', 'image_id'
           """
           # Charger l'image
           img_filename = self.image_files[idx]
           img_path = os.path.join(self.images_dir, img_filename)
           img = Image.open(img_path).convert('RGB')
           orig_width, orig_height = img.size
           
           # Calculer le ratio de resize pour garder l'aspect ratio
           scale = self.img_size / max(orig_width, orig_height)
           new_width = int(orig_width * scale)
           new_height = int(orig_height * scale)
           
           # Resize en gardant l'aspect ratio
           img = img.resize((new_width, new_height), Image.BILINEAR)
           
           # Cr√©er une image carr√©e avec padding noir
           padded_img = Image.new('RGB', (self.img_size, self.img_size), (0, 0, 0))
           # Centrer l'image resiz√©e
           paste_x = (self.img_size - new_width) // 2
           paste_y = (self.img_size - new_height) // 2
           padded_img.paste(img, (paste_x, paste_y))
           
           # Charger le label correspondant
           label_filename = os.path.splitext(img_filename)[0] + '.txt'
           label_path = os.path.join(self.labels_dir, label_filename)
           
           boxes = []
           labels = []
           
           # Lire le fichier de labels si il existe
           if os.path.exists(label_path):
               with open(label_path, 'r') as f:
                   for line in f:
                       parts = line.strip().split()
                       if len(parts) == 5:
                           class_id = int(parts[0])
                           x_center = float(parts[1])
                           y_center = float(parts[2])
                           width = float(parts[3])
                           height = float(parts[4])
                           
                           # Convertir du format YOLO normalis√© vers pixels dans l'image originale
                           x_center_orig = x_center * orig_width
                           y_center_orig = y_center * orig_height
                           width_orig = width * orig_width
                           height_orig = height * orig_height
                           
                           # Appliquer le scale et le padding
                           x_center_scaled = x_center_orig * scale + paste_x
                           y_center_scaled = y_center_orig * scale + paste_y
                           width_scaled = width_orig * scale
                           height_scaled = height_orig * scale
                           
                           # Convertir en format [x1, y1, x2, y2]
                           x1 = x_center_scaled - width_scaled / 2
                           y1 = y_center_scaled - height_scaled / 2
                           x2 = x_center_scaled + width_scaled / 2
                           y2 = y_center_scaled + height_scaled / 2
                           
                           boxes.append([x1, y1, x2, y2])
                           labels.append(class_id + 1)  # +1 car background=0 dans certains mod√®les
           
           # Convertir en tenseurs
           boxes = torch.as_tensor(boxes, dtype=torch.float32)
           labels = torch.as_tensor(labels, dtype=torch.int64)
           
           # Cr√©er le dictionnaire target
           target = {}
           target['boxes'] = boxes
           target['labels'] = labels
           target['image_id'] = torch.tensor([idx])
           
           # Si aucune bo√Æte, cr√©er des tenseurs vides
           if len(boxes) == 0:
               target['boxes'] = torch.zeros((0, 4), dtype=torch.float32)
               target['labels'] = torch.zeros((0,), dtype=torch.int64)
           
           # Convertir l'image en tensor
           img = self.to_tensor(padded_img)
           
           # Appliquer les transformations personnalis√©es si fournies
           if self.custom_transforms:
               img = self.custom_transforms(img)
           
           return img, target
       
       def get_class_name(self, class_id):
           """Retourne le nom d'une classe depuis son ID (class_id - 1 car on a ajout√© 1)."""
           return self.classes[class_id - 1]

.. note::

   üí° **Gestion intelligente du redimensionnement**
   
   - L'image est redimensionn√©e **proportionnellement** pour √©viter toute d√©formation
   - Un **padding noir** est ajout√© pour cr√©er une image carr√©e
   - Les **coordonn√©es des bounding boxes** sont automatiquement ajust√©es
   - Les IDs de classes commencent √† **1** (0 r√©serv√© au background)

.. slide::

5.3. Cr√©er les DataLoaders avec split automatique
~~~~~~~~~~~~~~~~~~~

Chargez le dataset et cr√©ez les splits train/val/test :

.. code-block:: python

   from torch.utils.data import DataLoader, random_split

   # Charger le dataset YOLO
   full_dataset = YOLODetectionDataset(
       images_dir='dataset_yolo/images',
       labels_dir='dataset_yolo/labels',
       classes_file='dataset_yolo/classes.txt'
   )

   # Split : 70% train, 15% val, 15% test
   total_size = len(full_dataset)
   train_size = int(0.70 * total_size)
   val_size = int(0.15 * total_size)
   test_size = total_size - train_size - val_size

   train_dataset, val_dataset, test_dataset = random_split(
       full_dataset, 
       [train_size, val_size, test_size],
       generator=torch.Generator().manual_seed(42)
   )

   print(f"\nüìÇ Split du dataset :")
   print(f"   Train : {len(train_dataset)} images")
   print(f"   Val   : {len(val_dataset)} images")
   print(f"   Test  : {len(test_dataset)} images")

   # Cr√©er les dataloaders
   def collate_fn(batch):
       """Fonction n√©cessaire car chaque image a un nombre diff√©rent d'objets."""
       return tuple(zip(*batch))

   train_loader = DataLoader(
       train_dataset,
       batch_size=4,
       shuffle=True,
       num_workers=2,
       collate_fn=collate_fn
   )

   val_loader = DataLoader(
       val_dataset,
       batch_size=4,
       shuffle=False,
       num_workers=2,
       collate_fn=collate_fn
   )

   test_loader = DataLoader(
       test_dataset,
       batch_size=4,
       shuffle=False,
       num_workers=2,
       collate_fn=collate_fn
   )

   print(f"\n‚úÖ DataLoaders cr√©√©s avec batch_size=4")

üí° **Avantage de** ``random_split`` : pas besoin de cr√©er manuellement les listes d'images pour chaque split !

.. slide::

5.4. Visualiser les donn√©es charg√©es
~~~~~~~~~~~~~~~~~~~

Toujours v√©rifier visuellement que le Dataset charge correctement les images et bounding boxes :

.. code-block:: python

   import matplotlib.pyplot as plt
   import matplotlib.patches as patches
   import numpy as np

   def visualize_yolo_batch(dataset, num_samples=4):
       """Affiche quelques exemples du dataset avec leurs bounding boxes."""
       fig, axes = plt.subplots(2, 2, figsize=(12, 12))
       axes = axes.flatten()
       
       for i in range(num_samples):
           img, target = dataset[i]
           
           # Convertir le tensor en numpy pour l'affichage
           img_np = img.permute(1, 2, 0).numpy()
           
           ax = axes[i]
           ax.imshow(img_np)
           ax.axis('off')
           
           # Dessiner les bounding boxes
           boxes = target['boxes'].numpy()
           labels = target['labels'].numpy()
           
           for box, label in zip(boxes, labels):
               x1, y1, x2, y2 = box
               width = x2 - x1
               height = y2 - y1
               
               # Cr√©er le rectangle
               rect = patches.Rectangle(
                   (x1, y1), width, height,
                   linewidth=2, edgecolor='red', facecolor='none'
               )
               ax.add_patch(rect)
               
               # Ajouter le label
               class_name = dataset.dataset.get_class_name(label) if hasattr(dataset, 'dataset') else f"Classe {label}"
               ax.text(x1, y1-5, class_name, 
                      bbox=dict(boxstyle='round', facecolor='red', alpha=0.7),
                      fontsize=10, color='white')
           
           ax.set_title(f'Image {i}')
       
       plt.tight_layout()
       plt.show()

   # Visualiser quelques exemples du train set
   print("üì∏ Visualisation d'exemples du training set:\n")
   visualize_yolo_batch(train_dataset, num_samples=4)

**Points √† v√©rifier** :

- ‚úÖ Les images sont bien carr√©es ($$224√ó224$$ par d√©faut)
- ‚úÖ Les bounding boxes englobent correctement les objets
- ‚úÖ Les labels affich√©s correspondent aux objets
- ‚úÖ Pas de d√©formation visible des objets

üí° **Astuce** : si les bounding boxes ne correspondent pas aux objets, v√©rifiez que les fichiers ``.txt`` dans ``labels/`` ont les m√™mes noms que les images.

