.. slide::

Chapitre 6 ‚Äî D√©tection d'objets avec des bo√Ætes englobantes
================

üéØ Objectifs du Chapitre
----------------------

.. important::

   √Ä la fin de ce chapitre, vous saurez : 

   - Comprendre la diff√©rence entre classification et d√©tection d'objets.
   - Extraire des images depuis une vid√©o.
   - Utiliser Label Studio pour annoter des objets avec des bo√Ætes englobantes de mani√®re collaborative.
   - Comprendre et manipuler les formats d'annotations.
   - Cr√©er un dataset PyTorch pour la d√©tection d'objets.
   - Entra√Æner un d√©tecteur custom.
   - Comparer avec YOLO et choisir le bon mod√®le selon le contexte.
   - Effectuer l'inf√©rence sur des images en temps r√©el.

.. slide::

üìñ 1. Classification vs D√©tection : comprendre la diff√©rence
----------------------

1.1. Classification d'images (chapitres pr√©c√©dents)
~~~~~~~~~~~~~~~~~~~

Dans les chapitres pr√©c√©dents, nous avons travaill√© sur la **classification d'images** : le mod√®le devait r√©pondre √† la question *"Qu'est-ce qu'il y a dans cette image ?"*

**Exemple** : 

- Entr√©e : une image $$224√ó224$$ pixels
- Sortie : une classe parmi N possibles (ex : "chat", "chien", "voiture")
- Une seule pr√©diction par image

.. code-block:: python

   # Classification : une image ‚Üí une classe
   output = model(image)  # Shape: [batch_size, num_classes]
   predicted_class = torch.argmax(output, dim=1)
   print(f"Cette image contient : {classes[predicted_class]}")

.. slide::

1.2. D√©tection d'objets : localiser ET classifier
~~~~~~~~~~~~~~~~~~~

La **d√©tection d'objets** va plus loin : le mod√®le doit r√©pondre √† *"Qu'est-ce qu'il y a dans cette image ET o√π se trouve chaque objet ?"*

**Pour chaque objet d√©tect√©, le mod√®le doit fournir** :

1. **La classe** de l'objet (ex : "personne", "voiture", "chien")
2. **La bo√Æte englobante** (bounding box en anglais) : 4 coordonn√©es d√©finissant un rectangle autour de l'objet
3. **Un score de confiance** : probabilit√© que la d√©tection soit correcte (0 √† 1)

**Exemple de sortie** :

.. code-block:: python

   # D√©tection : une image ‚Üí plusieurs objets localis√©s
   outputs = model(image)
   # outputs[0]['boxes']: tensor([[x1, y1, x2, y2], [x1, y1, x2, y2], ...])
   # outputs[0]['labels']: tensor([1, 3, 1, ...])  # IDs des classes
   # outputs[0]['scores']: tensor([0.95, 0.87, 0.76, ...])  # Confiances

üí° **Intuition** : imaginez que vous regardez une photo de famille. La classification dirait "photo de groupe", tandis que la d√©tection indiquerait "3 personnes aux positions (x1,y1,x2,y2), (x3,y3,x4,y4), (x5,y5,x6,y6)".

.. slide::

1.3. Qu'est-ce qu'une bo√Æte englobante ?
~~~~~~~~~~~~~~~~~~~

Une **bo√Æte englobante** est un rectangle d√©fini par 4 valeurs. Il existe plusieurs formats :

**Format 1 : (x1, y1, x2, y2)** ‚Äî> utilis√© par PyTorch/torchvision

- ``x1, y1`` : coordonn√©es du coin sup√©rieur gauche
- ``x2, y2`` : coordonn√©es du coin inf√©rieur droit

**Format 2 : (x, y, w, h)** ‚Äî> utilis√© par COCO

- ``x, y`` : coordonn√©es du coin sup√©rieur gauche
- ``w`` : largeur de la bo√Æte
- ``h`` : hauteur de la bo√Æte

**Format 3 : (x_center, y_center, w, h) normalis√©** ‚Äî> utilis√© par YOLO

- ``x_center, y_center`` : coordonn√©es du centre (normalis√©es entre 0 et 1)
- ``w, h`` : largeur et hauteur (normalis√©es entre 0 et 1)

.. code-block:: text

   Exemple d'une image $$640√ó480$$ pixels avec un objet :
   
   Format PyTorch : [100, 50, 300, 250]
   ‚Üí Rectangle du pixel (100,50) au pixel (300,250)
   
   Format COCO : [100, 50, 200, 200]
   ‚Üí Rectangle d√©marrant en (100,50) de taille $$200√ó200$$
   
   Format YOLO : [0.3125, 0.3125, 0.3125, 0.4167]
   ‚Üí Centre √† 31.25% de la largeur/hauteur, bo√Æte de 31.25%√ó41.67% de l'image

.. slide::

1.4. Applications concr√®tes de la d√©tection
~~~~~~~~~~~~~~~~~~~

La d√©tection d'objets est au c≈ìur de nombreuses applications :

- **V√©hicules autonomes** : d√©tecter pi√©tons, voitures, panneaux
- **Surveillance vid√©o** : compter les personnes, d√©tecter des comportements suspects
- **Commerce** : compter les produits en rayon, d√©tecter les vols
- **M√©dical** : localiser des tumeurs, anomalies sur des radiographies
- **R√©alit√© augment√©e** : d√©tecter des objets pour y superposer des informations

üí° Dans ce chapitre, nous allons apprendre √† cr√©er notre propre d√©tecteur d'objets personnalis√©, de A √† Z !

.. slide::

üìñ 2. Pr√©parer les donn√©es : de la vid√©o aux images annot√©es
----------------------

Le pipeline complet pour cr√©er un dataset de d√©tection :

1. **Capturer une vid√©o** de l'objet √† d√©tecter
2. **Extraire des images** (frames) depuis la vid√©o
3. **Annoter** les objets sur chaque image
4. **Exporter** les annotations dans un format standard
5. **Organiser** le dataset pour l'entra√Ænement

Voyons chaque √©tape en d√©tail.

.. slide::

2.1. Capturer une vid√©o
~~~~~~~~~~~~~~~~~~~

**Objectif** : filmer l'objet que vous voulez d√©tecter sous diff√©rents angles et conditions.

**Conseils pratiques** :

- Dur√©e : 30 secondes √† 2 minutes suffisent
- Vari√©t√© : filmez l'objet sous diff√©rents angles, distances, √©clairages
- Stabilit√© : √©vitez les mouvements trop brusques
- Qualit√© : r√©solution HD ($$1280√ó720$$ ou $$1920√ó1080$$) recommand√©e

**Exemple** : pour d√©tecter une bouteille d'eau :

- Filmez la bouteille sur un bureau (30 sec)
- Filmez-la dans une main (20 sec)
- Filmez-la avec diff√©rents arri√®re-plans (30 sec)

üí° **Astuce** : plus vous capturez de vari√©t√©, meilleur sera votre d√©tecteur !

.. note::

   **üí° Vous pouvez commencer avec beaucoup moins !**
   
   - 10-20 photos de votre smartphone suffisent pour d√©buter
   - Pas besoin de vid√©o : des images fixes fonctionnent tr√®s bien
   - R√©solution modeste (640√ó480) acceptable pour un prototype
   - M√™me avec peu de vari√©t√©, vous obtiendrez d√©j√† des r√©sultats !

.. slide::

.. slide::

2.2. Installation d'OpenCV
~~~~~~~~~~~~~~~~~~~

**OpenCV (cv2)** est une biblioth√®que Python tr√®s puissante pour manipuler des vid√©os. Elle s'utilise directement en Python sans installer d'outils externes.

.. code-block:: bash

   # Installer OpenCV dans votre environnement virtuel
   pip install opencv-python

.. slide::

2.3. Script d'extraction de base
~~~~~~~~~~~~~~~~~~~

Voici un script complet pour extraire toutes les frames d'une vid√©o :

.. code-block:: python

   import cv2
   import os

   def extraire_frames(video_path, output_dir):
       """
       Extrait toutes les frames d'une vid√©o.
       
       Args:
           video_path: chemin vers la vid√©o
           output_dir: dossier o√π sauvegarder les images
       """
       # Cr√©er le dossier de sortie (exist_ok=True √©vite l'erreur si le dossier existe d√©j√†)
       os.makedirs(output_dir, exist_ok=True)
       
       # Ouvrir la vid√©o
       cap = cv2.VideoCapture(video_path)
       
       # V√©rifier que la vid√©o s'ouvre correctement
       if not cap.isOpened():
           print(f"‚ùå Erreur : impossible d'ouvrir {video_path}")
           return

       # Obtenir les propri√©t√©s de la vid√©o (fps, nombre total de frames)
       fps = cap.get(cv2.CAP_PROP_FPS)
       total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
       
       print(f"üìπ Vid√©o : {total_frames} frames √† {fps:.2f} fps")
       
       frame_count = 0
       
       while True:
           # Lire la frame suivante
           ret, frame = cap.read()
           
           # Si plus de frames, sortir de la boucle
           if not ret:
               break
           
           # Sauvegarder la frame en jpg pour compression
           output_path = os.path.join(output_dir, f'frame_{frame_count:05d}.jpg')
           cv2.imwrite(output_path, frame)
           
           frame_count += 1
       
       # Lib√©rer les ressources
       cap.release()
       
       print(f"‚úì {frame_count} frames extraites dans {output_dir}")

   # Utilisation
   extraire_frames('ma_video.mp4', 'frames/')

.. warning::

   ‚ö†Ô∏è **Attention √† la quantit√© !**
   
   Une vid√©o de 30 secondes √† 30 fps g√©n√®re **900 images**. C'est souvent trop pour annoter manuellement !

.. slide::

2.4. Extraction intelligente (sous-√©chantillonnage)
~~~~~~~~~~~~~~~~~~~

Pour r√©duire le nombre d'images √† annoter, on extrait seulement certaines frames :

.. code-block:: python

   import cv2
   import os

   def extraire_frames_espacees(video_path, output_dir, intervalle=10):
       """
       Extrait 1 frame tous les N frames.
       
       Args:
           video_path: chemin vers la vid√©o
           output_dir: dossier de sortie
           intervalle: extraire 1 frame tous les N frames (ex: 10)
       """
       os.makedirs(output_dir, exist_ok=True)
       
       cap = cv2.VideoCapture(video_path)
       
       if not cap.isOpened():
           print(f"‚ùå Erreur : impossible d'ouvrir {video_path}")
           return
       
       fps = cap.get(cv2.CAP_PROP_FPS)
       total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
       
       print(f"üìπ Extraction de 1 frame tous les {intervalle} frames")
       print(f"   Total attendu : ~{total_frames // intervalle} images")
       
       frame_count = 0
       saved_count = 0
       
       while True:
           ret, frame = cap.read()
           if not ret:
               break
           
           # Sauvegarder seulement toutes les N frames
           if frame_count % intervalle == 0:
               output_path = os.path.join(output_dir, f'frame_{saved_count:05d}.jpg')
               cv2.imwrite(output_path, frame)
               saved_count += 1
           
           frame_count += 1
       
       cap.release()
       
       print(f"‚úì {saved_count} frames extraites sur {frame_count} totales")

   # Exemple : extraire 1 frame toutes les 10 frames
   extraire_frames_espacees('ma_video.mp4', 'frames/', intervalle=10)

**Recommandation pratique** : pour d√©buter, extraire 50-200 images est un bon compromis entre travail d'annotation et qualit√© du mod√®le.

**R√®gles de calcul de l'intervalle** :

- Vid√©o √† 30 fps, 1 frame/seconde ‚Üí ``intervalle=30``
- Vid√©o √† 30 fps, 1 frame toutes les 10 frames ‚Üí ``intervalle=10``
- Extraire ~100 images d'une vid√©o de 900 frames ‚Üí ``intervalle=9``

.. slide::

2.5. Redimensionner les images √† l'extraction
~~~~~~~~~~~~~~~~~~~

Pour √©conomiser l'espace disque et acc√©l√©rer le traitement, on peut redimensionner directement.

.. warning::

   ‚ö†Ô∏è **Attention √† la d√©formation !**
   
   Si votre vid√©o n'est **pas carr√©e** (ex : $$1920√ó1080$$) et que vous redimensionnez en **carr√©** (ex : $$224√ó224$$), l'image sera **d√©form√©e** (√©cras√©e ou √©tir√©e).
   
   **Deux solutions** :
   
   1. **Crop au centre** (RECOMMAND√â) : d√©couper un carr√© au centre avant de redimensionner
   2. **Padding** : ajouter des bordures noires pour garder le ratio

.. slide::

Voici les deux approches :

**Approche 1 : Crop au centre (recommand√©e - pas de d√©formation)**

.. code-block:: python

   import cv2
   import os

   def extraire_frames_crop_redimensionner(video_path, output_dir, intervalle=10, 
                                           target_size=224):
       """
       Extrait, crop au centre en carr√©, puis redimensionne.
       √âVITE la d√©formation en d√©coupant l'image.
       
       Args:
           video_path: chemin vers la vid√©o
           output_dir: dossier de sortie
           intervalle: extraire 1 frame tous les N frames
           target_size: taille finale du carr√© (ex: 224 pour 224√ó224)
       """
       os.makedirs(output_dir, exist_ok=True)
       
       cap = cv2.VideoCapture(video_path)
       
       if not cap.isOpened():
           print(f"‚ùå Erreur : impossible d'ouvrir {video_path}")
           return
       
       # Obtenir les dimensions originales
       original_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
       original_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
       
       print(f"üìπ R√©solution originale : {original_width}√ó{original_height}")
       print(f"üìê Nouvelle r√©solution : {target_size}√ó{target_size} (carr√©)")
       print(f"‚úÇÔ∏è  M√©thode : Crop au centre (pas de d√©formation)")
       
       frame_count = 0
       saved_count = 0
       
       while True:
           ret, frame = cap.read()
           if not ret:
               break
           
           if frame_count % intervalle == 0:
               # √âTAPE 1 : Crop au centre pour obtenir un carr√©
               h, w = frame.shape[:2]
               size = min(h, w)  # Prendre la plus petite dimension
               
               # Calculer les coordonn√©es du crop au centre
               start_y = (h - size) // 2
               start_x = (w - size) // 2
               
               # D√©couper le carr√© au centre
               cropped = frame[start_y:start_y+size, start_x:start_x+size]
               
               # √âTAPE 2 : Redimensionner le carr√© √† la taille souhait√©e
               resized = cv2.resize(cropped, (target_size, target_size))
               
               # Sauvegarder
               output_path = os.path.join(output_dir, f'frame_{saved_count:05d}.jpg')
               cv2.imwrite(output_path, resized)
               saved_count += 1
           
           frame_count += 1
       
       cap.release()
       
       print(f"‚úì {saved_count} frames extraites, cropp√©es et redimensionn√©es")

   # Exemple : extraire 1 frame/seconde en 224√ó224 (format standard CNN)
   extraire_frames_crop_redimensionner('ma_video.mp4', 'frames/', 
                                       intervalle=30, target_size=224)


.. slide::

**Approche 2 : Redimensionnement direct (D√âCONSEILL√â si ratio diff√©rent)**

.. code-block:: python

   def extraire_frames_redimensionner_simple(video_path, output_dir, intervalle=10, 
                                             target_width=640, target_height=480):
       """
       Redimensionne directement sans crop.
       ‚ö†Ô∏è ATTENTION : d√©forme l'image si le ratio change !
       """
       os.makedirs(output_dir, exist_ok=True)
       
       cap = cv2.VideoCapture(video_path)
       
       if not cap.isOpened():
           print(f"‚ùå Erreur : impossible d'ouvrir {video_path}")
           return
       
       original_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
       original_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
       
       print(f"üìπ R√©solution originale : {original_width}√ó{original_height}")
       print(f"üìê Nouvelle r√©solution : {target_width}√ó{target_height}")
       
       # V√©rifier si le ratio va changer
       original_ratio = original_width / original_height
       target_ratio = target_width / target_height
       
       if abs(original_ratio - target_ratio) > 0.01:
           print(f"‚ö†Ô∏è  ATTENTION : Le ratio va changer !")
           print(f"    Original : {original_ratio:.2f}")
           print(f"    Cible : {target_ratio:.2f}")
           print(f"    ‚Üí L'image sera d√©form√©e !")
       
       frame_count = 0
       saved_count = 0
       
       while True:
           ret, frame = cap.read()
           if not ret:
               break
           
           if frame_count % intervalle == 0:
               # Redimensionner directement (PEUT D√âFORMER !)
               resized = cv2.resize(frame, (target_width, target_height))
               
               output_path = os.path.join(output_dir, f'frame_{saved_count:05d}.jpg')
               cv2.imwrite(output_path, resized)
               saved_count += 1
           
           frame_count += 1
       
       cap.release()
       
       print(f"‚úì {saved_count} frames extraites et redimensionn√©es")

   # Exemple : ‚ö†Ô∏è Vid√©o 16:9 ‚Üí carr√© = D√âFORMATION !
   # extraire_frames_redimensionner_simple('ma_video.mp4', 'frames/', 
   #                                        intervalle=30, 
   #                                        target_width=224, target_height=224)

üí° **Recommandations** :

- **Pour la d√©tection d'objets** : utilisez le **crop au centre** pour √©viter les d√©formations
- **Pour la classification** : le crop au centre est aussi pr√©f√©rable
- R√©solutions recommand√©es : $$224√ó224$$ (standard CNN), $$640√ó480$$ (compromis vitesse/qualit√©), $$800√ó600$$ (bonne qualit√©)

.. slide::

üìñ 3. Annotation avec Label Studio
----------------------

**Label Studio** est un outil open-source d'annotation collaborative qui permet de cr√©er des bo√Ætes englobantes, de g√©rer plusieurs annotateurs et d'exporter dans diff√©rents formats.

3.1. Installation et premier lancement
~~~~~~~~~~~~~~~~~~~

.. code-block:: bash

   # Installer Label Studio (dans votre environnement virtuel)
   pip install label-studio
   
   # Lancer Label Studio
   label-studio start
   
   # L'interface web s'ouvre automatiquement sur http://localhost:8080

**Premier lancement : cr√©ation du compte**

Au premier lancement, Label Studio vous demande de cr√©er un compte.

.. note::

   üí° **Travail collaboratif**
   
   Si vous souhaitez travailler en √©quipe, vous pourrez inviter vos coll√®gues via **"Invite People"** (voir section 3.5). Label Studio leur enverra automatiquement un email d'invitation.

**Si vous avez d√©j√† un compte** : entrez simplement votre email et mot de passe pour vous connecter.

**En cas de probl√®me** : si Label Studio ne s'ouvre pas automatiquement, ouvrez manuellement votre navigateur et allez sur ``http://localhost:8080``

.. slide::

3.2. Cr√©er un projet d'annotation
~~~~~~~~~~~~~~~~~~~

**√âtapes dans l'interface web** :

1. Cliquer sur "Create Project"

2. Donner un nom au projet (ex : "Detection_Bouteille")

3. **Import des donn√©es** :
   
   - Onglet "Data Import"
   - S√©lectionner tous les fichiers du dossier ``frames/`` (ou le dossier o√π vous avez extrait les images)
   - Cliquer sur "Import"

4. **Configuration de l'annotation** :
   
   - Cliquez sur votre projet pour l'ouvrir
   - Cliquez sur "Settings" (en haut √† droite ou dans le menu du projet)
   - Allez dans l'onglet "Labeling Interface"
   - Cliquez sur "Browse Templates"
   - S√©lectionnez "Computer Vision"
   - Choisissez "Object Detection with Bounding Boxes"
   - Une page s'ouvre pour d√©finir les labels (classes d'objets)

.. slide::

3.3. D√©finir les classes d'objets
~~~~~~~~~~~~~~~~~~~

Apr√®s avoir choisi le template, une interface appara√Æt o√π vous pouvez d√©finir vos labels (classes d'objets).

**M√©thode simple : ajouter des labels via l'interface**

1. Dans le champ "Add Label Name", entrez le nom de votre premi√®re classe (ex : "bouteille")
2. Cliquez sur "Add" ou appuyez sur Entr√©e
3. R√©p√©tez pour chaque classe d'objet √† d√©tecter
4. Cliquez sur "Save" pour valider

**Si vous pr√©f√©rez √©diter le code XML directement**, vous pouvez voir/modifier le code de configuration :

**Exemple pour d√©tecter des bouteilles et des gobelets** :

.. code-block:: xml

   <View>
     <Image name="image" value="$image"/>
     <RectangleLabels name="label" toName="image">
       <Label value="bouteille" background="green"/>
       <Label value="gobelet" background="blue"/>
     </RectangleLabels>
   </View>

üí° **Astuce** : commencez avec une seule classe pour simplifier. Vous pourrez toujours ajouter des classes plus tard.

.. slide::

3.4. Annoter les images
~~~~~~~~~~~~~~~~~~~

**Pour cr√©er une annotation** :

1. Cliquer sur une t√¢che (image) dans la liste
2. S√©lectionner la classe dans le panneau en bas de l'image (ex : "bouteille")
3. Dessiner un rectangle autour de l'objet :
   
   - Cliquer et maintenir le bouton de la souris
   - D√©placer pour cr√©er le rectangle
   - Rel√¢cher quand l'objet est bien encadr√©

4. R√©p√©ter pour tous les objets de l'image
5. Cliquer sur "Submit" pour valider l'annotation

**Modifier une annotation existante** :

- **Double-cliquer** sur un rectangle pour le s√©lectionner
- Vous pouvez alors :
  
  - **D√©placer** le rectangle en le faisant glisser
  - **Redimensionner** en tirant sur les coins ou les bords
  - **Changer la classe** dans le panneau de droite
  - **Supprimer** avec la touche ``Suppr``

**Bonnes pratiques d'annotation** :

- La bo√Æte doit englober **tout l'objet** visible (pas trop serr√©e, pas trop large)
- Si un objet est **partiellement visible** (coup√© par le bord), l'annoter quand m√™me
- Si un objet est **tr√®s petit** (<10 pixels), c'est optionnel (difficiles √† d√©tecter)
- **Coh√©rence** : gardez le m√™me style d'annotation d'une image √† l'autre

.. slide::

3.5. Annotation collaborative : inviter des personnes
~~~~~~~~~~~~~~~~~~~

Pour travailler en √©quipe sur l'annotation, suivez ces √©tapes :

**√âtape 1 : Inviter des personnes**

1. Dans Label Studio, cliquez sur l'ic√¥ne **Organization** (en haut √† droite, ic√¥ne avec plusieurs personnes)

2. Allez dans l'onglet **"People"**

3. Cliquez sur le bouton **"Invite People"**

4. Entrez les adresses email de vos coll√®gues (ex : ``marie.dupont@example.com``, ``paul.martin@example.com``)

5. Choisissez le r√¥le pour chaque personne :
   
   - **Annotator** : peut uniquement annoter les images
   - **Reviewer** : peut annoter ET valider/corriger les annotations des autres
   - **Manager** : peut g√©rer les projets et les param√®tres

6. Cliquez sur **"Send Invitations"**

7. Vos coll√®gues recevront un email avec un lien pour cr√©er leur compte

.. slide::

**√âtape 2 : Ajouter les membres √† votre projet**

Une fois les invitations accept√©es :

1. Ouvrez votre projet d'annotation
2. Allez dans **"Settings"** ‚Üí **"Members"**
3. Cliquez sur **"Add Member"**
4. S√©lectionnez les personnes dans la liste
5. Assignez-leur le r√¥le appropri√© pour ce projet

**√âtape 3 : R√©partir le travail (optionnel mais recommand√©)**

Pour √©viter que deux personnes annotent les m√™mes images :

1. Dans le projet, onglet **"Tasks"** (liste des images)
2. S√©lectionnez un groupe d'images (ex : images 1-50)
3. Menu **"Actions"** ‚Üí **"Assign Annotators"**
4. Choisissez la personne
5. R√©p√©tez pour les autres groupes d'images

**Exemple de workflow collaboratif** :

- **Marie** (Annotator) : images 1-50
- **Paul** (Annotator) : images 51-100
- **Sophie** (Reviewer) : v√©rifie et corrige toutes les annotations
- **Vous** (Manager) : supervise et exporte les donn√©es finales

üí° **Astuce qualit√©** : faites annoter 10 images par deux personnes diff√©rentes et comparez. Un IoU (Intersection over Union) > 0.7 indique une bonne coh√©rence entre annotateurs.

.. slide::

3.6. Raccourcis clavier utiles
~~~~~~~~~~~~~~~~~~~

Pour acc√©l√©rer l'annotation :

- ``1, 2, 3...`` : s√©lectionner la classe 1, 2, 3...
- ``Ctrl + Enter`` ou ``Cmd + Enter`` : soumettre et passer √† l'image suivante
- ``Ctrl + Z`` : annuler la derni√®re action
- ``Suppr`` : supprimer la bo√Æte s√©lectionn√©e
- ``Fl√®ches`` : ajuster finement la position d'une bo√Æte

.. slide::

üìñ 4. Formats d'annotations : COCO, Pascal VOC et YOLO
----------------------

Apr√®s l'annotation, il faut exporter les donn√©es dans un format exploitable par nos mod√®les. Il existe trois formats principaux, chacun avec ses avantages.

4.1. Format COCO (JSON)
~~~~~~~~~~~~~~~~~~~

**COCO** (Common Objects in Context) est le format le plus riche et le plus utilis√© en recherche.

**Structure d'un fichier COCO** :

.. code-block:: json

   {
     "images": [
       {
         "id": 1,
         "file_name": "frame_00001.jpg",
         "width": 640,
         "height": 480
       },
       {
         "id": 2,
         "file_name": "frame_00002.jpg",
         "width": 640,
         "height": 480
       }
     ],
     "annotations": [
       {
         "id": 1,
         "image_id": 1,
         "category_id": 1,
         "bbox": [100, 50, 200, 150],
         "area": 30000,
         "iscrowd": 0
       },
       {
         "id": 2,
         "image_id": 1,
         "category_id": 2,
         "bbox": [350, 200, 100, 120],
         "area": 12000,
         "iscrowd": 0
       }
     ],
     "categories": [
       {"id": 1, "name": "bouteille"},
       {"id": 2, "name": "gobelet"}
     ]
   }

**D√©tails du format bbox** : ``[x, y, width, height]``

- ``x, y`` : coin sup√©rieur gauche (en pixels)
- ``width`` : largeur de la bo√Æte
- ``height`` : hauteur de la bo√Æte

**Avantages COCO** :

- Format standard de l'industrie
- Supporte beaucoup de m√©tadonn√©es (segmentation, keypoints, etc.)
- Compatible avec pycocotools (biblioth√®que d'√©valuation)

**Inconv√©nients** :

- Un seul fichier JSON pour tout le dataset (peut devenir lourd)
- Plus complexe √† manipuler manuellement

.. slide::

4.2. Format Pascal VOC (XML)
~~~~~~~~~~~~~~~~~~~

**Pascal VOC** est un format plus ancien mais encore utilis√©. Un fichier XML par image.

**Exemple de fichier** ``frame_00001.xml`` :

.. code-block:: xml

   <annotation>
     <folder>frames</folder>
     <filename>frame_00001.jpg</filename>
     <size>
       <width>640</width>
       <height>480</height>
       <depth>3</depth>
     </size>
     <object>
       <name>bouteille</name>
       <bndbox>
         <xmin>100</xmin>
         <ymin>50</ymin>
         <xmax>300</xmax>
         <ymax>200</ymax>
       </bndbox>
     </object>
     <object>
       <name>gobelet</name>
       <bndbox>
         <xmin>350</xmin>
         <ymin>200</ymin>
         <xmax>450</xmax>
         <ymax>320</ymax>
       </bndbox>
     </object>
   </annotation>

**D√©tails du format bbox** : ``xmin, ymin, xmax, ymax``

- ``xmin, ymin`` : coin sup√©rieur gauche
- ``xmax, ymax`` : coin inf√©rieur droit

**Avantages Pascal VOC** :

- Un fichier par image (facile √† g√©rer, parall√©lisable)
- Format lisible et modifiable manuellement
- Simple √† parser avec XML

**Inconv√©nients** :

- Beaucoup de fichiers √† g√©rer
- Format verbeux (fichiers plus gros)

.. slide::

4.3. Format YOLO (TXT) 
~~~~~~~~~~~~~~~~~~~

**YOLO** utilise un format ultra-simple : un fichier texte par image.

**Exemple de fichier** ``frame_00001.txt`` :

.. code-block:: text

   0 0.3125 0.2604 0.3125 0.3125
   1 0.6250 0.5417 0.1562 0.2500

**Format d'une ligne** : ``class_id x_center y_center width height``

**Toutes les valeurs sont normalis√©es entre 0 et 1** :

- ``class_id`` : entier (0, 1, 2...) correspondant √† l'index de la classe
- ``x_center`` : position X du centre / largeur de l'image
- ``y_center`` : position Y du centre / hauteur de l'image
- ``width`` : largeur de la bo√Æte / largeur de l'image
- ``height`` : hauteur de la bo√Æte / hauteur de l'image

**Exemple de calcul** (image $$640√ó480$$, objet de 100,50 √† 300,200) :

.. code-block:: python

   # Coordonn√©es en pixels
   x1, y1, x2, y2 = 100, 50, 300, 200
   img_width, img_height = 640, 480
   
   # Calcul des valeurs YOLO
   x_center = ((x1 + x2) / 2) / img_width  # (100+300)/2 / 640 = 0.3125
   y_center = ((y1 + y2) / 2) / img_height  # (50+200)/2 / 480 = 0.2604
   width = (x2 - x1) / img_width  # (300-100) / 640 = 0.3125
   height = (y2 - y1) / img_height  # (200-50) / 480 = 0.3125
   
   # Ligne YOLO : "0 0.3125 0.2604 0.3125 0.3125"

**Avantages YOLO** :

- Format ultra-compact et rapide √† parser
- Un fichier par image (facile √† parall√©liser)
- Coordonn√©es normalis√©es (insensible √† la r√©solution)

**Inconv√©nients** :

- N√©cessite un fichier ``classes.txt`` s√©par√© pour les noms de classes
- Moins d'informations que COCO

.. slide::

4.5. Exporter depuis Label Studio
~~~~~~~~~~~~~~~~~~~

Label Studio peut exporter dans plusieurs formats. **Dans ce chapitre, nous allons utiliser :**

1. **Le format JSON natif de Label Studio** pour cr√©er un **d√©tecteur CNN custom** (sections 6-7)

**√âtapes pour exporter** :

1. Ouvrez votre projet dans Label Studio
2. Cliquez sur "Export" en haut de la liste des t√¢ches
3. Choisir le format :
   
   - **"JSON"** ‚Üí format natif Label Studio (pour notre CNN custom)
   - "YOLO" ‚Üí fichiers .txt au format YOLO (si vous voulez entra√Æner YOLO sur votre dataset custom)
   - "COCO" ‚Üí fichier JSON au format COCO (autre m√©thode)
   - "Pascal VOC" ‚Üí archive ZIP avec XMLs (autre m√©thode)

4. T√©l√©charger le fichier

.. note::

   üí° **Choix du format selon votre objectif**
   
   - **Sections 6-7** : Nous utiliserons le **format JSON** de Label Studio pour construire notre propre d√©tecteur avec PyTorch
   - **Section 8** : Nous utiliserons **COCO** (t√©l√©charg√© automatiquement) pour apprendre YOLO sur un dataset standard

.. slide::

üìñ 5. Comprendre le format JSON de Label Studio
----------------------

Nous allons utiliser le **format JSON natif de Label Studio** pour entra√Æner notre d√©tecteur custom. Voyons d'abord sa structure.

5.1. Structure du fichier JSON export√©
~~~~~~~~~~~~~~~~~~~

Apr√®s avoir cliqu√© sur "Export" ‚Üí "JSON" dans Label Studio, vous obtenez un fichier avec cette structure :

.. code-block:: json

   [
     {
       "id": 1,
       "annotations": [
         {
           "id": 1,
           "completed_by": 1,
           "result": [
             {
               "original_width": 224,
               "original_height": 224,
               "value": {
                 "x": 24.67,
                 "y": 45.99,
                 "width": 52.41,
                 "height": 54.01,
                 "rotation": 0,
                 "rectanglelabels": ["cube"]
               },
               "type": "rectanglelabels",
               "from_name": "label",
               "to_name": "image"
             }
           ]
         }
       ],
       "file_upload": "ad2a7904-frame_000000.jpg",
       "data": {
         "image": "/data/upload/1/ad2a7904-frame_000000.jpg"
       }
     },
     {
       "id": 2,
       "annotations": [...],
       "file_upload": "caed06ef-frame_000001.jpg",
       "data": {
         "image": "/data/upload/1/caed06ef-frame_000001.jpg"
       }
     }
   ]

**Points importants** :

- Chaque √©l√©ment du tableau JSON repr√©sente **une image**
- ``file_upload`` : nom original du fichier image
- ``data.image`` : chemin dans Label Studio (√† ignorer, on utilise ``file_upload``)
- ``annotations[0].result`` : liste des bo√Ætes englobantes
- ``value.x, y, width, height`` : **coordonn√©es en pourcentage** (0-100) de l'image
- ``value.rectanglelabels`` : liste des labels (ici un seul)
- ``original_width`` et ``original_height`` : dimensions de l'image (utile pour v√©rifier)

.. slide::

5.2. Extraire le nom de fichier depuis le JSON
~~~~~~~~~~~~~~~~~~~

Le champ ``file_upload`` contient le nom du fichier tel que stock√© par Label Studio (avec un pr√©fixe UUID ajout√© automatiquement). Voici comment l'utiliser :

.. code-block:: python

   import json

   # Charger le JSON
   with open('project-1-annotations.json', 'r', encoding='utf-8') as f:
       data = json.load(f)

   # Examiner la premi√®re image
   first_item = data[0]
   
   # Le champ file_upload contient le nom avec le pr√©fixe UUID
   image_name = first_item['file_upload']
   print(f"Nom du fichier : {image_name}")
   # Exemple : "ad2a7904-frame_000000.jpg"
   
   # Vous pouvez aussi l'extraire depuis data.image (m√™me r√©sultat)
   import os
   image_path = first_item['data']['image']
   image_name_alt = os.path.basename(image_path)
   print(f"Nom du fichier (depuis path) : {image_name_alt}")
   # Exemple : "ad2a7904-frame_000000.jpg"
   
   # V√©rifier les dimensions de l'image dans les annotations
   result = first_item['annotations'][0]['result'][0]
   print(f"Dimensions : {result['original_width']}x{result['original_height']}")
   # Exemple : "Dimensions : 224x224"

.. warning::

   ‚ö†Ô∏è **Attention au pr√©fixe UUID !**
   
   Label Studio ajoute automatiquement un pr√©fixe UUID lors de l'upload (ex : ``ad2a7904-frame_000000.jpg``). 
   
   **Si vos fichiers images ont les noms originaux** (``frame_000000.jpg``), vous devrez extraire la partie originale du nom.

**Script complet : nettoyer le JSON et renommer les images** :

.. code-block:: python

   import json
   import os
   import shutil

   def clean_labelstudio_dataset(json_path, images_dir, output_json_path=None):
       """
       Nettoie compl√®tement un dataset Label Studio :
       - Enl√®ve les pr√©fixes UUID du JSON
       - Renomme les fichiers images correspondants
       
       Args:
           json_path: chemin vers le JSON Label Studio
           images_dir: dossier contenant les images
           output_json_path: chemin JSON de sortie (None = √©crase l'original)
       """
       
       def remove_prefix(filename):
           """Enl√®ve le pr√©fixe UUID (8 caract√®res hexa + tiret)."""
           if '-' in filename:
               parts = filename.split('-', 1)
               if len(parts[0]) == 8 and all(c in '0123456789abcdef' for c in parts[0].lower()):
                   return parts[1]
           return filename
       
       # 1. NETTOYER LE JSON
       print("üìÑ Nettoyage du JSON...")
       with open(json_path, 'r', encoding='utf-8') as f:
           data = json.load(f)
       
       json_changes = 0
       for item in data:
           # Nettoyer file_upload
           if 'file_upload' in item:
               original = remove_prefix(item['file_upload'])
               if original != item['file_upload']:
                   print(f"  ‚úì {item['file_upload']} ‚Üí {original}")
                   item['file_upload'] = original
                   json_changes += 1
           
           # Nettoyer data.image
           if 'data' in item and 'image' in item['data']:
               path = item['data']['image']
               filename = os.path.basename(path)
               cleaned = remove_prefix(filename)
               if '/' in path:
                   item['data']['image'] = path.rsplit('/', 1)[0] + '/' + cleaned
               else:
                   item['data']['image'] = cleaned
       
       # Sauvegarder le JSON nettoy√©
       output_path = output_json_path or json_path
       with open(output_path, 'w', encoding='utf-8') as f:
           json.dump(data, f, indent=2, ensure_ascii=False)
       
       print(f"  ‚úì {json_changes} noms nettoy√©s dans le JSON")
       print(f"  ‚úì JSON sauvegard√© : {output_path}\n")
       
       # 2. RENOMMER LES IMAGES
       print("üñºÔ∏è  Renommage des images...")
       if not os.path.exists(images_dir):
           print(f"  ‚ö†Ô∏è  Dossier introuvable : {images_dir}")
           return
       
       image_changes = 0
       for filename in os.listdir(images_dir):
           if not filename.lower().endswith(('.jpg', '.jpeg', '.png')):
               continue
           
           original = remove_prefix(filename)
           if original != filename:
               old_path = os.path.join(images_dir, filename)
               new_path = os.path.join(images_dir, original)
               
               if os.path.exists(new_path):
                   print(f"  ‚ö†Ô∏è  {original} existe d√©j√†, ignor√©")
                   continue
               
               shutil.move(old_path, new_path)
               print(f"  ‚úì {filename} ‚Üí {original}")
               image_changes += 1
       
       print(f"\n‚úÖ Termin√© ! {image_changes} images renomm√©es")

   # üéØ UTILISATION
   clean_labelstudio_dataset(
       json_path='project-1-annotations.json',
       images_dir='data/images/',
       output_json_path='project-1-annotations-clean.json'  # Ou None pour √©craser
   )

üí° **Astuce** : le format peut varier selon la configuration de Label Studio. Utilisez ``file_upload`` si disponible, sinon extrayez depuis ``data.image``.

.. slide::

5.3. V√©rifier que tout fonctionne
~~~~~~~~~~~~~~~~~~~

Apr√®s avoir nettoy√© le JSON, v√©rifiez que les donn√©es sont correctes :

.. code-block:: python

   import json
   import os
   import cv2

   def verify_labelstudio_dataset(json_path, images_dir, num_samples=5):
       """
       V√©rifie que le JSON et les images correspondent.
       Affiche les statistiques et dessine quelques exemples.
       
       Args:
           json_path: JSON Label Studio (nettoy√©)
           images_dir: dossier des images
           num_samples: nombre d'images √† visualiser
       """
       
       # Charger le JSON
       with open(json_path, 'r', encoding='utf-8') as f:
           data = json.load(f)
       
       print(f"üìä STATISTIQUES DU DATASET")
       print(f"   Nombre d'images : {len(data)}")
       
       # Compter les objets et classes
       total_objects = 0
       classes_count = {}
       missing_images = []
       
       for item in data:
           image_name = item['file_upload']
           full_path = os.path.join(images_dir, image_name)
           
           # V√©rifier que l'image existe
           if not os.path.exists(full_path):
               missing_images.append(image_name)
               continue
           
           # Compter les objets
           annotations = item.get('annotations', [])
           if annotations:
               result = annotations[-1].get('result', [])
               for ann in result:
                   if ann.get('type') == 'rectanglelabels':
                       total_objects += 1
                       label = ann['value']['rectanglelabels'][0]
                       classes_count[label] = classes_count.get(label, 0) + 1
       
       print(f"   Objets annot√©s : {total_objects}")
       print(f"   Classes : {list(classes_count.keys())}")
       for cls, count in classes_count.items():
           print(f"      - {cls}: {count} objets")
       
       if missing_images:
           print(f"\n‚ö†Ô∏è  {len(missing_images)} images manquantes :")
           for img in missing_images[:5]:
               print(f"      - {img}")
       else:
           print(f"\n‚úÖ Toutes les images sont pr√©sentes !")
       
       # Visualiser quelques exemples
       print(f"\nüñºÔ∏è  VISUALISATION DE {num_samples} EXEMPLES")
       os.makedirs('verification', exist_ok=True)
       
       for idx, item in enumerate(data[:num_samples]):
           image_name = item['file_upload']
           full_path = os.path.join(images_dir, image_name)
           
           if not os.path.exists(full_path):
               continue
           
           # Charger l'image
           img = cv2.imread(full_path)
           h, w = img.shape[:2]
           
           # Dessiner les bo√Ætes
           annotations = item.get('annotations', [])
           if annotations:
               result = annotations[-1].get('result', [])
               for ann in result:
                   if ann.get('type') != 'rectanglelabels':
                       continue
                   
                   value = ann['value']
                   label = value['rectanglelabels'][0]
                   
                   # Convertir % ‚Üí pixels
                   x1 = int(value['x'] * w / 100)
                   y1 = int(value['y'] * h / 100)
                   x2 = int((value['x'] + value['width']) * w / 100)
                   y2 = int((value['y'] + value['height']) * h / 100)
                   
                   # Dessiner
                   cv2.rectangle(img, (x1, y1), (x2, y2), (0, 255, 0), 2)
                   cv2.putText(img, label, (x1, y1-10),
                              cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)
           
           # Sauvegarder
           output_path = f'verification/check_{idx:02d}_{image_name}'
           cv2.imwrite(output_path, img)
           print(f"   ‚úì {output_path}")
       
       print(f"\n‚úÖ V√©rification termin√©e ! Consultez le dossier 'verification/'")

   # üéØ UTILISATION
   verify_labelstudio_dataset(
       json_path='project-1-annotations-clean.json',
       images_dir='data/images/',
       num_samples=5
   )

üí° **Conseil** : v√©rifiez toujours vos donn√©es avant de lancer l'entra√Ænement !

.. slide::

üìñ 6. Cr√©er un Dataset PyTorch pour la d√©tection
----------------------

Maintenant que nos annotations sont pr√™tes, cr√©ons un Dataset PyTorch personnalis√© qui charge directement le JSON de Label Studio.

6.1. Structure de dossiers recommand√©e
~~~~~~~~~~~~~~~~~~~

Organisez vos fichiers ainsi :

.. code-block:: text

   mon_projet_detection/
   ‚îú‚îÄ‚îÄ data/
   ‚îÇ   ‚îú‚îÄ‚îÄ images/           # Toutes les images
   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ frame_00001.jpg
   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ frame_00002.jpg
   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ...
   ‚îÇ   ‚îú‚îÄ‚îÄ annotations.json  # Export Label Studio
   ‚îÇ   ‚îî‚îÄ‚îÄ splits.json       # Split train/val/test (optionnel)
   ‚îî‚îÄ‚îÄ train.py              # Script d'entra√Ænement

**Fichier** ``splits.json`` **(optionnel)** : pour s√©parer train/val/test

.. code-block:: json

   {
     "train": ["frame_00001.jpg", "frame_00002.jpg", ...],
     "val": ["frame_00151.jpg", "frame_00152.jpg", ...],
     "test": ["frame_00181.jpg", "frame_00182.jpg", ...]
   }

.. note::

   üí° **Split automatique avec random_split**
   
   Pas besoin de cr√©er ``splits.json`` ! Vous pouvez s√©parer train/val/test directement dans le code avec ``random_split`` comme au chapitre 5.

.. slide::

6.2. Classe DetectionDataset compl√®te
~~~~~~~~~~~~~~~~~~~

Voici une impl√©mentation qui charge directement le JSON de Label Studio :

.. code-block:: python

   import torch
   from torch.utils.data import Dataset
   from PIL import Image
   import json
   import os
   from torchvision.transforms import functional as F

   class LabelStudioDetectionDataset(Dataset):
       """
       Dataset PyTorch qui charge directement les annotations Label Studio.
       """
       
       def __init__(self, json_path, images_dir, split_images=None, transforms=None):
           """
           Args:
               json_path: chemin vers le JSON export√© de Label Studio
               images_dir: dossier contenant les images
               split_images: liste de noms d'images √† utiliser (None = toutes)
               transforms: transformations √† appliquer (optionnel)
           """
           self.images_dir = images_dir
           self.transforms = transforms
           
           # Charger le JSON
           with open(json_path, 'r', encoding='utf-8') as f:
               all_data = json.load(f)
           
           # Filtrer selon split_images si fourni
           if split_images:
               split_set = set(split_images)
               self.data = [
                   item for item in all_data
                   if os.path.basename(item['data']['image']) in split_set
               ]
           else:
               self.data = all_data
           
           # Extraire les noms de classes uniques
           classes_set = set()
           for item in self.data:
               annotations = item.get('annotations', [])
               if annotations:
                   result = annotations[-1].get('result', [])
                   for ann in result:
                       if ann.get('type') == 'rectanglelabels':
                           labels = ann['value'].get('rectanglelabels', [])
                           classes_set.update(labels)
           
           self.classes = sorted(list(classes_set))
           self.class_to_idx = {cls: idx+1 for idx, cls in enumerate(self.classes)}
           
           print(f"Dataset initialis√© : {len(self.data)} images, "
                 f"{len(self.classes)} classes : {self.classes}")
       
       def __len__(self):
           return len(self.data)
       
       def __getitem__(self, idx):
           """
           Charge une image et ses annotations.
           
           Returns:
               img: tensor [3, H, W]
               target: dict avec 'boxes', 'labels', 'image_id'
           """
           item = self.data[idx]
           
           # Extraire le nom de l'image et la charger
           image_path_str = item['data']['image']
           image_name = os.path.basename(image_path_str)
           full_path = os.path.join(self.images_dir, image_name)
           
           img = Image.open(full_path).convert('RGB')
           img_width, img_height = img.size
           
           # R√©cup√©rer les annotations
           boxes = []
           labels = []
           
           annotations = item.get('annotations', [])
           if annotations:
               # Prendre la derni√®re version (plus r√©cente)
               result = annotations[-1].get('result', [])
               
               for ann in result:
                   if ann.get('type') != 'rectanglelabels':
                       continue
                   
                   value = ann['value']
                   
                   # Label Studio donne les coordonn√©es en pourcentages (0-100)
                   x_percent = value['x']
                   y_percent = value['y']
                   w_percent = value['width']
                   h_percent = value['height']
                   
                   # Convertir en pixels [x1, y1, x2, y2]
                   x1 = (x_percent / 100.0) * img_width
                   y1 = (y_percent / 100.0) * img_height
                   x2 = ((x_percent + w_percent) / 100.0) * img_width
                   y2 = ((y_percent + h_percent) / 100.0) * img_height
                   
                   boxes.append([x1, y1, x2, y2])
                   
                   # R√©cup√©rer la classe
                   class_name = value['rectanglelabels'][0]
                   class_idx = self.class_to_idx[class_name]
                   labels.append(class_idx)
           
           # Convertir en tenseurs
           boxes = torch.as_tensor(boxes, dtype=torch.float32)
           labels = torch.as_tensor(labels, dtype=torch.int64)
           
           # Cr√©er le dictionnaire target
           target = {}
           target['boxes'] = boxes
           target['labels'] = labels
           target['image_id'] = torch.tensor([idx])
           
           # Si aucune bo√Æte, cr√©er des tenseurs vides
           if len(boxes) == 0:
               target['boxes'] = torch.zeros((0, 4), dtype=torch.float32)
               target['labels'] = torch.zeros((0,), dtype=torch.int64)
           
           # Appliquer les transformations
           if self.transforms:
               img = self.transforms(img)
           else:
               img = F.to_tensor(img)
           
           return img, target
       
       def get_class_name(self, class_id):
           """Retourne le nom d'une classe depuis son ID."""
           return self.classes[class_id - 1]

.. note::

   üí° **Gestion des IDs de classes**
   
   - Les classes sont automatiquement extraites du JSON
   - Les IDs commencent √† **1** (0 est r√©serv√© au background dans torchvision)
   - ``class_to_idx`` : dictionnaire ``{'bouteille': 1, 'gobelet': 2}``

.. slide::

6.3. Cr√©er les DataLoaders avec split automatique
~~~~~~~~~~~~~~~~~~~

Si vous n'avez pas de fichier ``splits.json``, utilisez ``random_split`` comme au chapitre 5 :

.. code-block:: python

   from torch.utils.data import DataLoader, random_split

   # Charger le dataset complet
   full_dataset = LabelStudioDetectionDataset(
       json_path='data/annotations.json',
       images_dir='data/images/'
   )

   # Split : 70% train, 15% val, 15% test
   total_size = len(full_dataset)
   train_size = int(0.70 * total_size)
   val_size = int(0.15 * total_size)
   test_size = total_size - train_size - val_size

   train_dataset, val_dataset, test_dataset = random_split(
       full_dataset, 
       [train_size, val_size, test_size]
   )

   print(f"Train : {len(train_dataset)} images")
   print(f"Val   : {len(val_dataset)} images")
   print(f"Test  : {len(test_dataset)} images")

   # Cr√©er les dataloaders
   def collate_fn(batch):
       """Fonction n√©cessaire car chaque image a un nombre diff√©rent d'objets."""
       return tuple(zip(*batch))

   train_loader = DataLoader(
       train_dataset,
       batch_size=4,
       shuffle=True,
       num_workers=4,
       collate_fn=collate_fn
   )

   val_loader = DataLoader(
       val_dataset,
       batch_size=4,
       shuffle=False,
       num_workers=4,
       collate_fn=collate_fn
   )

üí° **Avantage** : tout en un ! Pas besoin de g√©rer des listes de noms de fichiers s√©par√©es.

.. slide::

6.5. Tester le chargement des donn√©es
~~~~~~~~~~~~~~~~~~~

Toujours v√©rifier que le Dataset charge correctement :

.. code-block:: python

   # Charger un exemple
   img, target = train_dataset[0]

   print(f"Image shape: {img.shape}")
   print(f"Nombre d'objets: {len(target['boxes'])}")
   print(f"Boxes:\n{target['boxes']}")
   print(f"Labels: {target['labels']}")

   # Visualiser quelques exemples
   import matplotlib.pyplot as plt
   import matplotlib.patches as patches

   def visualize_sample(dataset, idx):
       img, target = dataset[idx]
       
       # Convertir le tensor en numpy pour l'affichage
       img_np = img.permute(1, 2, 0).numpy()
       
       fig, ax = plt.subplots(1, figsize=(12, 8))
       ax.imshow(img_np)
       
       # Dessiner chaque bo√Æte
       for box, label in zip(target['boxes'], target['labels']):
           x1, y1, x2, y2 = box.tolist()
           width = x2 - x1
           height = y2 - y1
           
           rect = patches.Rectangle(
               (x1, y1), width, height,
               linewidth=2, edgecolor='r', facecolor='none'
           )
           ax.add_patch(rect)
           
           # Ajouter le label
           class_name = dataset.get_class_name(label.item())
           ax.text(x1, y1-5, class_name, 
                  bbox=dict(facecolor='red', alpha=0.5),
                  fontsize=12, color='white')
       
       plt.axis('off')
       plt.tight_layout()
       plt.savefig(f'check_sample_{idx}.png')
       print(f"‚úì Visualisation sauvegard√©e : check_sample_{idx}.png")

   # V√©rifier les 5 premiers exemples
   for i in range(5):
       visualize_sample(train_dataset, i)




.. slide::

üìñ 7. CNN ultra-simple : r√©gression directe de bo√Æte
----------------------

Pour des cas simples avec **1 seul objet par image**, on peut utiliser une approche beaucoup plus simple que YOLO ou Faster R-CNN : **r√©gression directe des coordonn√©es** de la bo√Æte. Le mod√®le pr√©dit directement 4 nombres : ``(x_center, y_center, width, height)`` normalis√©s dans [0,1].

.. note::

   üí° **Quand utiliser cette approche ?**
   
   ‚úÖ **OUI** : 1 objet par image, objet centr√©, peu de variations (ex: d√©tection de visage, logo)
   
   ‚ùå **NON** : plusieurs objets, positions variables, objets qui se chevauchent, etc.

.. slide::
    
7.1. Architecture ultra-simple
~~~~~~~~~~~~~~~~~~~

Le mod√®le est constitu√© d'un **backbone CNN** (4 couches Conv2D + MaxPool) suivi d'un **head de r√©gression** (2 couches FC) qui pr√©dit directement les 4 coordonn√©es normalis√©es. Dans l'exemple, l‚Äôentr√©e $$224√ó224$$ est r√©duite 5 fois par MaxPool(2): 224‚Üí112‚Üí56‚Üí28‚Üí14‚Üí7; la carte de features finale est donc $$7√ó7$$. Si vous changez la taille d‚Äôentr√©e ou le nombre de couches √† stride 2, la taille de la grille changera.

.. code-block:: python

   import torch
   import torch.nn as nn
   import torch.nn.functional as F
   from tqdm import tqdm

   class SimpleBBoxRegressor(nn.Module):
       """
       CNN ultra-simple qui r√©gresse directement UNE bo√Æte par image.
       Sortie : [x_center, y_center, width, height] normalis√©s dans [0,1]
       """
       
       def __init__(self):
           super().__init__()
           
           # Backbone simple : Conv2D + MaxPool (comme chapitre 5)
           self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)
           self.pool1 = nn.MaxPool2d(2)  # 224 -> 112
           
           self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)
           self.pool2 = nn.MaxPool2d(2)  # 112 -> 56
           
           self.conv3 = nn.Conv2d(32, 64, kernel_size=3, padding=1)
           self.pool3 = nn.MaxPool2d(2)  # 56 -> 28
           
           self.conv4 = nn.Conv2d(64, 128, kernel_size=3, padding=1)
           self.pool4 = nn.MaxPool2d(2)  # 28 -> 14
           
           # Apr√®s 4 MaxPool: 224->112->56->28->14
           # Taille finale: [B, 128, 14, 14]
           
           # Head de r√©gression : 4 sorties (x, y, w, h)
           self.fc1 = nn.Linear(128 * 14 * 14, 128)
           self.fc2 = nn.Linear(128, 4)  # x_center, y_center, width, height
       
       def forward(self, x):
           # Backbone
           x = self.pool1(F.relu(self.conv1(x)))
           x = self.pool2(F.relu(self.conv2(x)))
           x = self.pool3(F.relu(self.conv3(x)))
           x = self.pool4(F.relu(self.conv4(x)))
           
           # Flatten
           x = x.view(x.size(0), -1)  # [B, 128*14*14]
           
           # R√©gression
           x = F.relu(self.fc1(x))
           x = torch.sigmoid(self.fc2(x))  # Sortie dans [0, 1]
           
           return x  # [B, 4] : (x_center, y_center, w, h) normalis√©s


   # Cr√©er le mod√®le
   simple_model = SimpleBBoxRegressor().to(device)
   num_params = sum(p.numel() for p in simple_model.parameters() if p.requires_grad)
   print(f"‚úÖ Mod√®le cr√©√© : {num_params:,} param√®tres")
   print(f"üìä Architecture : Conv2D (16‚Üí32‚Üí64‚Üí128) + Flatten + FC(128‚Üí4)")

.. note::

   üìä **Taille du mod√®le**

   Ce mod√®le a environ **25 millions** de param√®tres (principalement dans la premi√®re couche FC ``128*14*14 ‚Üí 128``). C'est bien plus petit que Faster R-CNN (``>40M``) qui est plus g√©n√©rique.

.. slide::

7.2. Loss et optimiseur
~~~~~~~~~~~~~~~~~~~

**Loss MSE** pour les coordonn√©es normalis√©es (x_center, y_center, width, height) + **pr√©paration des targets**.

.. code-block:: python

   # Loss simple : MSE sur les coordonn√©es
   criterion = nn.MSELoss()
   optimizer = optim.Adam(simple_model.parameters(), lr=1e-3)
   
   # Fonction de pr√©paration des targets
   def prepare_single_box_target(target):
       """
       Convertit les boxes du format [x1,y1,x2,y2] (pixels)
       vers [x_center, y_center, width, height] normalis√©s dans [0, 1].
       
       Suppose 1 seule bo√Æte par image.
       """
       boxes = target['boxes']  # [N, 4] en pixels avec N=1 (le seul objet √† d√©tecter)
       box = boxes[0]  # Prendre la premi√®re et unique bo√Æte (s√©curit√©)
       
       x1, y1, x2, y2 = box
       x_center = (x1 + x2) / 2 / 224  # Normaliser par la taille de l'image
       y_center = (y1 + y2) / 2 / 224
       width = (x2 - x1) / 224
       height = (y2 - y1) / 224
       
       return torch.tensor([x_center, y_center, width, height], dtype=torch.float32)

.. note::

   üìê **Normalisation des coordonn√©es**
   
   - Entr√©e : bo√Ætes en pixels ``[x1, y1, x2, y2]`` dans ``[0, 224]``
   - Sortie : coordonn√©es normalis√©es ``[x_c, y_c, w, h]`` dans ``[0, 1]``
   - Le mod√®le pr√©dit directement ces 4 valeurs normalis√©es

.. slide::

7.3. Entra√Ænement (boucles train/val)
~~~~~~~~~~~~~~~~~~~

Boucles simples d'entra√Ænement et d'√©valuation.

.. code-block:: python
   
   # Fonctions d'entra√Ænement
   def train_simple_epoch(model, criterion, optimizer, data_loader, device, epoch):
       """Entra√Æne le mod√®le pendant une epoch."""
       model.train()
       total_loss = 0
       
       # tqdm enveloppe le data_loader : it√®re sur les batchs + affiche une barre de progression
       pbar = tqdm(data_loader, desc=f"Epoch {epoch}")
       
       for images, targets in pbar:
           images = torch.stack([img.to(device) for img in images])
           
           # Pr√©parer les targets (batch de vecteurs [x_c, y_c, w, h])
           target_boxes = torch.stack([
               prepare_single_box_target(t).to(device) for t in targets
           ])  # [B, 4]
           
           # Forward
           predictions = model(images)  # [B, 4]
           
           # Loss MSE
           loss = criterion(predictions, target_boxes)
           
           # Backward
           optimizer.zero_grad()
           loss.backward()
           optimizer.step()
           
           total_loss += loss.item()
           pbar.set_postfix({'loss': f"{loss.item():.4f}"})
       
       return total_loss / len(data_loader)

   @torch.no_grad()
   def eval_simple_epoch(model, criterion, data_loader, device):
       """√âvalue le mod√®le."""
       model.eval()
       total_loss = 0
       
       for images, targets in tqdm(data_loader, desc="Validation"):
           images = torch.stack([img.to(device) for img in images])
           target_boxes = torch.stack([
               prepare_single_box_target(t).to(device) for t in targets
           ])
           
           predictions = model(images)
           loss = criterion(predictions, target_boxes)
           total_loss += loss.item()
       
       return total_loss / len(data_loader)

   # LANCER L'ENTRA√éNEMENT
   print("\nüöÄ Entra√Ænement du mod√®le simple...\n")

   num_epochs = 20
   best_val_loss = float('inf')

   for epoch in range(num_epochs):
       train_loss = train_simple_epoch(
           simple_model, criterion, optimizer, train_loader, device, epoch
       )
       
       val_loss = eval_simple_epoch(simple_model, criterion, val_loader, device)
       
       print(f"\nüìä Epoch {epoch}:")
       print(f"  Train Loss: {train_loss:.4f}")
       print(f"  Val Loss:   {val_loss:.4f}")
       
       # Sauvegarder le meilleur
       if val_loss < best_val_loss:
           best_val_loss = val_loss
           torch.save(simple_model.state_dict(), 'simple_bbox_regressor.pth')
           print("  ‚úÖ Meilleur mod√®le sauvegard√© !")

   print("\nüéâ Entra√Ænement termin√© !")
   print(f"üìÅ Mod√®le sauvegard√© : simple_bbox_regressor.pth")

.. note::

   üèãÔ∏è **Convergence**
   
   Avec ce mod√®le simple, vous devriez voir la loss descendre rapidement (√† partir de l'epoch 5). Si la loss ne descend pas, v√©rifiez que vos donn√©es sont bien normalis√©es !


.. slide::

7.4. √âvaluation avec IoU
~~~~~~~~~~~~~~~~~~~

Calcul de l'**IoU moyen** (Intersection over Union) sur le test set.

.. code-block:: python

   def compute_iou(box1, box2):
       """Calcule l'IoU entre deux bo√Ætes [x1, y1, x2, y2]."""
       x1 = max(box1[0], box2[0])
       y1 = max(box1[1], box2[1])
       x2 = min(box1[2], box2[2])
       y2 = min(box1[3], box2[3])
       
       if x2 < x1 or y2 < y1:
           return 0.0
       
       inter = (x2 - x1) * (y2 - y1)
       area1 = (box1[2] - box1[0]) * (box1[3] - box1[1])
       area2 = (box2[2] - box2[0]) * (box2[3] - box2[1])
       union = area1 + area2 - inter
       
       return inter / (union + 1e-6)
   
   @torch.no_grad()
   def evaluate_on_test(model, test_dataset, device):
       """√âvalue le mod√®le sur le test set et calcule l'IoU moyen."""
       model.eval()
       ious = []
       
       for idx in range(len(test_dataset)):
           img, target = test_dataset[idx]
           
           # Pr√©diction
           pred = model(img.unsqueeze(0).to(device))[0].cpu()  # [4]
           
           # GT
           gt_boxes = target['boxes']  # [N, 4] en pixels
           gt_box = gt_boxes[0]  # Prendre la premi√®re bo√Æte
           
           # Convertir pr√©diction [x_c, y_c, w, h] normalis√©s ‚Üí [x1, y1, x2, y2] pixels
           # (inverse de prepare_single_box_target)
           x_c, y_c, w, h = pred
           x1 = (x_c - w/2) * 224
           y1 = (y_c - h/2) * 224
           x2 = (x_c + w/2) * 224
           y2 = (y_c + h/2) * 224
           pred_box = [x1.item(), y1.item(), x2.item(), y2.item()]
           
           # Calculer IoU
           iou = compute_iou(pred_box, gt_box.tolist())
           ious.append(iou)
       
       mean_iou = torch.tensor(ious).mean().item()
       print(f"\nüìä IoU moyen sur le test set : {mean_iou:.3f}")
       print(f"  - IoU > 0.5 : {sum(1 for x in ious if x > 0.5)}/{len(ious)} images")
       print(f"  - IoU > 0.75 : {sum(1 for x in ious if x > 0.75)}/{len(ious)} images")
       
       return mean_iou
   
   # Charger le meilleur mod√®le et √©valuer
   simple_model.load_state_dict(torch.load('simple_bbox_regressor.pth'))
   evaluate_on_test(simple_model, test_dataset, device)

.. note::

   üìà **Interpr√©tation de l'IoU**

   - IoU $$> 0.5$$ : Bonne d√©tection
   - IoU $$> 0.75$$ : Tr√®s bonne d√©tection
   - IoU $$> 0.9$$ : D√©tection quasi-parfaite

   Un mod√®le bien entra√Æn√© sur ce dataset simple devrait obtenir un IoU moyen $$> 0.8$$.

.. slide::

7.5. Visualisation
~~~~~~~~~~~~~~~~~~~


Affichage des pr√©dictions sur une grille d'images avec GT (vert) et pr√©dictions (rouge).

.. code-block:: python

   import matplotlib.pyplot as plt
   import matplotlib.patches as patches
   import numpy as np
   
   @torch.no_grad()
   def visualize_predictions(model, dataset, device, n=9):
       """Affiche n pr√©dictions avec les GT en vert et pr√©dictions en rouge."""
       model.eval()
       
       fig, axes = plt.subplots(3, 3, figsize=(12, 12))
       axes = axes.flatten()
       
       for i in range(n):
           img, target = dataset[i]
           
           # Pr√©diction
           pred = model(img.unsqueeze(0).to(device))[0].cpu()
           
           # Affichage image
           img_np = img.permute(1, 2, 0).cpu().numpy()
           # D√©normaliser (ImageNet)
           mean = torch.tensor([0.485, 0.456, 0.406])
           std = torch.tensor([0.229, 0.224, 0.225])
           img_np = img_np * std.numpy() + mean.numpy()
           img_np = np.clip(img_np, 0, 1)
           
           axes[i].imshow(img_np)
           axes[i].axis('off')
           
           # GT box (vert)
           gt_box = target['boxes'][0]
           x1, y1, x2, y2 = gt_box
           w_gt, h_gt = x2 - x1, y2 - y1
           rect_gt = patches.Rectangle((x1, y1), w_gt, h_gt,
                                       linewidth=2, edgecolor='green',
                                       facecolor='none', label='GT')
           axes[i].add_patch(rect_gt)
           
           # Predicted box (rouge)
           x_c, y_c, w_pred, h_pred = pred
           x1_pred = (x_c - w_pred/2) * 224
           y1_pred = (y_c - h_pred/2) * 224
           w_pred_pix = w_pred * 224
           h_pred_pix = h_pred * 224
           rect_pred = patches.Rectangle((x1_pred, y1_pred), w_pred_pix, h_pred_pix,
                                         linewidth=2, edgecolor='red',
                                         facecolor='none', linestyle='--', label='Pred')
           axes[i].add_patch(rect_pred)
       
       # L√©gende
       handles, labels = axes[0].get_legend_handles_labels()
       fig.legend(handles, labels, loc='upper center', ncol=2, fontsize=12)
       plt.tight_layout()
       plt.show()
   
   # Visualiser
   visualize_predictions(simple_model, test_dataset, device, n=9)

.. note::

   üé® **L√©gende**
   
   - **Vert** : Ground truth (annotation r√©elle)
   - **Rouge** (pointill√©) : Pr√©diction du mod√®le
   
   Si les bo√Ætes se superposent bien, le mod√®le fonctionne correctement !

.. slide::

üìñ 8. Entra√Ænement avec YOLO sur dataset existant
----------------------

Nous allons maintenant utiliser **YOLOv11** (Ultralytics) pour entra√Æner un d√©tecteur sur un dataset standard (COCO ou Pascal VOC). YOLO (You Only Look Once) est un mod√®le utilis√© pour la d√©tection d'objets rapide et efficace, parfait pour la d√©tection en temps r√©el.

8.1. Introduction √† YOLO
~~~~~~~~~~~~~~~~~~~

**YOLO** divise l'image en une **grille** (ex: $$7√ó7$$, $$13√ó13$$, etc.) et pour chaque **cellule** de la grille, pr√©dit :

- **Plusieurs bo√Ætes englobantes candidates** (typiquement 3-9 selon les versions) gr√¢ce aux **anchors**
- Chaque bo√Æte est repr√©sent√©e par : **(x, y, w, h)** relatives au centre de la cellule
- **Objectness** : probabilit√© qu'un objet soit pr√©sent dans cette bo√Æte
- **Classes** : probabilit√©s pour chaque classe (si objet d√©tect√©)

**Avantages de YOLO :**

- ‚úÖ **Rapide** : 30-80 FPS (temps r√©el)
- ‚úÖ **One-stage** : pr√©diction directe
- ‚úÖ **Pr√©cis** : performances sup√©rieures √† Faster R-CNN
- ‚úÖ **Facile √† utiliser** : librairie Ultralytics tr√®s simple

**YOLOv11** est la derni√®re version stable (2024) avec des am√©liorations significatives par rapport √† YOLOv8 (2023) :

- Architecture optimis√©e 

- Meilleure pr√©cision 

- Plus rapide 

.. note::

   üìö **Ressources YOLO**
   
   - Documentation officielle : https://docs.ultralytics.com/
   - GitHub : https://github.com/ultralytics/ultralytics
   - Papier YOLOv11 (2024) : https://arxiv.org/abs/2410.17725
   - Papier YOLOv1 original (2015) : https://arxiv.org/abs/1506.02640

.. slide::

8.2. Concepts cl√©s : Anchors et NMS
~~~~~~~~~~~~~~~~~~~

**C'est quoi un anchor (ancre) ?**

Un anchor est une bo√Æte de r√©f√©rence pr√©d√©finie avec des proportions sp√©cifiques (largeur/hauteur).

**Exemple d'anchors :** 

- Anchor 1 : petit carr√© ($$0.2 √ó 0.2$$ de l'image) ‚Üí pour d√©tecter petits objets

- Anchor 2 : rectangle vertical ($$0.1 √ó 0.3$$) ‚Üí pour personnes debout

- Anchor 3 : rectangle horizontal ($$0.4 √ó 0.2$$) ‚Üí pour voitures

Le mod√®le **ajuste** ces anchors (d√©cale et redimensionne) pour coller aux objets r√©els. C'est plus efficace que de pr√©dire la taille depuis z√©ro !

‚û°Ô∏è **Au total** : Si grille $$13√ó13$$ avec 3 anchors par cellule = $$13√ó13√ó3$$ = **507 bo√Ætes candidates** par image !

**üßπ C'est quoi le NMS (Non-Maximum Suppression) ?**

Probl√®me : Plusieurs bo√Ætes d√©tectent souvent le **m√™me objet** (ex: 5 bo√Ætes qui se chevauchent sur une voiture).

**NMS √©limine les doublons en 3 √©tapes :**

1. **Trier** les bo√Ætes par score de confiance (objectness) d√©croissant

2. **Garder** la bo√Æte avec le meilleur score

3. **Supprimer** toutes les bo√Ætes qui se chevauchent trop (IoU > seuil, ex: 0.5) avec la bo√Æte gard√©e

4. R√©p√©ter pour les bo√Ætes restantes

**Exemple :**

- Avant NMS : 507 bo√Ætes candidates

- Apr√®s NMS : 3-10 d√©tections finales (les meilleures, sans doublons)

Le mod√®le filtre ainsi avec **NMS** pour garder les meilleures d√©tections sans redondance.

.. slide::

8.3. Installation de YOLOv11 (Ultralytics)
~~~~~~~~~~~~~~~~~~~

Installation simple via pip :

.. code-block:: python

   # Installer Ultralytics (inclut YOLOv11)
   !pip install ultralytics
   
   # Imports
   from ultralytics import YOLO
   import torch
   
   print(f"‚úÖ Ultralytics install√© !")
   print(f"üî• PyTorch version: {torch.__version__}")
   print(f"üéÆ CUDA disponible: {torch.cuda.is_available()}")

.. note::

   üí° **Versions compatibles**
   
   - Python ‚â• 3.8
   - PyTorch ‚â• 1.8
   - Ultralytics maintient automatiquement les d√©pendances

.. slide::

8.4. Dataset COCO (Common Objects in Context)
~~~~~~~~~~~~~~~~~~~

**COCO** est le dataset de r√©f√©rence pour la d√©tection d'objets :

- **80 classes** d'objets courants (personne, voiture, chien, etc.)
- **118 000 images** d'entra√Ænement (COCO complet)
- **5 000 images** de validation
- Annotations au format JSON (bo√Ætes + segmentation)

**Pour ce cours, nous utilisons COCO128**, une version r√©duite avec seulement 128 images, car :

- ‚úÖ T√©l√©chargement rapide (6.8 Mo au lieu de ~20 Go)
- ‚úÖ Entra√Ænement rapide (2-3 min au lieu de 6-10h)
- ‚úÖ Parfait pour apprendre et tester

.. note::

   üìä **Classes COCO (extrait)**
   
   0: person, 1: bicycle, 2: car, 3: motorcycle, ... 5: bus, ... 7: truck, ... 15: bird, 16: cat, 17: dog, ... 39: bottle, ... 41: cup, ... 56: chair, ...


.. slide::

8.5. Entra√Ænement YOLOv11 sur COCO128
~~~~~~~~~~~~~~~~~~~

**8.5.1. Choisir et charger le mod√®le**

YOLOv11 propose plusieurs tailles. Nous utilisons **YOLOv11n (Nano)** pour le cours car il est rapide :

.. code-block:: python
   
   # Charger YOLOv11 Nano (le plus rapide)
   model = YOLO('yolo11n.pt')
   
   print(f"‚úÖ Mod√®le YOLOv11n charg√© (3M param√®tres, 80+ FPS)")

.. note::

   üì¶ **Autres mod√®les disponibles** (pour information)
   
   - ``yolo11n.pt`` : Nano (3M params, 80+ FPS) ‚Üê **on utilise celui-ci**
   - ``yolo11s.pt`` : Small (9M params, 60 FPS)
   - ``yolo11m.pt`` : Medium (20M params, 45 FPS)
   - ``yolo11l.pt`` : Large (26M params, 35 FPS)
   - ``yolo11x.pt`` : XLarge (57M params, 30 FPS)

.. slide::

**8.5.2. T√©l√©charger COCO128**

T√©l√©chargez le dataset COCO128 via Ultralytics :

.. code-block:: python

   from ultralytics.data.utils import check_det_dataset
   
   # T√©l√©charger COCO128 (6.8 Mo, 128 images)
   print("üì• T√©l√©chargement de COCO128 (6.8 Mo)...")
   data_dict = check_det_dataset('coco128.yaml', autodownload=True)
   print(f"‚úÖ Dataset t√©l√©charg√© : {data_dict['path']}")

.. note::

   üíæ **COCO128 : 128 images, 80 classes possibles**
   
   - **128 images** : le nombre d'images dans le dataset
   - **80 classes** : les types d'objets que le mod√®le peut d√©tecter (person, car, dog, etc.)
   - Les 128 images contiennent des objets de ces 80 classes
   - Dataset t√©l√©charg√© dans : ``./datasets/coco128/``

.. slide::

**8.5.3. Lancer l'entra√Ænement**

.. code-block:: python

   # Entra√Æner YOLOv11n sur COCO128
   results = model.train(
       data='coco128.yaml',        # COCO128 (128 images)
       epochs=3,                   # 3 epochs pour le cours (rapide)
       imgsz=640,                  # Taille des images
       batch=16,                   # Batch size (ajuster selon votre GPU)
       device=0,                   # GPU 0 (ou 'cpu' sans GPU)
       project='runs/detect',      # Dossier de sortie
       name='yolo11_coco128'       # Nom de l'exp√©rience
   )
   
   print(f"‚úÖ Entra√Ænement termin√© !")
   print(f"üìÅ R√©sultats : runs/detect/yolo11_coco128/")

.. note::

   ‚è±Ô∏è **Temps d'entra√Ænement**
   
   - **COCO128** (128 images, 3 epochs) : ~2-3 minutes sur GPU
   - **COCO complet** (118k images, 50 epochs) : ~6-10 heures sur GPU
   
   Pour ce cours, COCO128 suffit amplement pour comprendre le fonctionnement !

.. slide::

**8.5.4. Visualiser les r√©sultats de l'entra√Ænement**

Ultralytics g√©n√®re automatiquement plusieurs fichiers de r√©sultats dans ``runs/detect/yolo11_coco128/`` :

- **results.png** : graphiques avec toutes les courbes (loss, mAP, etc.)
- **Courbes de loss** (train/val)
- **M√©triques mAP** (mean Average Precision)
- **Exemples de pr√©dictions**

.. code-block:: python

   # Afficher les r√©sultats de l'entra√Ænement
   from IPython.display import Image, display
   
   # Afficher la courbe de loss
   results_path = 'runs/detect/yolo11_coco128/results.png'
   try:
       print(f"üìä Affichage des courbes d'entra√Ænement YOLO\n")
       display(Image(filename=results_path))
       print(f"\n‚úÖ Graphiques charg√©s depuis : {results_path}")
   except FileNotFoundError:
       print(f"‚ö†Ô∏è Fichier non trouv√© : {results_path}")
       print("   Les r√©sultats seront disponibles apr√®s l'entra√Ænement.")

.. slide::

**8.5.5. Pour aller plus loin : COCO complet (optionnel)**

Si vous voulez entra√Æner sur le dataset complet apr√®s avoir test√© avec COCO128 :

.. code-block:: python

   # T√©l√©charger COCO complet (~20 Go, peut prendre 30-60 min)
   # print("üì• T√©l√©chargement de COCO complet (~20 Go)...")
   # data_dict = check_det_dataset('coco.yaml', autodownload=True)
   
   # Entra√Æner sur COCO complet (plusieurs heures)
   # results = model.train(
   #     data='coco.yaml',         # COCO complet (118k images)
   #     epochs=50,                # 50 epochs minimum
   #     imgsz=640,
   #     batch=16,
   #     device=0,
   #     project='runs/detect',
   #     name='yolo11_coco_full'
   # )

STOP ICI

STOP ICI

STOP ICI

STOP ICI

.. slide::

8.6. √âvaluation sur le test set
~~~~~~~~~~~~~~~~~~~

.. code-block:: python

   # Charger le meilleur mod√®le
    model = YOLO('runs/detect/yolo11_coco128/weights/best.pt')
    
    # √âvaluer sur le validation set
    metrics = model.val()

    print(f"üìä mAP@0.5: {metrics.box.map50:.3f}")
    print(f"üìä mAP@0.5:0.95: {metrics.box.map:.3f}")
    print(f"üìä Precision: {metrics.box.mp:.3f}")
    print(f"üìä Recall: {metrics.box.mr:.3f}")

.. note::

   üìà **M√©triques COCO**
   
   - **mAP@0.5** : Pr√©cision moyenne avec seuil IoU=0.5
   - **mAP@0.5:0.95** : Pr√©cision moyenne sur plusieurs seuils (standard COCO)
   - **Objectif** : mAP@0.5:0.95 > 0.40 pour un bon mod√®le

.. slide::

8.7. Inf√©rence et visualisation
~~~~~~~~~~~~~~~~~~~

Une fois le mod√®le entra√Æn√©, vous pouvez l'utiliser pour d√©tecter des objets dans de nouvelles images.

**√âtape 1 : Charger le mod√®le entra√Æn√©**

.. code-block:: python

   from ultralytics import YOLO
   
   # Charger le meilleur mod√®le entra√Æn√©
   model = YOLO('runs/detect/yolo11_coco128/weights/best.pt')
   print("‚úÖ Mod√®le charg√© !")

**√âtape 2 : Faire une pr√©diction sur une image**

.. code-block:: python

   # Pr√©diction sur une image
   results = model.predict(
       source='path/to/image.jpg',  # Chemin vers votre image
       conf=0.5,                    # Seuil de confiance minimum
       iou=0.45,                    # Seuil NMS (√©limination des doublons)
       show=False,                  # Ne pas afficher automatiquement
       save=False                   # Ne pas sauvegarder automatiquement
   )

.. note::

   üéØ **Param√®tres de pr√©diction**
   
   - ``source`` : Chemin vers l'image, dossier, vid√©o, ou URL
   - ``conf=0.5`` : **Seuil de confiance**. Le mod√®le ne garde que les d√©tections avec une confiance ‚â• 50%
   - ``iou=0.45`` : **Seuil NMS** (Non-Maximum Suppression). √âlimine les bo√Ætes qui se chevauchent trop (IoU > 45%) pour √©viter les doublons
   
   ‚ö†Ô∏è **Diff√©rence importante** : Ce seuil IoU (0.45) sert √† **filtrer les doublons** du mod√®le. C'est diff√©rent de l'IoU d'**√©valuation** (section 7.4) qui compare les pr√©dictions avec la v√©rit√© terrain.

.. slide::

**√âtape 3 : Extraire les r√©sultats**

.. code-block:: python

   # R√©cup√©rer les r√©sultats de la premi√®re image
   result = results[0]
   
   # Extraire les informations des d√©tections
   boxes = result.boxes.xyxy.cpu().numpy()    # Coordonn√©es [x1, y1, x2, y2] en pixels
   confs = result.boxes.conf.cpu().numpy()    # Confiances [0-1]
   classes = result.boxes.cls.cpu().numpy()   # IDs des classes d√©tect√©es
   
   print(f"üéØ {len(boxes)} objets d√©tect√©s !")
   
   # Afficher les d√©tails de chaque d√©tection
   for i, (box, conf, cls) in enumerate(zip(boxes, confs, classes)):
       x1, y1, x2, y2 = box
       class_name = model.names[int(cls)]  # Nom de la classe
       print(f"  Objet {i+1}: {class_name} (confiance: {conf:.2f})")

**√âtape 4 : Visualiser les d√©tections**

.. code-block:: python

   from IPython.display import Image, display
   import matplotlib.pyplot as plt
   
   # M√©thode 1 : Visualisation automatique (recommand√©e)
   # Ultralytics dessine automatiquement les bo√Ætes avec labels
   img_with_boxes = result.plot()  # Image numpy avec bo√Ætes dessin√©es
   
   plt.figure(figsize=(12, 8))
   plt.imshow(img_with_boxes)
   plt.axis('off')
   plt.title(f'{len(boxes)} objets d√©tect√©s')
   # L'image s'affiche automatiquement dans le notebook

.. note::

   üí° **Astuce visualisation**
   
   La m√©thode ``result.plot()`` dessine automatiquement :
   - Les bo√Ætes englobantes avec couleurs par classe
   - Les noms des classes
   - Les scores de confiance
   
   Vous n'avez rien d'autre √† faire !

**Visualisation sur plusieurs images :**

.. code-block:: python

   import os
   from pathlib import Path
   
   # Pr√©dire sur un dossier
   results = model.predict(
       source='datasets/coco/images/val2017/',
       conf=0.5,
       save=True,            # Sauvegarder les images annot√©es
       project='runs/detect',
       name='predictions'
   )
   
   print(f"‚úÖ Pr√©dictions sauvegard√©es dans runs/detect/predictions/")

.. slide::

8.8. Dataset personnalis√© (format YOLO)
~~~~~~~~~~~~~~~~~~~

Pour entra√Æner sur **votre propre dataset**, utilisez le format YOLO :

**Structure du dataset :**

.. code-block:: text

   my_dataset/
   ‚îú‚îÄ‚îÄ images/
   ‚îÇ   ‚îú‚îÄ‚îÄ train/
   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ img1.jpg
   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ img2.jpg
   ‚îÇ   ‚îî‚îÄ‚îÄ val/
   ‚îÇ       ‚îî‚îÄ‚îÄ img3.jpg
   ‚îî‚îÄ‚îÄ labels/
       ‚îú‚îÄ‚îÄ train/
       ‚îÇ   ‚îú‚îÄ‚îÄ img1.txt
       ‚îÇ   ‚îî‚îÄ‚îÄ img2.txt
       ‚îî‚îÄ‚îÄ val/
           ‚îî‚îÄ‚îÄ img3.txt

**Format des annotations (fichier .txt) :**

.. code-block:: text

   # Une ligne par objet : <class_id> <x_center> <y_center> <width> <height>
   # Coordonn√©es normalis√©es dans [0, 1]
   0 0.5 0.5 0.3 0.4
   1 0.2 0.3 0.15 0.2

**Fichier de configuration (my_dataset.yaml) :**

.. code-block:: yaml

   path: ../my_dataset
   train: images/train
   val: images/val
   
   nc: 2  # Nombre de classes
   names: ['class0', 'class1']

**Convertir Label Studio ‚Üí YOLO :**

.. code-block:: python

   import json
   
   def labelstudio_to_yolo(json_path, output_dir, img_width=224, img_height=224):
       """Convertit Label Studio JSON vers format YOLO."""
       with open(json_path) as f:
           data = json.load(f)
       
       for item in data:
           img_name = item['file_upload'].split('-')[-1]
           label_file = output_dir / f"{img_name.split('.')[0]}.txt"
           
           with open(label_file, 'w') as f:
               for annot in item['annotations'][0]['result']:
                   if annot['type'] == 'rectanglelabels':
                       val = annot['value']
                       # Label Studio : pourcentages [0, 100]
                       x = val['x'] / 100
                       y = val['y'] / 100
                       w = val['width'] / 100
                       h = val['height'] / 100
                       
                       # Convertir en center format
                       x_center = x + w/2
                       y_center = y + h/2
                       
                       class_id = 0  # Adapter selon vos classes
                       f.write(f"{class_id} {x_center} {y_center} {w} {h}\n")
   
   # Utiliser
   labelstudio_to_yolo('project.json', Path('my_dataset/labels/train'))

.. slide::



8.10. Exercices pratiques
~~~~~~~~~~~~~~~~~~~

**Exercice 1 : Entra√Æner YOLOv11n sur subset COCO**

1. Cr√©er un subset de 1000 images avec 3 classes (person, car, dog)
2. Entra√Æner YOLOv11n pendant 20 epochs
3. √âvaluer avec mAP@0.5
4. Visualiser 10 pr√©dictions

**Exercice 2 : Convertir votre dataset Label Studio**

1. Utiliser le script de conversion `labelstudio_to_yolo()`
2. Cr√©er le fichier `.yaml` de configuration
3. Entra√Æner YOLOv11n sur votre dataset
4. Comparer avec SimpleBBoxRegressor (¬ß7)

**Exercice 3 : Fine-tuning et hyperparam√®tres**

1. Tester diff√©rentes tailles de mod√®le (n, s, m)
2. Varier le learning rate (0.001, 0.01, 0.1)
3. Tester diff√©rents augmentations (flip, rotate, etc.)
4. Analyser l'impact sur mAP

.. warning::

   ‚ö†Ô∏è **Limites GPU**
   
   L'entra√Ænement YOLO demande une GPU avec au moins 8GB VRAM pour des batchs raisonnables. Utilisez Google Colab (gratuit) si vous n'avez pas de GPU locale.

.. slide::






###### ATTENTION : d√©finir clairement format coco vs yolo ######################



faire une section yolo avec le cube. 

exo faire un r√©seax custum pour 2 objets ou plus et utiliser yolo

faire du traking