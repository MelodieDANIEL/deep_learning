.. slide::

Chapitre 6 ‚Äî D√©tection d'objets avec des bo√Ætes englobantes
================

üéØ Objectifs du Chapitre
----------------------

.. important::

   √Ä la fin de ce chapitre, vous saurez : 

   - Comprendre la diff√©rence entre classification et d√©tection d'objets.
   - Extraire des images depuis une vid√©o.
   - Utiliser Label Studio pour annoter des objets avec des bo√Ætes englobantes de mani√®re collaborative.
   - Comprendre et manipuler les formats d'annotations.
   - Cr√©er un dataset PyTorch pour la d√©tection d'objets.
   - Entra√Æner un d√©tecteur custom.
   - Comparer avec YOLO et choisir le bon mod√®le selon le contexte.
   - Effectuer l'inf√©rence sur des images en temps r√©el.

.. slide::

üìñ 1. Classification vs D√©tection : comprendre la diff√©rence
----------------------

1.1. Classification d'images (chapitres pr√©c√©dents)
~~~~~~~~~~~~~~~~~~~

Dans les chapitres pr√©c√©dents, nous avons travaill√© sur la **classification d'images** : le mod√®le devait r√©pondre √† la question *"Qu'est-ce qu'il y a dans cette image ?"*

**Exemple** : 

- Entr√©e : une image $$224√ó224$$ pixels
- Sortie : une classe parmi N possibles (ex : "chat", "chien", "voiture")
- Une seule pr√©diction par image

.. code-block:: python

   # Classification : une image ‚Üí une classe
   output = model(image)  # Shape: [batch_size, num_classes]
   predicted_class = torch.argmax(output, dim=1)
   print(f"Cette image contient : {classes[predicted_class]}")

.. slide::

1.2. D√©tection d'objets : localiser ET classifier
~~~~~~~~~~~~~~~~~~~

La **d√©tection d'objets** va plus loin : le mod√®le doit r√©pondre √† *"Qu'est-ce qu'il y a dans cette image ET o√π se trouve chaque objet ?"*

**Pour chaque objet d√©tect√©, le mod√®le doit fournir** :

1. **La classe** de l'objet (ex : "personne", "voiture", "chien")
2. **La bo√Æte englobante** (bounding box en anglais) : 4 coordonn√©es d√©finissant un rectangle autour de l'objet
3. **Un score de confiance** : probabilit√© que la d√©tection soit correcte (0 √† 1)

**Exemple de sortie** :

.. code-block:: python

   # D√©tection : une image ‚Üí plusieurs objets localis√©s
   outputs = model(image)
   # outputs[0]['boxes']: tensor([[x1, y1, x2, y2], [x1, y1, x2, y2], ...])
   # outputs[0]['labels']: tensor([1, 3, 1, ...])  # IDs des classes
   # outputs[0]['scores']: tensor([0.95, 0.87, 0.76, ...])  # Confiances

üí° **Intuition** : imaginez que vous regardez une photo de famille. La classification dirait "photo de groupe", tandis que la d√©tection indiquerait "3 personnes aux positions (x1,y1,x2,y2), (x3,y3,x4,y4), (x5,y5,x6,y6)".

.. slide::

1.3. Qu'est-ce qu'une bo√Æte englobante ?
~~~~~~~~~~~~~~~~~~~

Une **bo√Æte englobante** est un rectangle d√©fini par 4 valeurs. Il existe plusieurs formats :

**Format 1 : (x1, y1, x2, y2)** ‚Äî> utilis√© par PyTorch/torchvision

- ``x1, y1`` : coordonn√©es du coin sup√©rieur gauche
- ``x2, y2`` : coordonn√©es du coin inf√©rieur droit

**Format 2 : (x, y, w, h)** ‚Äî> utilis√© par COCO

- ``x, y`` : coordonn√©es du coin sup√©rieur gauche
- ``w`` : largeur de la bo√Æte
- ``h`` : hauteur de la bo√Æte

**Format 3 : (x_center, y_center, w, h) normalis√©** ‚Äî> utilis√© par YOLO

- ``x_center, y_center`` : coordonn√©es du centre (normalis√©es entre 0 et 1)
- ``w, h`` : largeur et hauteur (normalis√©es entre 0 et 1)

.. code-block:: text

   Exemple d'une image $$640√ó480$$ pixels avec un objet :
   
   Format PyTorch : [100, 50, 300, 250]
   ‚Üí Rectangle du pixel (100,50) au pixel (300,250)
   
   Format COCO : [100, 50, 200, 200]
   ‚Üí Rectangle d√©marrant en (100,50) de taille $$200√ó200$$
   
   Format YOLO : [0.3125, 0.3125, 0.3125, 0.4167]
   ‚Üí Centre √† 31.25% de la largeur/hauteur, bo√Æte de 31.25%√ó41.67% de l'image

.. slide::

1.4. Applications concr√®tes de la d√©tection
~~~~~~~~~~~~~~~~~~~

La d√©tection d'objets est au c≈ìur de nombreuses applications :

- **V√©hicules autonomes** : d√©tecter pi√©tons, voitures, panneaux
- **Surveillance vid√©o** : compter les personnes, d√©tecter des comportements suspects
- **Commerce** : compter les produits en rayon, d√©tecter les vols
- **M√©dical** : localiser des tumeurs, anomalies sur des radiographies
- **R√©alit√© augment√©e** : d√©tecter des objets pour y superposer des informations

üí° Dans ce chapitre, nous allons apprendre √† cr√©er notre propre d√©tecteur d'objets personnalis√©, de A √† Z !

.. slide::

üìñ 2. Pr√©parer les donn√©es : de la vid√©o aux images annot√©es
----------------------

Le pipeline complet pour cr√©er un dataset de d√©tection :

1. **Capturer une vid√©o** de l'objet √† d√©tecter
2. **Extraire des images** (frames) depuis la vid√©o
3. **Annoter** les objets sur chaque image
4. **Exporter** les annotations dans un format standard
5. **Organiser** le dataset pour l'entra√Ænement

Voyons chaque √©tape en d√©tail.

.. slide::

2.1. Capturer une vid√©o
~~~~~~~~~~~~~~~~~~~

**Objectif** : filmer l'objet que vous voulez d√©tecter sous diff√©rents angles et conditions.

**Conseils pratiques** :

- Dur√©e : 30 secondes √† 2 minutes suffisent
- Vari√©t√© : filmez l'objet sous diff√©rents angles, distances, √©clairages
- Stabilit√© : √©vitez les mouvements trop brusques
- Qualit√© : r√©solution HD ($$1280√ó720$$ ou $$1920√ó1080$$) recommand√©e

**Exemple** : pour d√©tecter une bouteille d'eau :

- Filmez la bouteille sur un bureau (30 sec)
- Filmez-la dans une main (20 sec)
- Filmez-la avec diff√©rents arri√®re-plans (30 sec)

üí° **Astuce** : plus vous capturez de vari√©t√©, meilleur sera votre d√©tecteur !

.. note::

   **üí° Vous pouvez commencer avec beaucoup moins !**
   
   - 10-20 photos de votre smartphone suffisent pour d√©buter
   - Pas besoin de vid√©o : des images fixes fonctionnent tr√®s bien
   - R√©solution modeste (640√ó480) acceptable pour un prototype
   - M√™me avec peu de vari√©t√©, vous obtiendrez d√©j√† des r√©sultats !

.. slide::

.. slide::

2.2. Installation d'OpenCV
~~~~~~~~~~~~~~~~~~~

**OpenCV (cv2)** est une biblioth√®que Python tr√®s puissante pour manipuler des vid√©os. Elle s'utilise directement en Python sans installer d'outils externes.

.. code-block:: bash

   # Installer OpenCV dans votre environnement virtuel
   pip install opencv-python

.. slide::

2.3. Script d'extraction de base
~~~~~~~~~~~~~~~~~~~

Voici un script complet pour extraire toutes les frames d'une vid√©o :

.. code-block:: python

   import cv2
   import os

   def extraire_frames(video_path, output_dir):
       """
       Extrait toutes les frames d'une vid√©o.
       
       Args:
           video_path: chemin vers la vid√©o
           output_dir: dossier o√π sauvegarder les images
       """
       # Cr√©er le dossier de sortie (exist_ok=True √©vite l'erreur si le dossier existe d√©j√†)
       os.makedirs(output_dir, exist_ok=True)
       
       # Ouvrir la vid√©o
       cap = cv2.VideoCapture(video_path)
       
       # V√©rifier que la vid√©o s'ouvre correctement
       if not cap.isOpened():
           print(f"‚ùå Erreur : impossible d'ouvrir {video_path}")
           return

       # Obtenir les propri√©t√©s de la vid√©o (fps, nombre total de frames)
       fps = cap.get(cv2.CAP_PROP_FPS)
       total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
       
       print(f"üìπ Vid√©o : {total_frames} frames √† {fps:.2f} fps")
       
       frame_count = 0
       
       while True:
           # Lire la frame suivante
           ret, frame = cap.read()
           
           # Si plus de frames, sortir de la boucle
           if not ret:
               break
           
           # Sauvegarder la frame en jpg pour compression
           output_path = os.path.join(output_dir, f'frame_{frame_count:05d}.jpg')
           cv2.imwrite(output_path, frame)
           
           frame_count += 1
       
       # Lib√©rer les ressources
       cap.release()
       
       print(f"‚úì {frame_count} frames extraites dans {output_dir}")

   # Utilisation
   extraire_frames('ma_video.mp4', 'frames/')

.. warning::

   ‚ö†Ô∏è **Attention √† la quantit√© !**
   
   Une vid√©o de 30 secondes √† 30 fps g√©n√®re **900 images**. C'est souvent trop pour annoter manuellement !

.. slide::

2.4. Extraction intelligente (sous-√©chantillonnage)
~~~~~~~~~~~~~~~~~~~

Pour r√©duire le nombre d'images √† annoter, on extrait seulement certaines frames :

.. code-block:: python

   import cv2
   import os

   def extraire_frames_espacees(video_path, output_dir, intervalle=10):
       """
       Extrait 1 frame tous les N frames.
       
       Args:
           video_path: chemin vers la vid√©o
           output_dir: dossier de sortie
           intervalle: extraire 1 frame tous les N frames (ex: 10)
       """
       os.makedirs(output_dir, exist_ok=True)
       
       cap = cv2.VideoCapture(video_path)
       
       if not cap.isOpened():
           print(f"‚ùå Erreur : impossible d'ouvrir {video_path}")
           return
       
       fps = cap.get(cv2.CAP_PROP_FPS)
       total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
       
       print(f"üìπ Extraction de 1 frame tous les {intervalle} frames")
       print(f"   Total attendu : ~{total_frames // intervalle} images")
       
       frame_count = 0
       saved_count = 0
       
       while True:
           ret, frame = cap.read()
           if not ret:
               break
           
           # Sauvegarder seulement toutes les N frames
           if frame_count % intervalle == 0:
               output_path = os.path.join(output_dir, f'frame_{saved_count:05d}.jpg')
               cv2.imwrite(output_path, frame)
               saved_count += 1
           
           frame_count += 1
       
       cap.release()
       
       print(f"‚úì {saved_count} frames extraites sur {frame_count} totales")

   # Exemple : extraire 1 frame toutes les 10 frames
   extraire_frames_espacees('ma_video.mp4', 'frames/', intervalle=10)

**Recommandation pratique** : pour d√©buter, extraire 50-200 images est un bon compromis entre travail d'annotation et qualit√© du mod√®le.

**R√®gles de calcul de l'intervalle** :

- Vid√©o √† 30 fps, 1 frame/seconde ‚Üí ``intervalle=30``
- Vid√©o √† 30 fps, 1 frame toutes les 10 frames ‚Üí ``intervalle=10``
- Extraire ~100 images d'une vid√©o de 900 frames ‚Üí ``intervalle=9``

.. slide::

2.5. Redimensionner les images √† l'extraction
~~~~~~~~~~~~~~~~~~~

Pour √©conomiser l'espace disque et acc√©l√©rer le traitement, on peut redimensionner directement.

.. warning::

   ‚ö†Ô∏è **Attention √† la d√©formation !**
   
   Si votre vid√©o n'est **pas carr√©e** (ex : $$1920√ó1080$$) et que vous redimensionnez en **carr√©** (ex : $$224√ó224$$), l'image sera **d√©form√©e** (√©cras√©e ou √©tir√©e).
   
   **Deux solutions** :
   
   1. **Crop au centre** (RECOMMAND√â) : d√©couper un carr√© au centre avant de redimensionner
   2. **Padding** : ajouter des bordures noires pour garder le ratio

.. slide::

Voici les deux approches :

**Approche 1 : Crop au centre (recommand√©e - pas de d√©formation)**

.. code-block:: python

   import cv2
   import os

   def extraire_frames_crop_redimensionner(video_path, output_dir, intervalle=10, 
                                           target_size=224):
       """
       Extrait, crop au centre en carr√©, puis redimensionne.
       √âVITE la d√©formation en d√©coupant l'image.
       
       Args:
           video_path: chemin vers la vid√©o
           output_dir: dossier de sortie
           intervalle: extraire 1 frame tous les N frames
           target_size: taille finale du carr√© (ex: 224 pour 224√ó224)
       """
       os.makedirs(output_dir, exist_ok=True)
       
       cap = cv2.VideoCapture(video_path)
       
       if not cap.isOpened():
           print(f"‚ùå Erreur : impossible d'ouvrir {video_path}")
           return
       
       # Obtenir les dimensions originales
       original_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
       original_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
       
       print(f"üìπ R√©solution originale : {original_width}√ó{original_height}")
       print(f"üìê Nouvelle r√©solution : {target_size}√ó{target_size} (carr√©)")
       print(f"‚úÇÔ∏è  M√©thode : Crop au centre (pas de d√©formation)")
       
       frame_count = 0
       saved_count = 0
       
       while True:
           ret, frame = cap.read()
           if not ret:
               break
           
           if frame_count % intervalle == 0:
               # √âTAPE 1 : Crop au centre pour obtenir un carr√©
               h, w = frame.shape[:2]
               size = min(h, w)  # Prendre la plus petite dimension
               
               # Calculer les coordonn√©es du crop au centre
               start_y = (h - size) // 2
               start_x = (w - size) // 2
               
               # D√©couper le carr√© au centre
               cropped = frame[start_y:start_y+size, start_x:start_x+size]
               
               # √âTAPE 2 : Redimensionner le carr√© √† la taille souhait√©e
               resized = cv2.resize(cropped, (target_size, target_size))
               
               # Sauvegarder
               output_path = os.path.join(output_dir, f'frame_{saved_count:05d}.jpg')
               cv2.imwrite(output_path, resized)
               saved_count += 1
           
           frame_count += 1
       
       cap.release()
       
       print(f"‚úì {saved_count} frames extraites, cropp√©es et redimensionn√©es")

   # Exemple : extraire 1 frame/seconde en 224√ó224 (format standard CNN)
   extraire_frames_crop_redimensionner('ma_video.mp4', 'frames/', 
                                       intervalle=30, target_size=224)


.. slide::

**Approche 2 : Redimensionnement direct (D√âCONSEILL√â si ratio diff√©rent)**

.. code-block:: python

   def extraire_frames_redimensionner_simple(video_path, output_dir, intervalle=10, 
                                             target_width=640, target_height=480):
       """
       Redimensionne directement sans crop.
       ‚ö†Ô∏è ATTENTION : d√©forme l'image si le ratio change !
       """
       os.makedirs(output_dir, exist_ok=True)
       
       cap = cv2.VideoCapture(video_path)
       
       if not cap.isOpened():
           print(f"‚ùå Erreur : impossible d'ouvrir {video_path}")
           return
       
       original_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
       original_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
       
       print(f"üìπ R√©solution originale : {original_width}√ó{original_height}")
       print(f"üìê Nouvelle r√©solution : {target_width}√ó{target_height}")
       
       # V√©rifier si le ratio va changer
       original_ratio = original_width / original_height
       target_ratio = target_width / target_height
       
       if abs(original_ratio - target_ratio) > 0.01:
           print(f"‚ö†Ô∏è  ATTENTION : Le ratio va changer !")
           print(f"    Original : {original_ratio:.2f}")
           print(f"    Cible : {target_ratio:.2f}")
           print(f"    ‚Üí L'image sera d√©form√©e !")
       
       frame_count = 0
       saved_count = 0
       
       while True:
           ret, frame = cap.read()
           if not ret:
               break
           
           if frame_count % intervalle == 0:
               # Redimensionner directement (PEUT D√âFORMER !)
               resized = cv2.resize(frame, (target_width, target_height))
               
               output_path = os.path.join(output_dir, f'frame_{saved_count:05d}.jpg')
               cv2.imwrite(output_path, resized)
               saved_count += 1
           
           frame_count += 1
       
       cap.release()
       
       print(f"‚úì {saved_count} frames extraites et redimensionn√©es")

   # Exemple : ‚ö†Ô∏è Vid√©o 16:9 ‚Üí carr√© = D√âFORMATION !
   # extraire_frames_redimensionner_simple('ma_video.mp4', 'frames/', 
   #                                        intervalle=30, 
   #                                        target_width=224, target_height=224)

üí° **Recommandations** :

- **Pour la d√©tection d'objets** : utilisez le **crop au centre** pour √©viter les d√©formations
- **Pour la classification** : le crop au centre est aussi pr√©f√©rable
- R√©solutions recommand√©es : $$224√ó224$$ (standard CNN), $$640√ó480$$ (compromis vitesse/qualit√©), $$800√ó600$$ (bonne qualit√©)

.. slide::

üìñ 3. Annotation avec Label Studio
----------------------

**Label Studio** est un outil open-source d'annotation collaborative qui permet de cr√©er des bo√Ætes englobantes, de g√©rer plusieurs annotateurs et d'exporter dans diff√©rents formats.

3.1. Installation et premier lancement
~~~~~~~~~~~~~~~~~~~

.. code-block:: bash

   # Installer Label Studio (dans votre environnement virtuel)
   pip install label-studio
   
   # Lancer Label Studio
   label-studio start
   
   # L'interface web s'ouvre automatiquement sur http://localhost:8080

**Premier lancement : cr√©ation du compte**

Au premier lancement, Label Studio vous demande de cr√©er un compte.

.. note::

   üí° **Travail collaboratif**
   
   Si vous souhaitez travailler en √©quipe, vous pourrez inviter vos coll√®gues via **"Invite People"** (voir section 3.5). Label Studio leur enverra automatiquement un email d'invitation.

**Si vous avez d√©j√† un compte** : entrez simplement votre email et mot de passe pour vous connecter.

**En cas de probl√®me** : si Label Studio ne s'ouvre pas automatiquement, ouvrez manuellement votre navigateur et allez sur ``http://localhost:8080``

.. slide::

3.2. Cr√©er un projet d'annotation
~~~~~~~~~~~~~~~~~~~

**√âtapes dans l'interface web** :

1. Cliquer sur "Create Project"

2. Donner un nom au projet (ex : "Detection_Bouteille")

3. **Import des donn√©es** :
   
   - Onglet "Data Import"
   - S√©lectionner tous les fichiers du dossier ``frames/`` (ou le dossier o√π vous avez extrait les images)
   - Cliquer sur "Import"

4. **Configuration de l'annotation** :
   
   - Cliquez sur votre projet pour l'ouvrir
   - Cliquez sur "Settings" (en haut √† droite ou dans le menu du projet)
   - Allez dans l'onglet "Labeling Interface"
   - Cliquez sur "Browse Templates"
   - S√©lectionnez "Computer Vision"
   - Choisissez "Object Detection with Bounding Boxes"
   - Une page s'ouvre pour d√©finir les labels (classes d'objets)

.. slide::

3.3. D√©finir les classes d'objets
~~~~~~~~~~~~~~~~~~~

Apr√®s avoir choisi le template, une interface appara√Æt o√π vous pouvez d√©finir vos labels (classes d'objets).

**M√©thode simple : ajouter des labels via l'interface**

1. Dans le champ "Add Label Name", entrez le nom de votre premi√®re classe (ex : "bouteille")
2. Cliquez sur "Add" ou appuyez sur Entr√©e
3. R√©p√©tez pour chaque classe d'objet √† d√©tecter
4. Cliquez sur "Save" pour valider

**Si vous pr√©f√©rez √©diter le code XML directement**, vous pouvez voir/modifier le code de configuration :

**Exemple pour d√©tecter des bouteilles et des gobelets** :

.. code-block:: xml

   <View>
     <Image name="image" value="$image"/>
     <RectangleLabels name="label" toName="image">
       <Label value="bouteille" background="green"/>
       <Label value="gobelet" background="blue"/>
     </RectangleLabels>
   </View>

üí° **Astuce** : commencez avec une seule classe pour simplifier. Vous pourrez toujours ajouter des classes plus tard.

.. slide::

3.4. Annoter les images
~~~~~~~~~~~~~~~~~~~

**Pour cr√©er une annotation** :

1. Cliquer sur une t√¢che (image) dans la liste
2. S√©lectionner la classe dans le panneau en bas de l'image (ex : "bouteille")
3. Dessiner un rectangle autour de l'objet :
   
   - Cliquer et maintenir le bouton de la souris
   - D√©placer pour cr√©er le rectangle
   - Rel√¢cher quand l'objet est bien encadr√©

4. R√©p√©ter pour tous les objets de l'image
5. Cliquer sur "Submit" pour valider l'annotation

**Modifier une annotation existante** :

- **Double-cliquer** sur un rectangle pour le s√©lectionner
- Vous pouvez alors :
  
  - **D√©placer** le rectangle en le faisant glisser
  - **Redimensionner** en tirant sur les coins ou les bords
  - **Changer la classe** dans le panneau de droite
  - **Supprimer** avec la touche ``Suppr``

**Bonnes pratiques d'annotation** :

- La bo√Æte doit englober **tout l'objet** visible (pas trop serr√©e, pas trop large)
- Si un objet est **partiellement visible** (coup√© par le bord), l'annoter quand m√™me
- Si un objet est **tr√®s petit** (<10 pixels), c'est optionnel (difficiles √† d√©tecter)
- **Coh√©rence** : gardez le m√™me style d'annotation d'une image √† l'autre

.. slide::

3.5. Annotation collaborative : inviter des personnes
~~~~~~~~~~~~~~~~~~~

Pour travailler en √©quipe sur l'annotation, suivez ces √©tapes :

**√âtape 1 : Inviter des personnes**

1. Dans Label Studio, cliquez sur l'ic√¥ne **Organization** (en haut √† droite, ic√¥ne avec plusieurs personnes)

2. Allez dans l'onglet **"People"**

3. Cliquez sur le bouton **"Invite People"**

4. Entrez les adresses email de vos coll√®gues (ex : ``marie.dupont@example.com``, ``paul.martin@example.com``)

5. Choisissez le r√¥le pour chaque personne :
   
   - **Annotator** : peut uniquement annoter les images
   - **Reviewer** : peut annoter ET valider/corriger les annotations des autres
   - **Manager** : peut g√©rer les projets et les param√®tres

6. Cliquez sur **"Send Invitations"**

7. Vos coll√®gues recevront un email avec un lien pour cr√©er leur compte

.. slide::

**√âtape 2 : Ajouter les membres √† votre projet**

Une fois les invitations accept√©es :

1. Ouvrez votre projet d'annotation
2. Allez dans **"Settings"** ‚Üí **"Members"**
3. Cliquez sur **"Add Member"**
4. S√©lectionnez les personnes dans la liste
5. Assignez-leur le r√¥le appropri√© pour ce projet

**√âtape 3 : R√©partir le travail (optionnel mais recommand√©)**

Pour √©viter que deux personnes annotent les m√™mes images :

1. Dans le projet, onglet **"Tasks"** (liste des images)
2. S√©lectionnez un groupe d'images (ex : images 1-50)
3. Menu **"Actions"** ‚Üí **"Assign Annotators"**
4. Choisissez la personne
5. R√©p√©tez pour les autres groupes d'images

**Exemple de workflow collaboratif** :

- **Marie** (Annotator) : images 1-50
- **Paul** (Annotator) : images 51-100
- **Sophie** (Reviewer) : v√©rifie et corrige toutes les annotations
- **Vous** (Manager) : supervise et exporte les donn√©es finales

üí° **Astuce qualit√©** : faites annoter 10 images par deux personnes diff√©rentes et comparez. Un IoU (Intersection over Union) > 0.7 indique une bonne coh√©rence entre annotateurs.

.. slide::

3.6. Raccourcis clavier utiles
~~~~~~~~~~~~~~~~~~~

Pour acc√©l√©rer l'annotation :

- ``1, 2, 3...`` : s√©lectionner la classe 1, 2, 3...
- ``Ctrl + Enter`` ou ``Cmd + Enter`` : soumettre et passer √† l'image suivante
- ``Ctrl + Z`` : annuler la derni√®re action
- ``Suppr`` : supprimer la bo√Æte s√©lectionn√©e
- ``Fl√®ches`` : ajuster finement la position d'une bo√Æte

.. slide::

üìñ 4. Formats d'annotations : COCO, Pascal VOC et YOLO
----------------------

Apr√®s l'annotation, il faut exporter les donn√©es dans un format exploitable par nos mod√®les. Il existe trois formats principaux, chacun avec ses avantages.

4.1. Format COCO (JSON)
~~~~~~~~~~~~~~~~~~~

**COCO** (Common Objects in Context) est le format le plus riche et le plus utilis√© en recherche.

**Structure d'un fichier COCO** :

.. code-block:: json

   {
     "images": [
       {
         "id": 1,
         "file_name": "frame_00001.jpg",
         "width": 640,
         "height": 480
       },
       {
         "id": 2,
         "file_name": "frame_00002.jpg",
         "width": 640,
         "height": 480
       }
     ],
     "annotations": [
       {
         "id": 1,
         "image_id": 1,
         "category_id": 1,
         "bbox": [100, 50, 200, 150],
         "area": 30000,
         "iscrowd": 0
       },
       {
         "id": 2,
         "image_id": 1,
         "category_id": 2,
         "bbox": [350, 200, 100, 120],
         "area": 12000,
         "iscrowd": 0
       }
     ],
     "categories": [
       {"id": 1, "name": "bouteille"},
       {"id": 2, "name": "gobelet"}
     ]
   }

**D√©tails du format bbox** : ``[x, y, width, height]``

- ``x, y`` : coin sup√©rieur gauche (en pixels)
- ``width`` : largeur de la bo√Æte
- ``height`` : hauteur de la bo√Æte

**Avantages COCO** :

- Format standard de l'industrie
- Supporte beaucoup de m√©tadonn√©es (segmentation, keypoints, etc.)
- Compatible avec pycocotools (biblioth√®que d'√©valuation)

**Inconv√©nients** :

- Un seul fichier JSON pour tout le dataset (peut devenir lourd)
- Plus complexe √† manipuler manuellement

.. slide::

4.2. Format Pascal VOC (XML)
~~~~~~~~~~~~~~~~~~~

**Pascal VOC** est un format plus ancien mais encore utilis√©. Un fichier XML par image.

**Exemple de fichier** ``frame_00001.xml`` :

.. code-block:: xml

   <annotation>
     <folder>frames</folder>
     <filename>frame_00001.jpg</filename>
     <size>
       <width>640</width>
       <height>480</height>
       <depth>3</depth>
     </size>
     <object>
       <name>bouteille</name>
       <bndbox>
         <xmin>100</xmin>
         <ymin>50</ymin>
         <xmax>300</xmax>
         <ymax>200</ymax>
       </bndbox>
     </object>
     <object>
       <name>gobelet</name>
       <bndbox>
         <xmin>350</xmin>
         <ymin>200</ymin>
         <xmax>450</xmax>
         <ymax>320</ymax>
       </bndbox>
     </object>
   </annotation>

**D√©tails du format bbox** : ``xmin, ymin, xmax, ymax``

- ``xmin, ymin`` : coin sup√©rieur gauche
- ``xmax, ymax`` : coin inf√©rieur droit

**Avantages Pascal VOC** :

- Un fichier par image (facile √† g√©rer, parall√©lisable)
- Format lisible et modifiable manuellement
- Simple √† parser avec XML

**Inconv√©nients** :

- Beaucoup de fichiers √† g√©rer
- Format verbeux (fichiers plus gros)

.. slide::

4.3. Format YOLO (TXT) 
~~~~~~~~~~~~~~~~~~~

**YOLO** utilise un format ultra-simple : un fichier texte par image.

**Exemple de fichier** ``frame_00001.txt`` :

.. code-block:: text

   0 0.3125 0.2604 0.3125 0.3125
   1 0.6250 0.5417 0.1562 0.2500

**Format d'une ligne** : ``class_id x_center y_center width height``

**Toutes les valeurs sont normalis√©es entre 0 et 1** :

- ``class_id`` : entier (0, 1, 2...) correspondant √† l'index de la classe
- ``x_center`` : position X du centre / largeur de l'image
- ``y_center`` : position Y du centre / hauteur de l'image
- ``width`` : largeur de la bo√Æte / largeur de l'image
- ``height`` : hauteur de la bo√Æte / hauteur de l'image

**Exemple de calcul** (image $$640√ó480$$, objet de 100,50 √† 300,200) :

.. code-block:: python

   # Coordonn√©es en pixels
   x1, y1, x2, y2 = 100, 50, 300, 200
   img_width, img_height = 640, 480
   
   # Calcul des valeurs YOLO
   x_center = ((x1 + x2) / 2) / img_width  # (100+300)/2 / 640 = 0.3125
   y_center = ((y1 + y2) / 2) / img_height  # (50+200)/2 / 480 = 0.2604
   width = (x2 - x1) / img_width  # (300-100) / 640 = 0.3125
   height = (y2 - y1) / img_height  # (200-50) / 480 = 0.3125
   
   # Ligne YOLO : "0 0.3125 0.2604 0.3125 0.3125"

**Avantages YOLO** :

- Format ultra-compact et rapide √† parser
- Un fichier par image (facile √† parall√©liser)
- Coordonn√©es normalis√©es (insensible √† la r√©solution)

**Inconv√©nients** :

- N√©cessite un fichier ``classes.txt`` s√©par√© pour les noms de classes
- Moins d'informations que COCO

.. slide::

4.5. Exporter depuis Label Studio
~~~~~~~~~~~~~~~~~~~

Label Studio peut exporter dans plusieurs formats. **Dans ce chapitre, nous allons utiliser :**

1. **Le format JSON natif de Label Studio** pour cr√©er un **d√©tecteur CNN custom** (sections suivantes)
2. **Le format YOLO** pour utiliser les mod√®les YOLO pr√©-entra√Æn√©s (fin du chapitre)

**√âtapes pour exporter** :

1. Ouvrez votre projet dans Label Studio
2. Cliquez sur "Export" en haut de la liste des t√¢ches
3. Choisir le format :
   
   - **"JSON"** ‚Üí format natif Label Studio (pour notre CNN custom)
   - **"YOLO"** ‚Üí fichiers .txt au format YOLO (pour YOLO v5/v8/etc.)
   - "COCO" ‚Üí fichier JSON au format COCO (autre m√©thode)
   - "Pascal VOC" ‚Üí archive ZIP avec XMLs (autre m√©thode)

4. T√©l√©charger le fichier

.. note::

   üí° **Choix du format**
   
   - **Dans la suite de ce chapitre**, nous utiliserons le **format JSON** de Label Studio pour construire notre propre d√©tecteur avec PyTorch
   - **Plus loin dans le chapitre**, nous verrons comment utiliser le **format YOLO** avec les mod√®les YOLO pr√©-entra√Æn√©s
   - Les formats **COCO** et **Pascal VOC** sont disponibles si vous souhaitez utiliser d'autres frameworks.

.. slide::

üìñ 5. Comprendre le format JSON de Label Studio
----------------------

Nous allons utiliser le **format JSON natif de Label Studio** pour entra√Æner notre d√©tecteur custom. Voyons d'abord sa structure.

5.1. Structure du fichier JSON export√©
~~~~~~~~~~~~~~~~~~~

Apr√®s avoir cliqu√© sur "Export" ‚Üí "JSON" dans Label Studio, vous obtenez un fichier avec cette structure :

.. code-block:: json

   [
     {
       "id": 1,
       "annotations": [
         {
           "id": 1,
           "completed_by": 1,
           "result": [
             {
               "original_width": 224,
               "original_height": 224,
               "value": {
                 "x": 24.67,
                 "y": 45.99,
                 "width": 52.41,
                 "height": 54.01,
                 "rotation": 0,
                 "rectanglelabels": ["cube"]
               },
               "type": "rectanglelabels",
               "from_name": "label",
               "to_name": "image"
             }
           ]
         }
       ],
       "file_upload": "ad2a7904-frame_000000.jpg",
       "data": {
         "image": "/data/upload/1/ad2a7904-frame_000000.jpg"
       }
     },
     {
       "id": 2,
       "annotations": [...],
       "file_upload": "caed06ef-frame_000001.jpg",
       "data": {
         "image": "/data/upload/1/caed06ef-frame_000001.jpg"
       }
     }
   ]

**Points importants** :

- Chaque √©l√©ment du tableau JSON repr√©sente **une image**
- ``file_upload`` : nom original du fichier image
- ``data.image`` : chemin dans Label Studio (√† ignorer, on utilise ``file_upload``)
- ``annotations[0].result`` : liste des bo√Ætes englobantes
- ``value.x, y, width, height`` : **coordonn√©es en pourcentage** (0-100) de l'image
- ``value.rectanglelabels`` : liste des labels (ici un seul)
- ``original_width`` et ``original_height`` : dimensions de l'image (utile pour v√©rifier)

.. slide::

5.2. Extraire le nom de fichier depuis le JSON
~~~~~~~~~~~~~~~~~~~

Le champ ``file_upload`` contient le nom du fichier tel que stock√© par Label Studio (avec un pr√©fixe UUID ajout√© automatiquement). Voici comment l'utiliser :

.. code-block:: python

   import json

   # Charger le JSON
   with open('project-1-annotations.json', 'r', encoding='utf-8') as f:
       data = json.load(f)

   # Examiner la premi√®re image
   first_item = data[0]
   
   # Le champ file_upload contient le nom avec le pr√©fixe UUID
   image_name = first_item['file_upload']
   print(f"Nom du fichier : {image_name}")
   # Exemple : "ad2a7904-frame_000000.jpg"
   
   # Vous pouvez aussi l'extraire depuis data.image (m√™me r√©sultat)
   import os
   image_path = first_item['data']['image']
   image_name_alt = os.path.basename(image_path)
   print(f"Nom du fichier (depuis path) : {image_name_alt}")
   # Exemple : "ad2a7904-frame_000000.jpg"
   
   # V√©rifier les dimensions de l'image dans les annotations
   result = first_item['annotations'][0]['result'][0]
   print(f"Dimensions : {result['original_width']}x{result['original_height']}")
   # Exemple : "Dimensions : 224x224"

.. warning::

   ‚ö†Ô∏è **Attention au pr√©fixe UUID !**
   
   Label Studio ajoute automatiquement un pr√©fixe UUID lors de l'upload (ex : ``ad2a7904-frame_000000.jpg``). 
   
   **Si vos fichiers images ont les noms originaux** (``frame_000000.jpg``), vous devrez extraire la partie originale du nom.

**Script complet : nettoyer le JSON et renommer les images** :

.. code-block:: python

   import json
   import os
   import shutil

   def clean_labelstudio_dataset(json_path, images_dir, output_json_path=None):
       """
       Nettoie compl√®tement un dataset Label Studio :
       - Enl√®ve les pr√©fixes UUID du JSON
       - Renomme les fichiers images correspondants
       
       Args:
           json_path: chemin vers le JSON Label Studio
           images_dir: dossier contenant les images
           output_json_path: chemin JSON de sortie (None = √©crase l'original)
       """
       
       def remove_prefix(filename):
           """Enl√®ve le pr√©fixe UUID (8 caract√®res hexa + tiret)."""
           if '-' in filename:
               parts = filename.split('-', 1)
               if len(parts[0]) == 8 and all(c in '0123456789abcdef' for c in parts[0].lower()):
                   return parts[1]
           return filename
       
       # 1. NETTOYER LE JSON
       print("üìÑ Nettoyage du JSON...")
       with open(json_path, 'r', encoding='utf-8') as f:
           data = json.load(f)
       
       json_changes = 0
       for item in data:
           # Nettoyer file_upload
           if 'file_upload' in item:
               original = remove_prefix(item['file_upload'])
               if original != item['file_upload']:
                   print(f"  ‚úì {item['file_upload']} ‚Üí {original}")
                   item['file_upload'] = original
                   json_changes += 1
           
           # Nettoyer data.image
           if 'data' in item and 'image' in item['data']:
               path = item['data']['image']
               filename = os.path.basename(path)
               cleaned = remove_prefix(filename)
               if '/' in path:
                   item['data']['image'] = path.rsplit('/', 1)[0] + '/' + cleaned
               else:
                   item['data']['image'] = cleaned
       
       # Sauvegarder le JSON nettoy√©
       output_path = output_json_path or json_path
       with open(output_path, 'w', encoding='utf-8') as f:
           json.dump(data, f, indent=2, ensure_ascii=False)
       
       print(f"  ‚úì {json_changes} noms nettoy√©s dans le JSON")
       print(f"  ‚úì JSON sauvegard√© : {output_path}\n")
       
       # 2. RENOMMER LES IMAGES
       print("üñºÔ∏è  Renommage des images...")
       if not os.path.exists(images_dir):
           print(f"  ‚ö†Ô∏è  Dossier introuvable : {images_dir}")
           return
       
       image_changes = 0
       for filename in os.listdir(images_dir):
           if not filename.lower().endswith(('.jpg', '.jpeg', '.png')):
               continue
           
           original = remove_prefix(filename)
           if original != filename:
               old_path = os.path.join(images_dir, filename)
               new_path = os.path.join(images_dir, original)
               
               if os.path.exists(new_path):
                   print(f"  ‚ö†Ô∏è  {original} existe d√©j√†, ignor√©")
                   continue
               
               shutil.move(old_path, new_path)
               print(f"  ‚úì {filename} ‚Üí {original}")
               image_changes += 1
       
       print(f"\n‚úÖ Termin√© ! {image_changes} images renomm√©es")

   # üéØ UTILISATION
   clean_labelstudio_dataset(
       json_path='project-1-annotations.json',
       images_dir='data/images/',
       output_json_path='project-1-annotations-clean.json'  # Ou None pour √©craser
   )

üí° **Astuce** : le format peut varier selon la configuration de Label Studio. Utilisez ``file_upload`` si disponible, sinon extrayez depuis ``data.image``.

.. slide::

5.3. V√©rifier que tout fonctionne
~~~~~~~~~~~~~~~~~~~

Apr√®s avoir nettoy√© le JSON, v√©rifiez que les donn√©es sont correctes :

.. code-block:: python

   import json
   import os
   import cv2

   def verify_labelstudio_dataset(json_path, images_dir, num_samples=5):
       """
       V√©rifie que le JSON et les images correspondent.
       Affiche les statistiques et dessine quelques exemples.
       
       Args:
           json_path: JSON Label Studio (nettoy√©)
           images_dir: dossier des images
           num_samples: nombre d'images √† visualiser
       """
       
       # Charger le JSON
       with open(json_path, 'r', encoding='utf-8') as f:
           data = json.load(f)
       
       print(f"üìä STATISTIQUES DU DATASET")
       print(f"   Nombre d'images : {len(data)}")
       
       # Compter les objets et classes
       total_objects = 0
       classes_count = {}
       missing_images = []
       
       for item in data:
           image_name = item['file_upload']
           full_path = os.path.join(images_dir, image_name)
           
           # V√©rifier que l'image existe
           if not os.path.exists(full_path):
               missing_images.append(image_name)
               continue
           
           # Compter les objets
           annotations = item.get('annotations', [])
           if annotations:
               result = annotations[-1].get('result', [])
               for ann in result:
                   if ann.get('type') == 'rectanglelabels':
                       total_objects += 1
                       label = ann['value']['rectanglelabels'][0]
                       classes_count[label] = classes_count.get(label, 0) + 1
       
       print(f"   Objets annot√©s : {total_objects}")
       print(f"   Classes : {list(classes_count.keys())}")
       for cls, count in classes_count.items():
           print(f"      - {cls}: {count} objets")
       
       if missing_images:
           print(f"\n‚ö†Ô∏è  {len(missing_images)} images manquantes :")
           for img in missing_images[:5]:
               print(f"      - {img}")
       else:
           print(f"\n‚úÖ Toutes les images sont pr√©sentes !")
       
       # Visualiser quelques exemples
       print(f"\nüñºÔ∏è  VISUALISATION DE {num_samples} EXEMPLES")
       os.makedirs('verification', exist_ok=True)
       
       for idx, item in enumerate(data[:num_samples]):
           image_name = item['file_upload']
           full_path = os.path.join(images_dir, image_name)
           
           if not os.path.exists(full_path):
               continue
           
           # Charger l'image
           img = cv2.imread(full_path)
           h, w = img.shape[:2]
           
           # Dessiner les bo√Ætes
           annotations = item.get('annotations', [])
           if annotations:
               result = annotations[-1].get('result', [])
               for ann in result:
                   if ann.get('type') != 'rectanglelabels':
                       continue
                   
                   value = ann['value']
                   label = value['rectanglelabels'][0]
                   
                   # Convertir % ‚Üí pixels
                   x1 = int(value['x'] * w / 100)
                   y1 = int(value['y'] * h / 100)
                   x2 = int((value['x'] + value['width']) * w / 100)
                   y2 = int((value['y'] + value['height']) * h / 100)
                   
                   # Dessiner
                   cv2.rectangle(img, (x1, y1), (x2, y2), (0, 255, 0), 2)
                   cv2.putText(img, label, (x1, y1-10),
                              cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)
           
           # Sauvegarder
           output_path = f'verification/check_{idx:02d}_{image_name}'
           cv2.imwrite(output_path, img)
           print(f"   ‚úì {output_path}")
       
       print(f"\n‚úÖ V√©rification termin√©e ! Consultez le dossier 'verification/'")

   # üéØ UTILISATION
   verify_labelstudio_dataset(
       json_path='project-1-annotations-clean.json',
       images_dir='data/images/',
       num_samples=5
   )

üí° **Conseil** : v√©rifiez toujours vos donn√©es avant de lancer l'entra√Ænement !

.. slide::

üìñ 6. Cr√©er un Dataset PyTorch pour la d√©tection
----------------------

Maintenant que nos annotations sont pr√™tes, cr√©ons un Dataset PyTorch personnalis√© qui charge directement le JSON de Label Studio.

6.1. Structure de dossiers recommand√©e
~~~~~~~~~~~~~~~~~~~

Organisez vos fichiers ainsi :

.. code-block:: text

   mon_projet_detection/
   ‚îú‚îÄ‚îÄ data/
   ‚îÇ   ‚îú‚îÄ‚îÄ images/           # Toutes les images
   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ frame_00001.jpg
   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ frame_00002.jpg
   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ...
   ‚îÇ   ‚îú‚îÄ‚îÄ annotations.json  # Export Label Studio
   ‚îÇ   ‚îî‚îÄ‚îÄ splits.json       # Split train/val/test (optionnel)
   ‚îî‚îÄ‚îÄ train.py              # Script d'entra√Ænement

**Fichier** ``splits.json`` **(optionnel)** : pour s√©parer train/val/test

.. code-block:: json

   {
     "train": ["frame_00001.jpg", "frame_00002.jpg", ...],
     "val": ["frame_00151.jpg", "frame_00152.jpg", ...],
     "test": ["frame_00181.jpg", "frame_00182.jpg", ...]
   }

.. note::

   üí° **Split automatique avec random_split**
   
   Pas besoin de cr√©er ``splits.json`` ! Vous pouvez s√©parer train/val/test directement dans le code avec ``random_split`` comme au chapitre 5.

.. slide::

6.2. Classe DetectionDataset compl√®te
~~~~~~~~~~~~~~~~~~~

Voici une impl√©mentation qui charge directement le JSON de Label Studio :

.. code-block:: python

   import torch
   from torch.utils.data import Dataset
   from PIL import Image
   import json
   import os
   from torchvision.transforms import functional as F

   class LabelStudioDetectionDataset(Dataset):
       """
       Dataset PyTorch qui charge directement les annotations Label Studio.
       """
       
       def __init__(self, json_path, images_dir, split_images=None, transforms=None):
           """
           Args:
               json_path: chemin vers le JSON export√© de Label Studio
               images_dir: dossier contenant les images
               split_images: liste de noms d'images √† utiliser (None = toutes)
               transforms: transformations √† appliquer (optionnel)
           """
           self.images_dir = images_dir
           self.transforms = transforms
           
           # Charger le JSON
           with open(json_path, 'r', encoding='utf-8') as f:
               all_data = json.load(f)
           
           # Filtrer selon split_images si fourni
           if split_images:
               split_set = set(split_images)
               self.data = [
                   item for item in all_data
                   if os.path.basename(item['data']['image']) in split_set
               ]
           else:
               self.data = all_data
           
           # Extraire les noms de classes uniques
           classes_set = set()
           for item in self.data:
               annotations = item.get('annotations', [])
               if annotations:
                   result = annotations[-1].get('result', [])
                   for ann in result:
                       if ann.get('type') == 'rectanglelabels':
                           labels = ann['value'].get('rectanglelabels', [])
                           classes_set.update(labels)
           
           self.classes = sorted(list(classes_set))
           self.class_to_idx = {cls: idx+1 for idx, cls in enumerate(self.classes)}
           
           print(f"Dataset initialis√© : {len(self.data)} images, "
                 f"{len(self.classes)} classes : {self.classes}")
       
       def __len__(self):
           return len(self.data)
       
       def __getitem__(self, idx):
           """
           Charge une image et ses annotations.
           
           Returns:
               img: tensor [3, H, W]
               target: dict avec 'boxes', 'labels', 'image_id'
           """
           item = self.data[idx]
           
           # Extraire le nom de l'image et la charger
           image_path_str = item['data']['image']
           image_name = os.path.basename(image_path_str)
           full_path = os.path.join(self.images_dir, image_name)
           
           img = Image.open(full_path).convert('RGB')
           img_width, img_height = img.size
           
           # R√©cup√©rer les annotations
           boxes = []
           labels = []
           
           annotations = item.get('annotations', [])
           if annotations:
               # Prendre la derni√®re version (plus r√©cente)
               result = annotations[-1].get('result', [])
               
               for ann in result:
                   if ann.get('type') != 'rectanglelabels':
                       continue
                   
                   value = ann['value']
                   
                   # Label Studio donne les coordonn√©es en pourcentages (0-100)
                   x_percent = value['x']
                   y_percent = value['y']
                   w_percent = value['width']
                   h_percent = value['height']
                   
                   # Convertir en pixels [x1, y1, x2, y2]
                   x1 = (x_percent / 100.0) * img_width
                   y1 = (y_percent / 100.0) * img_height
                   x2 = ((x_percent + w_percent) / 100.0) * img_width
                   y2 = ((y_percent + h_percent) / 100.0) * img_height
                   
                   boxes.append([x1, y1, x2, y2])
                   
                   # R√©cup√©rer la classe
                   class_name = value['rectanglelabels'][0]
                   class_idx = self.class_to_idx[class_name]
                   labels.append(class_idx)
           
           # Convertir en tenseurs
           boxes = torch.as_tensor(boxes, dtype=torch.float32)
           labels = torch.as_tensor(labels, dtype=torch.int64)
           
           # Cr√©er le dictionnaire target
           target = {}
           target['boxes'] = boxes
           target['labels'] = labels
           target['image_id'] = torch.tensor([idx])
           
           # Si aucune bo√Æte, cr√©er des tenseurs vides
           if len(boxes) == 0:
               target['boxes'] = torch.zeros((0, 4), dtype=torch.float32)
               target['labels'] = torch.zeros((0,), dtype=torch.int64)
           
           # Appliquer les transformations
           if self.transforms:
               img = self.transforms(img)
           else:
               img = F.to_tensor(img)
           
           return img, target
       
       def get_class_name(self, class_id):
           """Retourne le nom d'une classe depuis son ID."""
           return self.classes[class_id - 1]

.. note::

   üí° **Gestion des IDs de classes**
   
   - Les classes sont automatiquement extraites du JSON
   - Les IDs commencent √† **1** (0 est r√©serv√© au background dans torchvision)
   - ``class_to_idx`` : dictionnaire ``{'bouteille': 1, 'gobelet': 2}``

.. slide::

6.3. Cr√©er les DataLoaders avec split automatique
~~~~~~~~~~~~~~~~~~~

Si vous n'avez pas de fichier ``splits.json``, utilisez ``random_split`` comme au chapitre 5 :

.. code-block:: python

   from torch.utils.data import DataLoader, random_split

   # Charger le dataset complet
   full_dataset = LabelStudioDetectionDataset(
       json_path='data/annotations.json',
       images_dir='data/images/'
   )

   # Split : 70% train, 15% val, 15% test
   total_size = len(full_dataset)
   train_size = int(0.70 * total_size)
   val_size = int(0.15 * total_size)
   test_size = total_size - train_size - val_size

   train_dataset, val_dataset, test_dataset = random_split(
       full_dataset, 
       [train_size, val_size, test_size]
   )

   print(f"Train : {len(train_dataset)} images")
   print(f"Val   : {len(val_dataset)} images")
   print(f"Test  : {len(test_dataset)} images")

   # Cr√©er les dataloaders
   def collate_fn(batch):
       """Fonction n√©cessaire car chaque image a un nombre diff√©rent d'objets."""
       return tuple(zip(*batch))

   train_loader = DataLoader(
       train_dataset,
       batch_size=4,
       shuffle=True,
       num_workers=4,
       collate_fn=collate_fn
   )

   val_loader = DataLoader(
       val_dataset,
       batch_size=4,
       shuffle=False,
       num_workers=4,
       collate_fn=collate_fn
   )

üí° **Avantage** : tout en un ! Pas besoin de g√©rer des listes de noms de fichiers s√©par√©es.

.. slide::

6.5. Tester le chargement des donn√©es
~~~~~~~~~~~~~~~~~~~

Toujours v√©rifier que le Dataset charge correctement :

.. code-block:: python

   # Charger un exemple
   img, target = train_dataset[0]

   print(f"Image shape: {img.shape}")
   print(f"Nombre d'objets: {len(target['boxes'])}")
   print(f"Boxes:\n{target['boxes']}")
   print(f"Labels: {target['labels']}")

   # Visualiser quelques exemples
   import matplotlib.pyplot as plt
   import matplotlib.patches as patches

   def visualize_sample(dataset, idx):
       img, target = dataset[idx]
       
       # Convertir le tensor en numpy pour l'affichage
       img_np = img.permute(1, 2, 0).numpy()
       
       fig, ax = plt.subplots(1, figsize=(12, 8))
       ax.imshow(img_np)
       
       # Dessiner chaque bo√Æte
       for box, label in zip(target['boxes'], target['labels']):
           x1, y1, x2, y2 = box.tolist()
           width = x2 - x1
           height = y2 - y1
           
           rect = patches.Rectangle(
               (x1, y1), width, height,
               linewidth=2, edgecolor='r', facecolor='none'
           )
           ax.add_patch(rect)
           
           # Ajouter le label
           class_name = dataset.get_class_name(label.item())
           ax.text(x1, y1-5, class_name, 
                  bbox=dict(facecolor='red', alpha=0.5),
                  fontsize=12, color='white')
       
       plt.axis('off')
       plt.tight_layout()
       plt.savefig(f'check_sample_{idx}.png')
       print(f"‚úì Visualisation sauvegard√©e : check_sample_{idx}.png")

   # V√©rifier les 5 premiers exemples
   for i in range(5):
       visualize_sample(train_dataset, i)


STOP ICI

STOP ICI

STOP ICI

.. slide::

üìñ 7. CNN ultra-simple : r√©gression directe de bo√Æte
----------------------

Pour des cas simples avec **1 seul objet par image**, on peut utiliser une approche beaucoup plus simple que YOLO ou Faster R-CNN : **r√©gression directe des coordonn√©es** de la bo√Æte. Le mod√®le pr√©dit directement 4 nombres : `(x_center, y_center, width, height)` normalis√©s dans [0,1].

.. note::

   üí° **Quand utiliser cette approche ?**
   
   ‚úÖ **OUI** : 1 objet par image, objet centr√©, peu de variations (ex: d√©tection de visage, logo)
   
   ‚ùå **NON** : plusieurs objets, positions variables, objets qui se chevauchent

7.1. Architecture ultra-simple
~~~~~~~~~~~~~~~~~~~

Le mod√®le est constitu√© d'un **backbone CNN** (4 couches Conv2D + MaxPool) suivi d'un **head de r√©gression** (2 couches FC) qui pr√©dit directement les 4 coordonn√©es normalis√©es. Dans l'exemple, l‚Äôentr√©e $$224√ó224$$ est r√©duite 5 fois par MaxPool(2): 224‚Üí112‚Üí56‚Üí28‚Üí14‚Üí7; la carte de features finale est donc $$7√ó7$$. Si vous changez la taille d‚Äôentr√©e ou le nombre de couches √† stride 2, la taille de la grille changera.

.. code-block:: python

   import torch
   import torch.nn as nn
   import torch.nn.functional as F
   from tqdm import tqdm

   class SimpleBBoxRegressor(nn.Module):
       """
       CNN ultra-simple qui r√©gresse directement UNE bo√Æte par image.
       Sortie : [x_center, y_center, width, height] normalis√©s dans [0,1]
       """
       
       def __init__(self):
           super().__init__()
           
           # Backbone simple : Conv2D + MaxPool (comme chapitre 5)
           self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)
           self.pool1 = nn.MaxPool2d(2)  # 224 -> 112
           
           self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)
           self.pool2 = nn.MaxPool2d(2)  # 112 -> 56
           
           self.conv3 = nn.Conv2d(32, 64, kernel_size=3, padding=1)
           self.pool3 = nn.MaxPool2d(2)  # 56 -> 28
           
           self.conv4 = nn.Conv2d(64, 128, kernel_size=3, padding=1)
           self.pool4 = nn.MaxPool2d(2)  # 28 -> 14
           
           # Apr√®s 4 MaxPool: 224->112->56->28->14
           # Taille finale: [B, 128, 14, 14]
           
           # Head de r√©gression : 4 sorties (x, y, w, h)
           self.fc1 = nn.Linear(128 * 14 * 14, 128)
           self.fc2 = nn.Linear(128, 4)  # x_center, y_center, width, height
       
       def forward(self, x):
           # Backbone
           x = self.pool1(F.relu(self.conv1(x)))
           x = self.pool2(F.relu(self.conv2(x)))
           x = self.pool3(F.relu(self.conv3(x)))
           x = self.pool4(F.relu(self.conv4(x)))
           
           # Flatten
           x = x.view(x.size(0), -1)  # [B, 128*14*14]
           
           # R√©gression
           x = F.relu(self.fc1(x))
           x = torch.sigmoid(self.fc2(x))  # Sortie dans [0, 1]
           
           return x  # [B, 4] : (x_center, y_center, w, h) normalis√©s


   # Cr√©er le mod√®le
   simple_model = SimpleBBoxRegressor().to(device)
   num_params = sum(p.numel() for p in simple_model.parameters() if p.requires_grad)
   print(f"‚úÖ Mod√®le cr√©√© : {num_params:,} param√®tres")
   print(f"üìä Architecture : Conv2D (16‚Üí32‚Üí64‚Üí128) + Flatten + FC(128‚Üí4)")

.. note::

   üìä **Taille du mod√®le**
   
   Ce mod√®le a environ **25 millions** de param√®tres (principalement dans la premi√®re couche FC `128*14*14 ‚Üí 128`). C'est bien plus petit que Faster R-CNN (>40M) mais simple et efficace pour 1 objet par image.

.. warning::

   Ce mod√®le suppose **1 seul objet par image**. Si votre dataset contient plusieurs objets par image, utilisez plut√¥t un mod√®le avec anchors (section 7.2 de la version compl√®te) ou Faster R-CNN (section 8).

7.2. Loss et optimiseur
~~~~~~~~~~~~~~~~~~~

**Loss MSE** pour les coordonn√©es normalis√©es (x_center, y_center, width, height) + **pr√©paration des targets**.

.. code-block:: python

   # Loss simple : MSE sur les coordonn√©es
   criterion = nn.MSELoss()
   optimizer = optim.Adam(simple_model.parameters(), lr=1e-3)
   
   # Fonction de pr√©paration des targets
   def prepare_single_box_target(target):
       """
       Convertit les boxes du format [x1,y1,x2,y2] (pixels)
       vers [x_center, y_center, width, height] normalis√©s dans [0, 1].
       
       Suppose 1 seule bo√Æte par image.
       """
       boxes = target['boxes']  # [N, 4] en pixels
       box = boxes[0]  # Prendre la premi√®re (et unique) bo√Æte
       
       x1, y1, x2, y2 = box
       x_center = (x1 + x2) / 2 / 224  # Normaliser par la taille de l'image
       y_center = (y1 + y2) / 2 / 224
       width = (x2 - x1) / 224
       height = (y2 - y1) / 224
       
       return torch.tensor([x_center, y_center, width, height], dtype=torch.float32)

.. note::

   üìê **Normalisation des coordonn√©es**
   
   - Entr√©e : bo√Ætes en pixels `[x1, y1, x2, y2]` dans `[0, 224]`
   - Sortie : coordonn√©es normalis√©es `[x_c, y_c, w, h]` dans `[0, 1]`
   - Le mod√®le pr√©dit directement ces 4 valeurs normalis√©es
   - Pas d'anchors, pas de grille, pas d'objectness !


7.3. Entra√Ænement (boucles train/val)
~~~~~~~~~~~~~~~~~~~

Boucles simples d'entra√Ænement et d'√©valuation.

.. code-block:: python

   from tqdm import tqdm
   
   # Fonctions d'entra√Ænement
   def train_simple_epoch(model, criterion, optimizer, data_loader, device, epoch):
       """Entra√Æne le mod√®le pendant une epoch."""
       model.train()
       total_loss = 0
       
       pbar = tqdm(data_loader, desc=f"Epoch {epoch}")
       
       for images, targets in pbar:
           images = torch.stack([img.to(device) for img in images])
           
           # Pr√©parer les targets (batch de vecteurs [x_c, y_c, w, h])
           target_boxes = torch.stack([
               prepare_single_box_target(t).to(device) for t in targets
           ])  # [B, 4]
           
           # Forward
           predictions = model(images)  # [B, 4]
           
           # Loss MSE
           loss = criterion(predictions, target_boxes)
           
           # Backward
           optimizer.zero_grad()
           loss.backward()
           optimizer.step()
           
           total_loss += loss.item()
           pbar.set_postfix({'loss': f"{loss.item():.4f}"})
       
       return total_loss / len(data_loader)

   @torch.no_grad()
   def eval_simple_epoch(model, criterion, data_loader, device):
       """√âvalue le mod√®le."""
       model.eval()
       total_loss = 0
       
       for images, targets in tqdm(data_loader, desc="Validation"):
           images = torch.stack([img.to(device) for img in images])
           target_boxes = torch.stack([
               prepare_single_box_target(t).to(device) for t in targets
           ])
           
           predictions = model(images)
           loss = criterion(predictions, target_boxes)
           total_loss += loss.item()
       
       return total_loss / len(data_loader)

   # LANCER L'ENTRA√éNEMENT
   print("\nüöÄ Entra√Ænement du mod√®le simple...\n")

   num_epochs = 20
   best_val_loss = float('inf')

   for epoch in range(num_epochs):
       train_loss = train_simple_epoch(
           simple_model, criterion, optimizer, train_loader, device, epoch
       )
       
       val_loss = eval_simple_epoch(simple_model, criterion, val_loader, device)
       
       print(f"\nüìä Epoch {epoch}:")
       print(f"  Train Loss: {train_loss:.4f}")
       print(f"  Val Loss:   {val_loss:.4f}")
       
       # Sauvegarder le meilleur
       if val_loss < best_val_loss:
           best_val_loss = val_loss
           torch.save(simple_model.state_dict(), 'simple_bbox_regressor.pth')
           print("  ‚úÖ Meilleur mod√®le sauvegard√© !")

   print("\nüéâ Entra√Ænement termin√© !")
   print(f"üìÅ Mod√®le sauvegard√© : simple_bbox_regressor.pth")

.. note::

   üèãÔ∏è **Convergence**
   
   Avec ce mod√®le simple, vous devriez voir la loss descendre rapidement :
   
   - Epoch 0 : ~0.02-0.03 (loss MSE √©lev√©e)
   - Epoch 5 : ~0.005-0.01 (convergence)
   - Epoch 15+ : ~0.001-0.003 (mod√®le bien entra√Æn√©)
   
   Si la loss ne descend pas, v√©rifiez que vos donn√©es sont bien normalis√©es !

7.4. √âvaluation avec IoU
~~~~~~~~~~~~~~~~~~~

Calcul de l'**IoU moyen** (Intersection over Union) sur le test set.

.. code-block:: python

   def compute_iou(box1, box2):
       """Calcule l'IoU entre deux bo√Ætes [x1, y1, x2, y2]."""
       x1 = max(box1[0], box2[0])
       y1 = max(box1[1], box2[1])
       x2 = min(box1[2], box2[2])
       y2 = min(box1[3], box2[3])
       
       if x2 < x1 or y2 < y1:
           return 0.0
       
       inter = (x2 - x1) * (y2 - y1)
       area1 = (box1[2] - box1[0]) * (box1[3] - box1[1])
       area2 = (box2[2] - box2[0]) * (box2[3] - box2[1])
       union = area1 + area2 - inter
       
       return inter / (union + 1e-6)
   
   @torch.no_grad()
   def evaluate_on_test(model, test_dataset, device):
       """√âvalue le mod√®le sur le test set et calcule l'IoU moyen."""
       model.eval()
       ious = []
       
       for idx in range(len(test_dataset)):
           img, target = test_dataset[idx]
           
           # Pr√©diction
           pred = model(img.unsqueeze(0).to(device))[0].cpu()  # [4]
           
           # GT
           gt_boxes = target['boxes']  # [N, 4] en pixels
           gt_box = gt_boxes[0]  # Prendre la premi√®re bo√Æte
           
           # Convertir pr√©diction vers pixels [x1, y1, x2, y2]
           x_c, y_c, w, h = pred
           x1 = (x_c - w/2) * 224
           y1 = (y_c - h/2) * 224
           x2 = (x_c + w/2) * 224
           y2 = (y_c + h/2) * 224
           pred_box = [x1.item(), y1.item(), x2.item(), y2.item()]
           
           # Calculer IoU
           iou = compute_iou(pred_box, gt_box.tolist())
           ious.append(iou)
       
       mean_iou = torch.tensor(ious).mean().item()
       print(f"\nüìä IoU moyen sur le test set : {mean_iou:.3f}")
       print(f"  - IoU > 0.5 : {sum(1 for x in ious if x > 0.5)}/{len(ious)} images")
       print(f"  - IoU > 0.75 : {sum(1 for x in ious if x > 0.75)}/{len(ious)} images")
       
       return mean_iou
   
   # Charger le meilleur mod√®le et √©valuer
   simple_model.load_state_dict(torch.load('simple_bbox_regressor.pth'))
   evaluate_on_test(simple_model, test_dataset, device)

.. note::

   üìà **Interpr√©tation de l'IoU**
   
   - IoU > **0.5** : Bonne d√©tection (standard COCO)
   - IoU > **0.75** : Tr√®s bonne d√©tection
   - IoU > **0.9** : D√©tection quasi-parfaite
   
   Un mod√®le bien entra√Æn√© sur ce dataset simple devrait obtenir un IoU moyen > 0.8.

7.5. Visualisation
~~~~~~~~~~~~~~~~~~~


Affichage des pr√©dictions sur une grille d'images avec GT (vert) et pr√©dictions (rouge).

.. code-block:: python

   import matplotlib.pyplot as plt
   import matplotlib.patches as patches
   import numpy as np
   
   @torch.no_grad()
   def visualize_predictions(model, dataset, device, n=9):
       """Affiche n pr√©dictions avec les GT en vert et pr√©dictions en rouge."""
       model.eval()
       
       fig, axes = plt.subplots(3, 3, figsize=(12, 12))
       axes = axes.flatten()
       
       for i in range(n):
           img, target = dataset[i]
           
           # Pr√©diction
           pred = model(img.unsqueeze(0).to(device))[0].cpu()
           
           # Affichage image
           img_np = img.permute(1, 2, 0).cpu().numpy()
           # D√©normaliser (ImageNet)
           mean = torch.tensor([0.485, 0.456, 0.406])
           std = torch.tensor([0.229, 0.224, 0.225])
           img_np = img_np * std.numpy() + mean.numpy()
           img_np = np.clip(img_np, 0, 1)
           
           axes[i].imshow(img_np)
           axes[i].axis('off')
           
           # GT box (vert)
           gt_box = target['boxes'][0]
           x1, y1, x2, y2 = gt_box
           w_gt, h_gt = x2 - x1, y2 - y1
           rect_gt = patches.Rectangle((x1, y1), w_gt, h_gt,
                                       linewidth=2, edgecolor='green',
                                       facecolor='none', label='GT')
           axes[i].add_patch(rect_gt)
           
           # Predicted box (rouge)
           x_c, y_c, w_pred, h_pred = pred
           x1_pred = (x_c - w_pred/2) * 224
           y1_pred = (y_c - h_pred/2) * 224
           w_pred_pix = w_pred * 224
           h_pred_pix = h_pred * 224
           rect_pred = patches.Rectangle((x1_pred, y1_pred), w_pred_pix, h_pred_pix,
                                         linewidth=2, edgecolor='red',
                                         facecolor='none', linestyle='--', label='Pred')
           axes[i].add_patch(rect_pred)
       
       # L√©gende
       handles, labels = axes[0].get_legend_handles_labels()
       fig.legend(handles, labels, loc='upper center', ncol=2, fontsize=12)
       plt.tight_layout()
       plt.show()
   
   # Visualiser
   visualize_predictions(simple_model, test_dataset, device, n=9)

.. note::

   üé® **L√©gende**
   
   - **Vert** : Ground truth (annotation r√©elle)
   - **Rouge** (pointill√©) : Pr√©diction du mod√®le
   
   Si les bo√Ætes se superposent bien, le mod√®le fonctionne correctement !

.. slide::

üìñ 8. Entra√Ænement avec Faster R-CNN (torchvision)
----------------------

Nous allons maintenant entra√Æner un d√©tecteur d'objets avec **Faster R-CNN**, l'un des mod√®les les plus populaires et performants.

8.1. Qu'est-ce que Faster R-CNN ?
~~~~~~~~~~~~~~~~~~~

**Faster R-CNN** (Region-based Convolutional Neural Network) est un mod√®le **two-stage** :

**Stage 1 : Region Proposal Network (RPN)**

- Scanne l'image pour proposer des r√©gions susceptibles de contenir des objets
- G√©n√®re ~1000-2000 propositions de bo√Ætes

**Stage 2 : Classification et raffinement**

- Pour chaque proposition, pr√©dit la classe et affine les coordonn√©es
- Filtre les bo√Ætes redondantes (Non-Maximum Suppression)

.. image:: images/faster_rcnn_architecture.png
   :width: 80%
   :align: center
   :alt: Architecture Faster R-CNN

**Avantages** :

- Tr√®s pr√©cis, surtout sur petits objets
- Entra√Ænement possible avec peu de donn√©es (quelques centaines d'images)
- Architecture bien comprise et stable

**Inconv√©nients** :

- Plus lent que YOLO en inf√©rence (~5-10 fps)
- Plus complexe qu'un mod√®le one-stage

.. slide::

8.2. Charger un mod√®le pr√©-entra√Æn√©
~~~~~~~~~~~~~~~~~~~

torchvision fournit Faster R-CNN pr√©-entra√Æn√© sur COCO (80 classes). On va le fine-tuner sur nos propres classes.

.. code-block:: python

   import torch
   import torchvision
   from torchvision.models.detection import fasterrcnn_resnet50_fpn
   from torchvision.models.detection.faster_rcnn import FastRCNNPredictor

   def get_model(num_classes):
       """
       Charge Faster R-CNN et adapte la t√™te de classification.
       
       Args:
           num_classes: nombre de classes + 1 (pour le background)
                       Ex : 2 classes ‚Üí num_classes = 3
       """
       # Charger le mod√®le pr√©-entra√Æn√©
       model = fasterrcnn_resnet50_fpn(pretrained=True)
       
       # R√©cup√©rer le nombre de features en entr√©e de la t√™te de classification
       in_features = model.roi_heads.box_predictor.cls_score.in_features
       
       # Remplacer la t√™te par une nouvelle pour nos classes
       model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)
       
       return model

   # Exemple : 2 classes (bouteille, gobelet) + background
   model = get_model(num_classes=3)
   print(model)

**Explication** :

- ``pretrained=True`` : charge les poids entra√Æn√©s sur COCO (80 classes)
- On **remplace uniquement la derni√®re couche** pour nos classes
- Les couches pr√©c√©dentes (backbone ResNet50, RPN) sont d√©j√† tr√®s performantes et seront fine-tun√©es

üí° **Transfer learning** : on r√©utilise les connaissances du mod√®le (formes, textures, objets g√©n√©riques) pour acc√©l√©rer l'apprentissage sur notre t√¢che sp√©cifique.

.. slide::

8.3. Configuration de l'entra√Ænement
~~~~~~~~~~~~~~~~~~~

.. code-block:: python

   import torch
   from torch.utils.data import DataLoader

.. code-block:: python

   # üìä √âVALUATION COMPL√àTE DU CNN CUSTOM SUR TOUT LE TEST SET (depuis le notebook)

   import numpy as np

   def compute_iou_boxes(box1, box2):
       """
       Calcule l'IoU entre deux bo√Ætes.
       
       Args:
           box1, box2: [x1, y1, x2, y2]
       
       Returns:
           iou: score IoU (0-1)
       """
       x1_inter = max(box1[0], box2[0])
       y1_inter = max(box1[1], box2[1])
       x2_inter = min(box1[2], box2[2])
       y2_inter = min(box1[3], box2[3])
       
       if x2_inter < x1_inter or y2_inter < y1_inter:
           return 0.0
       
       inter_area = (x2_inter - x1_inter) * (y2_inter - y1_inter)
       
       box1_area = (box1[2] - box1[0]) * (box1[3] - box1[1])
       box2_area = (box2[2] - box2[0]) * (box2[3] - box2[1])
       
       union_area = box1_area + box2_area - inter_area
       
       iou = inter_area / (union_area + 1e-6)
       return iou

   # Charger le meilleur mod√®le
   custom_model.load_state_dict(torch.load('best_custom_cube_detector.pth'))
   custom_model.eval()
   
   print("üîç √âVALUATION SUR LE TEST SET COMPLET")
   print("="*60)
   
   # M√©triques globales
   total_gt_objects = 0
   total_detected = 0
   total_true_positives = 0
   iou_threshold = 0.5
   conf_threshold = 0.75  # seuil relev√© pour r√©duire les faux positifs
   #conf_threshold = 0.3  # exemple d'autre seuil
   
   all_ious = []
   detection_per_image = []
   
   print(f"\nüìà Test sur {len(test_dataset)} images...\n")
   
   # Tester sur chaque image
   for idx in range(len(test_dataset)):
       test_img, test_target = test_dataset[idx]
       test_img_tensor = test_img.unsqueeze(0).to(device)
       
       # Pr√©diction
       with torch.no_grad():
           predictions = custom_model(test_img_tensor)
           results = custom_model.decode_predictions(predictions, conf_threshold=conf_threshold, device=device)
       
       detected_boxes = results[0]['boxes'].cpu().numpy()
       detected_labels = results[0]['labels'].cpu().numpy()
       detected_scores = results[0]['scores'].cpu().numpy()
       
       gt_boxes = test_target['boxes'].cpu().numpy()
       gt_labels = test_target['labels'].cpu().numpy()
       
       # Compter les objets
       num_gt = len(gt_boxes)
       num_detected = len(detected_boxes)
       
       total_gt_objects += num_gt
       total_detected += num_detected
       
       detection_per_image.append({
           'image_idx': idx,
           'gt_count': num_gt,
           'detected_count': num_detected
       })
       
       # Calculer les True Positives (matching avec IoU)
       matched_gt = set()
       image_ious = []
       
       for det_box in detected_boxes:
           best_iou = 0
           best_gt_idx = -1
           
           for gt_idx, gt_box in enumerate(gt_boxes):
               if gt_idx in matched_gt:
                   continue
               
               iou = compute_iou_boxes(det_box, gt_box)
               
               if iou > best_iou:
                   best_iou = iou
                   best_gt_idx = gt_idx
           
           if best_iou >= iou_threshold:
               total_true_positives += 1
               matched_gt.add(best_gt_idx)
               image_ious.append(best_iou)
       
       all_ious.extend(image_ious)
       
       # Afficher les d√©tails de cette image
       print(f"Image {idx+1}/{len(test_dataset)}:")
       print(f"   Ground Truth: {num_gt} cubes")
       print(f"   D√©tections:   {num_detected} cubes")
       print(f"   True Positives: {len(image_ious)}")
       if image_ious:
           print(f"   IoU moyen:    {np.mean(image_ious):.3f}")
   
   # Calculer les m√©triques globales
   print("\n" + "="*60)
   print("üìä R√âSULTATS GLOBAUX")
   print("="*60)
   
   precision = total_true_positives / total_detected if total_detected > 0 else 0
   recall = total_true_positives / total_gt_objects if total_gt_objects > 0 else 0
   f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0
   mean_iou = np.mean(all_ious) if all_ious else 0
   
   print(f"\nüéØ M√©triques de d√©tection (IoU threshold = {iou_threshold}, conf = {conf_threshold}):")
   print(f"   Objets ground truth:  {total_gt_objects}")
   print(f"   Objets d√©tect√©s:      {total_detected}")
   print(f"   True Positives:       {total_true_positives}")
   print(f"   False Positives:      {total_detected - total_true_positives}")
   print(f"   False Negatives:      {total_gt_objects - total_true_positives}")
   
   print(f"\nüìà Scores:")
   print(f"   Precision:  {precision:.3f} ({total_true_positives}/{total_detected})")
   print(f"   Recall:     {recall:.3f} ({total_true_positives}/{total_gt_objects})")
   print(f"   F1-Score:   {f1_score:.3f}")
   print(f"   IoU moyen:  {mean_iou:.3f}")
   
   # Taux de d√©tection par image
   perfect_detections = sum(1 for d in detection_per_image if d['detected_count'] == d['gt_count'])
   print(f"\nüé® D√©tections parfaites (nombre exact): {perfect_detections}/{len(test_dataset)} images ({perfect_detections/len(test_dataset)*100:.1f}%)")
   
   # Visualisation de quelques r√©sultats
   print("\n" + "="*60)
   print("üñºÔ∏è  VISUALISATION DE 3 EXEMPLES DU TEST SET")
   print("="*60)
   
   fig, axes = plt.subplots(3, 2, figsize=(16, 18))
   
   for i, test_idx in enumerate(range(min(3, len(test_dataset)))):
       test_img, test_target = test_dataset[test_idx]
       test_img_tensor = test_img.unsqueeze(0).to(device)
       
       # Pr√©diction
       with torch.no_grad():
           predictions = custom_model(test_img_tensor)
           results = custom_model.decode_predictions(predictions, conf_threshold=conf_threshold, device=device)
       
       detected_boxes = results[0]['boxes']
       detected_labels = results[0]['labels']
       detected_scores = results[0]['scores']
       
       # Convertir l'image pour affichage
       img_np = test_img.permute(1, 2, 0).cpu().numpy()
       
       # Ground Truth
       ax = axes[i, 0]
       ax.imshow(img_np)
       ax.set_title(f'Image {test_idx+1} - Ground Truth ({len(test_target["boxes"])} cubes)', 
                    fontsize=14, weight='bold')
       
       for box, label in zip(test_target['boxes'], test_target['labels']):
           x1, y1, x2, y2 = box.tolist()
           width = x2 - x1
           height = y2 - y1
           
           rect = patches.Rectangle(
               (x1, y1), width, height,
               linewidth=3, edgecolor='green', facecolor='none'
           )
           ax.add_patch(rect)
           
           class_name = full_dataset.get_class_name(label.item())
           ax.text(x1, y1-5, f"{class_name}",
                   bbox=dict(facecolor='green', alpha=0.7),
                   fontsize=10, color='white', weight='bold')
       
       ax.axis('off')
       
       # Pr√©dictions
       ax = axes[i, 1]
       ax.imshow(img_np)
       ax.set_title(f'Image {test_idx+1} - Pr√©dictions ({len(detected_boxes)} cubes)', 
                    fontsize=14, weight='bold')
       
       colors = ['red', 'blue', 'orange', 'purple', 'yellow']
       
       for box, label, score in zip(detected_boxes, detected_labels, detected_scores):
           x1, y1, x2, y2 = box.tolist()
           width = x2 - x1
           height = y2 - y1
           
           color = colors[(label.item() - 1) % len(colors)]
           
           rect = patches.Rectangle(
               (x1, y1), width, height,
               linewidth=3, edgecolor=color, facecolor='none'
           )
           ax.add_patch(rect)
           
           class_name = full_dataset.get_class_name(label.item())
           label_text = f"{class_name}: {score:.2f}"
           ax.text(x1, y1-5, label_text,
                   bbox=dict(facecolor=color, alpha=0.7),
                   fontsize=10, color='white', weight='bold')
       
       ax.axis('off')
   
   plt.tight_layout()
   plt.savefig('test_set_evaluation.png', bbox_inches='tight', dpi=150)
   print(f"\n‚úÖ Visualisation sauvegard√©e : test_set_evaluation.png")
   plt.show()
   
   print("\n" + "="*60)
   print("‚ú® CONCLUSION")
   print("="*60)
   
   if f1_score > 0.8:
       print("üåü Excellent ! Le CNN custom d√©tecte tr√®s bien les cubes.")
   elif f1_score > 0.6:
       print("üëç Tr√®s bien ! Le CNN custom a de bonnes performances.")
   elif f1_score > 0.4:
       print("üëå Correct. Le CNN custom fonctionne mais pourrait √™tre am√©lior√©.")
   else:
       print("‚ö†Ô∏è  Le mod√®le n√©cessite plus d'entra√Ænement ou d'ajustements.")
   
   print(f"\nLe mod√®le a {custom_model.count_parameters():,} param√®tres et a √©t√©")
   print("entra√Æn√© from scratch sans transfer learning sur seulement")
   print(f"{len(train_dataset)} images d'entra√Ænement.")

7.6. Remarque ‚Äî Batch Normalization et seuil de confiance
           # Forward pass
           loss_dict = model(images, targets)
Qu‚Äôest-ce que la BatchNorm (BN) ? Elle normalise les activations par la moyenne/variance du mini-batch. Effets concrets: stabilise et acc√©l√®re l‚Äôentra√Ænement, aide la profondeur et agit comme une l√©g√®re r√©gularisation. Avec de tr√®s petits batchs (1‚Äì4), les statistiques de batch peuvent √™tre bruyantes et la calibration des scores varier.

Dans ce notebook, la BN est comment√©e par simplicit√© (lignes `#self.bn...`). Vous pouvez:

- Laisser SANS BN (comme dans le code ci-dessus): souvent plus de faux positifs ‚Üí utilisez un seuil plus haut en inf√©rence (‚âà 0.75 par d√©faut dans l‚Äô√©valuation).
- Activer AVEC BN: d√©commentez les lignes `self.bn*` et les passes `F.relu(self.bn*(...))` dans le forward. Scores souvent mieux calibr√©s ‚Üí un seuil plus bas est possible (‚âà 0.5‚Äì0.6).

Observation du notebook: ¬´sans BN √ßa fonctionne, mais avec BN les r√©sultats √©taient meilleurs¬ª. Pr√©sentez les deux aux √©tudiants et faites varier le seuil: plus bas avec BN, plus haut sans BN.

Alternative petits batchs: GroupNorm (``nn.GroupNorm``) ne d√©pend pas de la taille de batch et peut remplacer BatchNorm2d(C) par ``GroupNorm(8, C)``.
           losses.backward()
           optimizer.step()
           
           # Tracking
           total_loss += losses.item()
           pbar.set_postfix({
               'loss': f"{losses.item():.4f}",
               'loss_classifier': f"{loss_dict['loss_classifier'].item():.3f}",
               'loss_box_reg': f"{loss_dict['loss_box_reg'].item():.3f}",
               'loss_objectness': f"{loss_dict['loss_objectness'].item():.3f}",
               'loss_rpn_box_reg': f"{loss_dict['loss_rpn_box_reg'].item():.3f}"
           })
       
       return total_loss / len(data_loader)

   @torch.no_grad()
   def evaluate(model, data_loader, device):
       """√âvalue le mod√®le sur le set de validation."""
       model.train()  # Faster R-CNN n√©cessite train() m√™me en eval !
       total_loss = 0
       
       for images, targets in tqdm(data_loader, desc="Validation"):
           images = list(image.to(device) for image in images)
           targets = [{k: v.to(device) for k, v in t.items()} for t in targets]
           
           loss_dict = model(images, targets)
           losses = sum(loss for loss in loss_dict.values())
           total_loss += losses.item()
       
       return total_loss / len(data_loader)

   # Entra√Ænement principal
   best_loss = float('inf')
   
   for epoch in range(num_epochs):
       # Entra√Ænement
       train_loss = train_one_epoch(model, optimizer, train_loader, device, epoch)
       
       # Validation
       val_loss = evaluate(model, val_loader, device)
       
       # Mise √† jour du learning rate
       lr_scheduler.step()
       
       # Affichage
       print(f"\nEpoch {epoch}:")
       print(f"  Train Loss: {train_loss:.4f}")
       print(f"  Val Loss:   {val_loss:.4f}")
       print(f"  LR: {optimizer.param_groups[0]['lr']:.6f}")
       
       # Sauvegarder le meilleur mod√®le
       if val_loss < best_loss:
           best_loss = val_loss
           torch.save(model.state_dict(), 'best_model.pth')
           print("  ‚úì Meilleur mod√®le sauvegard√© !")
   
   print("\n‚úì Entra√Ænement termin√© !")

**D√©tails importants** :

- **4 losses** dans Faster R-CNN :
  
  - ``loss_classifier`` : classification des objets
  - ``loss_box_reg`` : r√©gression des bo√Ætes
  - ``loss_objectness`` : score objectness du RPN
  - ``loss_rpn_box_reg`` : r√©gression des proposals du RPN

- Le mod√®le doit rester en mode ``train()`` m√™me pour la validation (particularit√© de l'impl√©mentation torchvision)

.. warning::

   ‚ö†Ô∏è **M√©moire GPU limit√©e ?**
   
   Si vous obtenez une erreur "CUDA out of memory" :
   
   - R√©duire ``batch_size`` √† 2 ou 1
   - R√©duire la r√©solution des images
   - Utiliser des images de validation moins nombreuses

.. slide::

8.5. Surveiller l'entra√Ænement
~~~~~~~~~~~~~~~~~~~

Pour un suivi plus d√©taill√©, utilisez TensorBoard ou wandb :

.. code-block:: python

   from torch.utils.tensorboard import SummaryWriter

   # Cr√©er un writer TensorBoard
   writer = SummaryWriter('runs/detection_experiment_1')

   # Dans la boucle d'entra√Ænement, ajouter :
   for epoch in range(num_epochs):
       train_loss = train_one_epoch(model, optimizer, train_loader, device, epoch)
       val_loss = evaluate(model, val_loader, device)
       
       # Logger dans TensorBoard
       writer.add_scalar('Loss/train', train_loss, epoch)
       writer.add_scalar('Loss/val', val_loss, epoch)
       writer.add_scalar('Learning_rate', optimizer.param_groups[0]['lr'], epoch)
   
   writer.close()

   # Visualiser avec : tensorboard --logdir=runs

üí° **Quand arr√™ter l'entra√Ænement ?**

- La val_loss ne diminue plus pendant 3-5 epochs ‚Üí probablement converg√©
- La train_loss continue de baisser mais val_loss augmente ‚Üí overfitting
- Apr√®s 10-20 epochs pour un petit dataset

.. slide::

üìñ 9. Inf√©rence : utiliser le mod√®le entra√Æn√©
----------------------

Maintenant que notre mod√®le est entra√Æn√©, voyons comment l'utiliser pour d√©tecter des objets sur de nouvelles images.

9.1. Charger le mod√®le sauvegard√©
~~~~~~~~~~~~~~~~~~~

.. code-block:: python

   import torch
   from PIL import Image
   from torchvision.transforms import functional as F
   import matplotlib.pyplot as plt
   import matplotlib.patches as patches

   # Charger le mod√®le
   num_classes = 3  # 2 classes + background
   model = get_model(num_classes)
   model.load_state_dict(torch.load('best_model.pth'))
   
   device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')
   model.to(device)
   model.eval()  # Mode √©valuation (important !)
   
   print("‚úì Mod√®le charg√©")

.. slide::

9.2. Faire une pr√©diction
~~~~~~~~~~~~~~~~~~~

.. code-block:: python

   @torch.no_grad()
   def predict(image_path, model, device, threshold=0.5):
       """
       Effectue une pr√©diction sur une image.
       
       Args:
           image_path: chemin vers l'image
           model: mod√®le entra√Æn√©
           device: CPU ou GPU
           threshold: seuil de confiance minimum (0-1)
       
       Returns:
           boxes: tensor [N, 4] des bo√Ætes d√©tect√©es
           labels: tensor [N] des classes
           scores: tensor [N] des scores de confiance
       """
       # Charger et pr√©parer l'image
       img = Image.open(image_path).convert('RGB')
       img_tensor = F.to_tensor(img).unsqueeze(0).to(device)
       
       # Pr√©diction
       model.eval()
       predictions = model(img_tensor)[0]
       
       # Filtrer par score de confiance
       keep = predictions['scores'] > threshold
       boxes = predictions['boxes'][keep].cpu()
       labels = predictions['labels'][keep].cpu()
       scores = predictions['scores'][keep].cpu()
       
       return img, boxes, labels, scores

   # Exemple d'utilisation
   img, boxes, labels, scores = predict(
       'data/images/test/frame_00001.jpg',
       model,
       device,
       threshold=0.5
   )

   print(f"Objets d√©tect√©s : {len(boxes)}")
   for i, (box, label, score) in enumerate(zip(boxes, labels, scores)):
       x1, y1, x2, y2 = box.tolist()
       class_name = train_dataset.get_class_name(label.item())
       print(f"  {i+1}. {class_name} (conf: {score:.2f}) - bbox: [{x1:.0f}, {y1:.0f}, {x2:.0f}, {y2:.0f}]")

.. slide::

9.3. Visualiser les d√©tections
~~~~~~~~~~~~~~~~~~~

.. code-block:: python

   def visualize_predictions(img, boxes, labels, scores, class_names, threshold=0.5):
       """Affiche l'image avec les bo√Ætes d√©tect√©es."""
       fig, ax = plt.subplots(1, figsize=(12, 8))
       ax.imshow(img)
       
       # Couleurs pour chaque classe
       colors = ['red', 'blue', 'green', 'yellow', 'orange']
       
       for box, label, score in zip(boxes, labels, scores):
           if score < threshold:
               continue
           
           x1, y1, x2, y2 = box.tolist()
           width = x2 - x1
           height = y2 - y1
           
           # Dessiner la bo√Æte
           color = colors[(label.item() - 1) % len(colors)]
           rect = patches.Rectangle(
               (x1, y1), width, height,
               linewidth=3, edgecolor=color, facecolor='none'
           )
           ax.add_patch(rect)
           
           # Ajouter le label et le score
           class_name = class_names[label.item() - 1]
           label_text = f'{class_name} {score:.2f}'
           ax.text(
               x1, y1 - 5,
               label_text,
               bbox=dict(facecolor=color, alpha=0.7),
               fontsize=12, color='white', weight='bold'
           )
       
       plt.axis('off')
       plt.tight_layout()
       return fig

   # Utilisation
   class_names = train_dataset.classes
   fig = visualize_predictions(img, boxes, labels, scores, class_names, threshold=0.5)
   plt.savefig('detection_result.jpg', bbox_inches='tight', dpi=150)
   print("‚úì R√©sultat sauvegard√© : detection_result.jpg")

.. slide::

9.4. Traiter une vid√©o compl√®te
~~~~~~~~~~~~~~~~~~~

Pour d√©tecter des objets dans une vid√©o, on traite chaque frame :

.. code-block:: python

   import cv2
   from tqdm import tqdm

   def detect_in_video(video_path, model, device, output_path='output_video.mp4', threshold=0.5):
       """Applique la d√©tection sur chaque frame d'une vid√©o."""
       cap = cv2.VideoCapture(video_path)
       
       # Propri√©t√©s de la vid√©o
       width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
       height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
       fps = int(cap.get(cv2.CAP_PROP_FPS))
       total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
       
       # Cr√©er le writer pour la vid√©o de sortie
       fourcc = cv2.VideoWriter_fourcc(*'mp4v')
       out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))
       
       pbar = tqdm(total=total_frames, desc="Traitement vid√©o")
       
       model.eval()
       
       while cap.isOpened():
           ret, frame = cap.read()
           if not ret:
               break
           
           # Convertir BGR ‚Üí RGB
           frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
           img_pil = Image.fromarray(frame_rgb)
           
           # Pr√©diction
           _, boxes, labels, scores = predict_from_pil(img_pil, model, device, threshold)
           
           # Dessiner les bo√Ætes sur la frame
           for box, label, score in zip(boxes, labels, scores):
               if score < threshold:
                   continue
               
               x1, y1, x2, y2 = map(int, box.tolist())
               class_name = train_dataset.get_class_name(label.item())
               
               # Dessiner le rectangle
               cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)
               
               # Ajouter le label
               label_text = f'{class_name} {score:.2f}'
               cv2.putText(frame, label_text, (x1, y1-10),
                          cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)
           
           # √âcrire la frame
           out.write(frame)
           pbar.update(1)
       
       cap.release()
       out.release()
       pbar.close()
       
       print(f"‚úì Vid√©o trait√©e : {output_path}")

   @torch.no_grad()
   def predict_from_pil(img_pil, model, device, threshold=0.5):
       """Version de predict qui prend directement une image PIL."""
       img_tensor = F.to_tensor(img_pil).unsqueeze(0).to(device)
       predictions = model(img_tensor)[0]
       
       keep = predictions['scores'] > threshold
       boxes = predictions['boxes'][keep].cpu()
       labels = predictions['labels'][keep].cpu()
       scores = predictions['scores'][keep].cpu()
       
       return img_pil, boxes, labels, scores

   # Utilisation
   detect_in_video(
       'ma_video.mp4',
       model,
       device,
       output_path='video_with_detections.mp4',
       threshold=0.6
   )

.. slide::

üìñ 10. Alternative : utiliser YOLO pr√©-entra√Æn√©
----------------------

Jusqu'ici, nous avons construit notre propre d√©tecteur CNN. Mais si vous voulez des r√©sultats rapides avec moins de code, **YOLO** est une excellente alternative.

9.1. Pourquoi YOLO ?
~~~~~~~~~~~~~~~~~~~

**YOLO (You Only Look Once)** est une famille de mod√®les de d√©tection tr√®s populaires :

**Avantages** :

- ‚ö° **Tr√®s rapide** : 30-100 fps (temps r√©el)
- üéØ **Facile √† utiliser** : 3-4 lignes de code
- üì¶ **Pr√©-entra√Æn√©s** : excellents r√©sultats sur COCO out-of-the-box
- üîÑ **Fine-tuning simple** : export depuis Label Studio ‚Üí entra√Ænement en 2 commandes

**Inconv√©nients** :

- Moins bon que Faster R-CNN sur petits objets
- Moins de contr√¥le sur l'architecture

üí° **Quand utiliser YOLO ?**

- Vous avez besoin de d√©tection en temps r√©el
- Vous d√©butez et voulez des r√©sultats rapides
- Votre dataset a >500 images

üí° **Quand utiliser Faster R-CNN (ce que nous avons fait) ?**

- Vous voulez la meilleure pr√©cision possible
- Vous avez peu de donn√©es (<500 images)
- Vous voulez comprendre et contr√¥ler l'architecture

.. slide::

9.2. Installation et premier test
~~~~~~~~~~~~~~~~~~~

YOLO version 8 (ultralytics) est la plus r√©cente et facile √† utiliser :

.. code-block:: bash

   pip install ultralytics

Test rapide avec un mod√®le pr√©-entra√Æn√© :

.. code-block:: python

   from ultralytics import YOLO
   
   # Charger le mod√®le pr√©-entra√Æn√©
   model = YOLO('yolov8n.pt')  # n = nano (le plus petit et rapide)
   
   # D√©tecter sur une image
   results = model('data/images/frame_00001.jpg')
   
   # Afficher les r√©sultats
   results[0].show()  # Affiche l'image avec les bo√Ætes
   
   # Sauvegarder
   results[0].save('yolo_detection.jpg')

Les mod√®les disponibles :

- ``yolov8n.pt`` : nano (le plus rapide, ~6 MB)
- ``yolov8s.pt`` : small
- ``yolov8m.pt`` : medium
- ``yolov8l.pt`` : large
- ``yolov8x.pt`` : extra large (le plus pr√©cis, ~130 MB)

.. slide::

9.3. Fine-tuner YOLO sur vos donn√©es Label Studio
~~~~~~~~~~~~~~~~~~~

**√âtape 1 : Exporter depuis Label Studio en format YOLO**

1. Dans Label Studio, cliquer sur "Export"
2. Choisir le format **"YOLO"**
3. T√©l√©charger le fichier ZIP

Le ZIP contient :

.. code-block:: text

   yolo_export.zip
   ‚îú‚îÄ‚îÄ classes.txt      # Liste des classes
   ‚îú‚îÄ‚îÄ notes.json       # M√©tadonn√©es
   ‚îî‚îÄ‚îÄ labels/          # Un .txt par image
       ‚îú‚îÄ‚îÄ frame_00001.txt
       ‚îú‚îÄ‚îÄ frame_00002.txt
       ‚îî‚îÄ‚îÄ ...

**√âtape 2 : Organiser les donn√©es**

.. code-block:: text

   yolo_dataset/
   ‚îú‚îÄ‚îÄ images/
   ‚îÇ   ‚îú‚îÄ‚îÄ train/
   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ frame_00001.jpg
   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ...
   ‚îÇ   ‚îî‚îÄ‚îÄ val/
   ‚îÇ       ‚îú‚îÄ‚îÄ frame_00151.jpg
   ‚îÇ       ‚îî‚îÄ‚îÄ ...
   ‚îú‚îÄ‚îÄ labels/
   ‚îÇ   ‚îú‚îÄ‚îÄ train/
   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ frame_00001.txt
   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ...
   ‚îÇ   ‚îî‚îÄ‚îÄ val/
   ‚îÇ       ‚îú‚îÄ‚îÄ frame_00151.txt
   ‚îÇ       ‚îî‚îÄ‚îÄ ...
   ‚îî‚îÄ‚îÄ data.yaml  # Fichier de configuration

**√âtape 3 : Cr√©er le fichier** ``data.yaml``

.. code-block:: yaml

   path: /chemin/absolu/vers/yolo_dataset
   train: images/train
   val: images/val

   names:
     0: bouteille
     1: gobelet

**√âtape 4 : Entra√Æner**

.. code-block:: python

   from ultralytics import YOLO

   # Charger le mod√®le pr√©-entra√Æn√©
   model = YOLO('yolov8n.pt')

   # Entra√Æner (fine-tuning)
   results = model.train(
       data='yolo_dataset/data.yaml',
       epochs=50,
       imgsz=640,
       batch=16,
       name='detection_bouteille'
   )

**√âtape 5 : Utiliser le mod√®le entra√Æn√©**

.. code-block:: python

   # Charger le meilleur mod√®le
   model = YOLO('runs/detect/detection_bouteille/weights/best.pt')
   
   # Pr√©dire
   results = model('nouvelle_image.jpg', conf=0.5)
   results[0].show()

.. note::

   üí° **Comparaison YOLO vs Faster R-CNN**
   
   **YOLO** : rapide (30-100 fps), facile, bon pour temps r√©el
   
   **Faster R-CNN** : plus pr√©cis, meilleur sur petits objets, plus de contr√¥le
   
   **Conseil** : commencez par YOLO pour prototyper rapidement, puis passez √† Faster R-CNN si vous avez besoin de plus de pr√©cision.

.. slide::

üìñ 10. Conclusion et aller plus loin
----------------------

üéì **Ce que vous avez appris** :

1. ‚úÖ Diff√©rence entre classification et d√©tection d'objets
2. ‚úÖ Extraire des frames d'une vid√©o avec OpenCV
3. ‚úÖ Annoter des images avec Label Studio (workflow collaboratif)
4. ‚úÖ Comprendre le format JSON de Label Studio
5. ‚úÖ Cr√©er un Dataset PyTorch pour la d√©tection
6. ‚úÖ Entra√Æner Faster R-CNN avec transfer learning
7. ‚úÖ Faire de l'inf√©rence sur images et vid√©os
8. ‚úÖ Utiliser YOLO comme alternative rapide

üöÄ **Pour aller plus loin** :

- **M√©triques d'√©valuation** : mAP (mean Average Precision), IoU
- **Data augmentation** : rotation, flip, changement de luminosit√©
- **Post-processing** : NMS (Non-Maximum Suppression), filtrage par taille
- **Mod√®les avanc√©s** : Mask R-CNN (segmentation), DETR (Transformers)
- **D√©ploiement** : ONNX, TensorRT, optimisation pour mobile



