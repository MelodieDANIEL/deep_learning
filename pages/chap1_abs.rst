<<<<<<< HEAD
.. slide::
RÃ©sumÃ© des concepts clÃ©s du chapitre 1
======================================

.. slide::
ğŸ“– 1. Objectif de lâ€™apprentissage
----------------------

- Le but de l'apprentissage automatique est **dâ€™apprendre Ã  approximer une fonction** Ã  partir de donnÃ©es. On cherche Ã  trouver des **paramÃ¨tres** qui permettent au modÃ¨le de prÃ©dire correctement les sorties Ã  partir des entrÃ©es.

- Exemple simple : fonction linÃ©aire $$f(x) = a x$$. On veut trouver $$a$$ pour que $$f(x)$$ prÃ©dise les sorties observÃ©es.

- Types de problÃ¨mes :
  - **RÃ©gression** : prÃ©dire une valeur continue (ex. prix dâ€™une maison, tempÃ©rature).  
  - **Classification** : prÃ©dire une catÃ©gorie (ex. spam/non-spam, chat/chien/oiseau).

**Pourquoi on a besoin dâ€™apprentissage ?**  
Les donnÃ©es seules ne donnent pas directement le modÃ¨le. On calcule **lâ€™erreur de prÃ©diction** et on ajuste les paramÃ¨tres pour la rÃ©duire. Pour reprÃ©senter ces donnÃ©es et paramÃ¨tres dans PyTorch, on utilise des **tenseurs**, des tableaux multidimensionnels.

.. slide::
ğŸ“– 2. Tenseurs
----------------------

- Tableaux multidimensionnels reprÃ©sentant **donnÃ©es et paramÃ¨tres**.
- Exemples : ``zeros``, ``ones``, ``arange``, ``linspace``, alÃ©atoire.
- Manipulation : changer la forme, ajouter/supprimer dimensions, permuter, concatÃ©ner.
- Base de tous les calculs dans un modÃ¨le.


Pour savoir comment ajuster les paramÃ¨tres du modÃ¨le, on calcule **la perte**, puis ses **gradients**.

.. slide::
ğŸ“– 3. Fonction de perte
----------------------

- Mesure combien le modÃ¨le se trompe : **plus la perte est grande â†’ plus lâ€™erreur est importante**.
- Guide lâ€™apprentissage : le modÃ¨le ajuste ses paramÃ¨tres pour **minimiser la perte**.
- Exemples : **MSE** pour la rÃ©gression, **Cross-Entropy** pour la classification.

Pour minimiser la perte, on calcule le **gradient de la perte par rapport aux paramÃ¨tres**, grÃ¢ce Ã  la rÃ©tropropagation.

.. slide::
ğŸ“– 4. RÃ©tropropagation et gradients
----------------------

- **RÃ©tropropagation** : parcours du graphe computationnel du modÃ¨le pour calculer les gradients.  
- Exemple simple : $$f(x) = x^2$$  
  - $$x > 0$$ â†’ gradient positif â†’ diminuer x pour rÃ©duire la perte.  
  - $$x < 0$$ â†’ gradient nÃ©gatif â†’ augmenter x.  
  - Au minimum : gradient = 0.

- Ces gradients indiquent **la direction dans laquelle ajuster les paramÃ¨tres** pour rÃ©duire lâ€™erreur.

.. slide::
ğŸ“– 5. Autograd
----------------------

- **Autograd** : module de PyTorch qui gÃ¨re la diffÃ©rentiation automatique.
- Suit les opÃ©rations sur les tenseurs et construit un graphe computationnel.
- Permet de calculer les gradients de maniÃ¨re efficace avec ``.backward()``.

.. slide::
ğŸ“– 6. Optimisation
----------------------

- On utilise les gradients pour **modifier les paramÃ¨tres dans la direction qui rÃ©duit la perte**.
- Algorithme classique : **descente de gradient**, ou adaptatif comme **Adam**.


.. slide::
ğŸ“– 7. Boucle dâ€™entraÃ®nement
----------------------

1) Initialisation : Initialiser les paramÃ¨tres du modÃ¨le.

2) PrÃ©diction : Calculer la sortie du modÃ¨le pour les donnÃ©es dâ€™entrÃ©e.

3) Perte : Calculer la perte en comparant la sortie estimÃ©e avec la valeur attendue.

4) Gradients : Calculer les gradients de la perte via ``.backward()``.

5) Mise Ã  Jour : Mettre Ã  jour les paramÃ¨tres pour rÃ©duire la perte.

6) RÃ©pÃ©ter les Ã©tapes 2 Ã  5 jusquâ€™Ã  la convergence du modÃ¨le.

.. slide::
ğŸ“– 8. Utilisation de la carte GPU avec les tenseurs
---------------------------------------------------

- En utilisant un GPU, on peut accÃ©lÃ©rer considÃ©rablement les calculs, surtout pour les grands modÃ¨les et ensembles de donnÃ©es.

- La commande ``set_default_device(device)`` permet de dÃ©finir le GPU comme appareil par dÃ©faut pour les tenseurs, si ``device = "cuda"``.

- Pour transformer un Numpy array situÃ© sur le CPU en tenseur sur le GPU, utilisez ``torch.from_numpy(nom_array).to(device)``.

- Si vous souhaitez utiliser Matplotlib pour afficher les donnÃ©es contenues dans un tenseur sur le GPU, vous devez dâ€™abord transfÃ©rer ce tenseur vers le CPU et le convertir en Numpy array avec ``tensor.cpu().numpy()`` avant de le passer Ã  Matplotlib.

- Dans le cas oÃ¹ le tenseur a Ã©tÃ© crÃ©Ã© avec ``requires_grad=True``, vous devez utiliser ``tensor.detach().cpu().numpy()`` pour Ã©viter les erreurs liÃ©es au suivi des gradients.

ğŸ‘‰ Retenez : **Numpy = CPU, Torch = CPU/GPU, Matplotlib = Numpy**.  


