
.. .. raw:: html
..     <script type="text/javascript" src="https://livejs.com/live.js"></script>

.. slide::

Chapitre 7 - Hyperparam√®tres: Contr√¥ler et optimiser son entra√Ænement
================

üéØ Objectifs du Chapitre
----------------------


.. important::

   √Ä la fin de ce chapitre, vous saurez d√©finir et utiliser les concepts suivants pour contr√¥ler et optimiser l'entra√Ænement de vos mod√®les de Deep Learning : 

   - Early Stopping
   - Learning Rate Scheduling
   - Regularizers
   - Normalizers 

.. slide::

Introduction g√©n√©rale
---------------------

L'entra√Ænement d'un r√©seau de neurones d√©pend fortement des hyper-param√®tres, ces valeurs fix√©es avant l'entra√Ænement et qui influencent profond√©ment la performance, la stabilit√© et la vitesse de convergence du mod√®le. Contrairement aux param√®tres appris automatiquement (poids et biais), les hyper-param√®tres requi√®rent une ma√Ætrise conceptuelle et une exp√©rimentation r√©fl√©chie.

Dans ce cours, nous examinerons quatre leviers essentiels pour optimiser un entra√Ænement :

- Early Stopping
- Learning Rate Scheduling
- R√©gularisations
- Normalisations internes (BatchNorm, LayerNorm, etc.)

.. slide::

üìñ 1. Early Stopping
---------------------

L'early stopping consiste √† arr√™ter l'entra√Ænement lorsque les performances sur le jeu de validation cessent de s'am√©liorer. Il s'agit d'un moyen simple et efficace d'√©viter l'overfitting.

Par exemple, lorsque l'on ne sait pas combien d'√©poques un mod√®le doit √™tre entra√Æn√©, il est commun de fixer le nombre d'√©poques √† une valeur √©lev√©e (e.g., 2000). L'early stopping permet d'arr√™ter l'entra√Ænement pour √©viter du surapprentissage et √©conomiser les ressources de calcul (notamment GPU).

.. slide::

Objectifs
~~~~~~~~~

- Comprendre comment d√©tecter le surapprentissage (overfitting) pendant l'entra√Ænement.
- Savoir interrompre l'entra√Ænement au bon moment pour optimiser la g√©n√©ralisation.
- Utiliser un m√©canisme automatique d'arr√™t avec une ¬´ patience ¬ª.

.. figure:: images/earlystopping.png
   :align: center
   :width: 400px
   :alt: Early Stopping
   **Figure 1** : Early Stopping - Arr√™t de l'entra√Ænement lorsque la performance sur le jeu de validation se d√©grade.

.. slide::

Exemple de code dans la boucle d'entra√Ænement qui utilise l'Early Stopping :
.. code-block:: python

    import torch
    import numpy as np

    patience = 10
    best_val_loss = np.inf
    patience_cpt = 0

    for epoch in range(2000):
        train(...)
        val_loss = validate(...)

        # Il y a am√©lioration : on r√©initialise le compteur de patience et on sauvegarde le mod√®le
        if val_loss < best_val_loss: 
            best_val_loss = val_loss 
            patience_cpt = 0 
            torch.save(model.state_dict(), "best_model.pt")
        # Il n'y a pas d'am√©lioration : on incr√©mente le compteur de patience
        else:
            patience_cpt += 1
        # La patience est √©puis√©e : on arr√™te l'entra√Ænement
        if patience_cpt >= patience:
            print("Early stopping at epoch", epoch)
            break 

    model.load_state_dict(torch.load("best_model.pt"))

.. warning::

   ‚ö†Ô∏è **Le suivi de l'Early Stopping se fait sur le jeu de validation !**
   
    Ne jamais utiliser le jeu de test pour d√©cider d'arr√™ter l'entra√Ænement, car cela biaiserait l'√©valuation finale du mod√®le.

.. slide::

üìñ 2. Learning Rate Scheduler
---------------------

Le learning rate (pas d'apprentissage en fran√ßais) influence directement la vitesse et la stabilit√© de la convergence. Un scheduler modifie automatiquement sa valeur selon une strat√©gie.

.. note::
    **Rappel:** Le learning rate est un hyper-param√®tre crucial qui d√©termine la taille des pas effectu√©s lors de la mise √† jour des poids du mod√®le pendant l'entra√Ænement. Un learning rate trop √©lev√© peut entra√Æner une divergence, tandis qu'un learning rate trop faible peut ralentir la convergence.

    C'est la norme (i.e., longueur) du vecteur de mise √† jour des poids.

    .. figure:: images/LRissue.png
        :align: center
        :width: 800px
        :alt: SGD
        **Figure 2** : SGD - Illustration de la mise √† jour des poids avec un learning rate trop petit (gauche), trop grand (droite) et ajust√© dynamiquement (centre). La valeur du learning rate influence la norme du vecteur gradient.

R√©duire le Learning Rate au cours de l'entra√Ænement permet souvent d'am√©liorer la convergence et la performance finale du mod√®le. Plusieurs strat√©gies existent, telles que la r√©duction par palier, la r√©duction exponentielle, ou les m√©thodes bas√©es sur la performance (e.g., ReduceLROnPlateau).

.. slide::

PyTorch propose plusieurs classes de scheduler dans le module ``torch.optim.lr_scheduler``. Ces classes prennent n√©cessairement un optimiseur en argument lors de leur initialisation.

Une fois d√©clar√©, utiliser le scheduler dans la boucle d'entra√Ænement consiste √† appeler la m√©thode ``step()`` √† chaque √©poque (ou selon une autre fr√©quence, selon le scheduler).


.. code-block:: python
    optimizer = torch.optim.SGD(model.parameters(), lr=0.5)

    # StepLR -- R√©duction √† intervalles fixes
    step_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)

    # ReduceLROnPlateau -- R√©duction bas√©e sur la performance (patience)
    plateau_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
        optimizer, factor=0.5, patience=3
    )

    # ExponentialLR -- R√©duction exponentielle
    exp_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.95)

    # CosineAnnealingLR -- R√©duction bas√©e sur le temps, suivant une loi cosinuso√Ødale
    cosine_time_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=50)

    # LinearLR -- R√©duction lin√©aire
    linear_scheduler = torch.optim.lr_scheduler.LinearLR(optimizer, start_factor=1.0, end_factor=0.1, total_iters=100)

    # Utilisation du scheduler dans la boucle d'entra√Ænement
    for epoch in range(50):
        train_loss = train(...)
        val_loss = validate(...)

        scheduler.step(val_loss) # Pour ReduceLROnPlateau, on passe aussi la m√©trique de validation

Voici une illustration du comportement de ces diff√©rentes strat√©gies au cours de l'entra√Ænement :

.. figure:: images/lrscheduler.png
        :align: center
        :width: 800px
        :alt: Schedulers strategies
        **Figure 3** : Strat√©gies de Learning Rate Scheduling - Illustration des diff√©rentes strat√©gies de r√©duction du learning rate au cours de l'entra√Ænement.

.. slide::

Bonus : Momentum
^^^^^^^^^^^^^^^^^

Le momentum est une technique compl√©mentaire qui aide √† acc√©l√©rer la convergence en accumulant une "vitesse" dans la direction des gradients. C'est une **inertie (ou √©lan)** qui permet de lisser les mises √† jour des poids en fonction des gradients pr√©c√©dents. Tous les optimiseurs bas√©s sur le gradient (e.g., SGD) peuvent int√©grer le momentum. 


.. figure:: images/momentum.png
        :align: center
        :width: 800px
        :alt: Momentum effect
        **Figure 4** : Effet du Momentum - Illustration de la mise √† jour des poids avec et sans momentum.


.. warning::
    ‚ö†Ô∏è Il est important de faire attention aux valeurs des gradients lorsque l'on utilise √† la fois un optimiseur avec momentum et un Learning Rate Scheduler. Bien que cela soit la plupart du temps efficace, cela multiplie les sources de variations dans les mises √† jour des poids, ce qui peut parfois d√©stabiliser l'entra√Ænement.


.. slide::

üìñ 3. Regularizers
---------------------

Les Regularizers (r√©gularisateurs en fran√ßais)1 p√©nalisent la complexit√© du mod√®le en ajoutant une contrainte aux poids. Ils aident √† pr√©venir l'overfitting en limitant la capacit√© du mod√®le √† s'adapter trop √©troitement aux donn√©es d'entra√Ænement. 

Les r√©gularisations les plus courantes sont : 

- **L2 Regularization** (Ridge) : p√©nalise la somme des carr√©s des poids, encourageant des poids plus petits et r√©partis.
- **L1 Regularization** (Lasso) : p√©nalise la somme des valeurs absolues des poids, favorisant la sparsit√© (beaucoup de poids deviennent exactement z√©ro).
- **Dropout** : technique qui consiste √† "√©teindre" al√©atoirement certains neurones pendant l'entra√Ænement, ce qui aide √† pr√©venir la co-adaptation des neurones et am√©liore la g√©n√©ralisation.

.. slide::

3.1. L2 Regularization
~~~~~~~~~~~~~~~~~~~

La r√©gularisation L2 ajoute une p√©nalit√© proportionnelle √† la somme des carr√©s des poids du mod√®le dans la fonction de perte. La formule est donn√©e par :

.. math::

    L_{total} = L_{original} + \lambda \sum_{i} w_i^2

o√π :

- $$L_{total}$$ est la nouvelle fonction de perte avec r√©gularisation.
- $$L_{original}$$ est la fonction de perte originale (e.g., erreur quadratique moyenne, entropie crois√©e, etc.).
- $$\lambda$$ est le coefficient de r√©gularisation (hyper-param√®tre √† ajuster).
- $$w_i$$ repr√©sente les poids du mod√®le.

Cette r√©gularisation encourage les poids √† √™tre petits, ce qui r√©duit la complexit√© du mod√®le et aide √† pr√©venir l'overfitting.

En PyTorch, la r√©gularisation L2 peut √™tre facilement appliqu√©e en utilisant le param√®tre ``weight_decay`` (correspondant √† $$\lambda$$ dans la formule ci-dessus.) lors de la cr√©ation de l'optimiseur :
.. code-block:: python
    optimizer = torch.optim.SGD(model.parameters(), lr=0.01, weight_decay=0.001)


.. slide::

3.2. L1 Regularization
~~~~~~~~~~~~~~~~~~~

La r√©gularisation L1 ajoute une p√©nalit√© proportionnelle √† la somme des valeurs absolues des poids du mod√®le dans la fonction de perte. La formule est donn√©e par :

.. math::

    L_{total} = L_{original} + \lambda \sum_{i} |w_i|

o√π :

- $$L_{total}$$ est la nouvelle fonction de perte avec r√©gularisation.
- $$L_{original}$$ est la fonction de perte originale (e.g., erreur quadratique moyenne, entropie crois√©e, etc.).
- $$\lambda$$ est le coefficient de r√©gularisation (hyper-param√®tre √† ajuster).
- $$w_i$$ repr√©sente les poids du mod√®le.

Cette r√©gularisation encourage les poids √† valoir 0, ce qui favorise la sparsit√© dans le mod√®le. Cela peut √™tre utile pour s√©lectionner automatiquement les caract√©ristiques les plus importantes dans les donn√©es.

Contrairement √† la r√©gularisation L2, PyTorch ne propose pas de param√®tre weight_decay pour la r√©gularisation L1. Il est donc n√©cessaire de l'impl√©menter manuellement comme montr√© ci-dessous : 

.. code-block:: python
    l1_lambda = 0.001
    for epoch in range(epochs):
        optimizer.zero_grad()
        output = model(input)
        loss = criterion(output, target)

        # Ajout de la r√©gularisation L1
        l1_norm = sum(p.abs().sum() for p in model.parameters())
        loss = loss + l1_lambda * l1_norm

        loss.backward()
        optimizer.step()

.. slide::

.. note::
    üß† Pour r√©capituler. 

    **L1 Regularization** :

        - P√©nalise la somme des valeurs absolues des poids.
        - Encourage la **sparsit√©**, c'est-√†-dire que de nombreux poids deviennent exactement √©gaux √† z√©ro.
        - Utile lorsque vous souhaitez obtenir un mod√®le √©conome ou effectuer une s√©lection automatique des caract√©ristiques.

    **L2 Regularization** :

        - P√©nalise la somme des carr√©s des poids.
        - Encourage des poids plus petits et r√©partis, mais rarement √©gaux √† z√©ro.
        - Utile pour r√©duire la complexit√© du mod√®le tout en conservant toutes les caract√©ristiques.

    **Quand utiliser l'une ou l'autre ?**

    **L1 Regularization** :

        - Lorsque vous travaillez avec des donn√©es comportant de nombreuses caract√©ristiques inutiles ou redondantes.
        - Lorsque vous souhaitez interpr√©ter le mod√®le en identifiant les caract√©ristiques les plus importantes.

    **L2 Regularization** :

        - Lorsque vous souhaitez √©viter l'overfitting tout en conservant toutes les caract√©ristiques.
        - Lorsque vous travaillez avec des mod√®les o√π la sparsit√© n'est pas une priorit√©.

    Les deux approches peuvent bien entendu √™tre combin√©es pour b√©n√©ficier de leurs avantages respectifs.

.. slide::

3.3. Dropout
~~~~~~~~~~~

Le Dropout (dilution en fran√ßais) est une technique de r√©gularisation qui consiste √† "√©teindre" al√©atoirement un pourcentage de neurones dans un r√©seau pendant l'entra√Ænement. Cela emp√™che les neurones de co-adapter leurs poids, ce qui am√©liore la g√©n√©ralisation du mod√®le.

Le dropout est g√©n√©ralement appliqu√© apr√®s une couche d'activation (e.g., ReLU) et avant la couche suivante.
Cette technique n'est utilis√©e que pendant l'entra√Ænement. Lors de l'inf√©rence (√©valuation), tous les neurones sont actifs.

.. math::

    y_i = 
    \begin{cases} 
    z_i & \text{avec probabilit√© 1-p} \\
    0 & \text{avec probabilit√© p}
    \end{cases}

o√π :

- $$y_i$$ est la sortie du neurone apr√®s application du dropout.
- $$z_i$$ est la sortie initiale du neurone avant dropout.
- $$p$$ est la probabilit√© de diluer (i.e., ignorer) un neurone (hyper-param√®tre √† ajuster).

.. warning::
    ‚ö†Ô∏è Ici c'est bien la sortie du neurone (feature map) qui est dilu√©e, et non la valeur des poids.
    En d√©montrera l'impl√©mentation o√π le Dropout est appliqu√©e sur les caract√©ristiques (features) des donn√©es.
    
.. slide::

En PyTorch, le Dropout peut √™tre facilement appliqu√© comme une couche en utilisant la classe ``nn.Dropout``. Voici un exemple d'utilisation :

.. code-block:: python

    import torch
    import torch.nn as nn

    class My_Network(nn.Module):
        def __init__(self):
            super().__init__()
            self.fc1 = nn.Linear(784, 512)
            self.drop = nn.Dropout(0.5) # Couche de Dropout avec une proba p=0.5 de dilution
            self.fc2 = nn.Linear(512, 10)

        def forward(self, x):
            x = torch.relu(self.fc1(x))
            x = self.drop(x) # Application du Dropout **uniquement pendant l'entra√Ænement** (model.train())
            return self.fc2(x)

Comme on peut le voir dans le code ci-dessus, la couche de Dropout est appliqu√©e sur les caract√©ristiques (features) des donn√©es ``x = self.drop(x)``.
Cette couche se d√©sactive automatiquement lors de l'√©valuation du mod√®le (i.e., mode √©valuation avec ``model.eval()``).

.. slide::

üìñ 4. Normalizers
---------------------

4.1. Probl√®me de distribution
~~~~~~~~~~~~~~~~~~~~~~

Les r√©seaux de neurones sont des machines statistiques sensibles aux distributions des donn√©es, √† leur domaine de valeur. On peut d√©finir deux types de distributions de donn√©es :

- In-distribution (ID) : donn√©es qui suivent la distribution que le mod√®le a l'habitude de voir et est capable de traiter efficacement.
- Out-of-distribution (OOD) : donn√©es qui suivent une distribution que le mod√®le n'a jamais vu et qu'il ne traite donc pas efficacement.

.. note:: 
    Cette distinction ID/OOD est vraie pour les domaines de valeur mais √©galement pour la distribution globale des caract√©ristiques d'une donn√©e. 

    Par exemple : Soit un r√©seau entra√Æn√© √† classer des chiens et chats sur des images dont les canaux RGB sont normalis√©s entre 0 et 1. 

    1. Si on lui pr√©sente une **image de chien** avec des canaux RGB **entre 0 et 1**, l'image est in-distribution (ID).
    2. Si on lui pr√©sente une **image de chien** avec des canaux RGB **entre 0 et 255**, l'image est out-of-distribution (OOD) car le mod√®le n'a jamais vu ce type de donn√©es.
    3. Si on lui pr√©sente une **image d'oiseau** avec des canaux RGB **entre 0 et 1**, l'image est √©galement out-of-distribution (OOD) car le mod√®le n'a jamais vu ce type de donn√©es.

    Avec les normalizers, nous allons nous int√©resser au domaine de valeur des caract√©ristiques internes (i.e., sorties des couches interm√©diaires) d'un r√©seau de neurones (normaliser le cas 2. ci-dessus).

Pourtant, il est normal que les distributions des caract√©ristiques internes (i.e., sorties des couches interm√©diaires) varient au cours de l'entra√Ænement puisqu'elles sont d√©pendantes des poids qui sont mis √† jour √† chaque it√©ration. Ce changement de distribution interne est appel√© le **internal covariate shift** (d√©calage interne des covariables en fran√ßais) et peut ralentir l'entra√Ænement.

.. slide::

4.2. Normalizers pour contrer l'internal covariate shift
~~~~~~~~~~~~~~~~~~~~~~

Les Normalizers (normalisateurs en fran√ßais) sont des techniques utilis√©es pour standardiser ou normaliser les activations des couches interm√©diaires d'un r√©seau de neurones, et contrebalancent donc l'internal covariate shift.

Pour normaliser ces distributions internes, la plupart des Normalizers se basent sur la m√™me formule : 

.. math::

    \hat{x} = \frac{x - \mu(x)}{\sigma(x) + \epsilon} * \gamma + \beta

o√π :

- $$\hat{x}$$ est la valeur normalis√©e.
- $$x$$ est la valeur d'entr√©e (activation de la couche interm√©diaire).
- $$\mu(x)$$ est la moyenne des activations.
- $$\sigma(x)$$ est l'√©cart-type des activations.
- $$\epsilon$$ est une petite constante pour √©viter la division par z√©ro.
- $$\gamma$$ et $$\beta$$ sont des param√®tres appris qui permettent de redimensionner et de recentrer les activations normalis√©es. Le mod√®le a donc une capacit√© de d√©crire le domaine de valeur qu'il est capable de traiter.

.. slide::

Diff√©rents Normalizers se distinguent par la mani√®re dont ils calculent $$\mu$$ et $$\sigma$$, ainsi que par le moment o√π ils sont appliqu√©s dans le r√©seau.

Les normalizers les plus courants sont :

- **Batch Normalization (BatchNorm)** : normalise les activations en utilisant la moyenne et l'√©cart-type calcul√©s **sur un mini-batch de donn√©es**. Cela aide √† stabiliser et acc√©l√©rer l'entra√Ænement.
- **Layer Normalization (LayerNorm)** : normalise les activations en utilisant la moyenne et l'√©cart-type calcul√©s **sur toutes les caract√©ristiques d'une seule donn√©e**. Utile pour les architectures r√©currentes, souvent utilis√© en natural language processing.
- **Instance Normalization (InstanceNorm)** : normalise les activations en utilisant la moyenne et l'√©cart-type calcul√©s **sur chaque canal d'une seule donn√©e**. Souvent utilis√© en computer vision.
- **Group Normalization (GroupNorm)** : divise les canaux en groupes et normalise les activations au sein de chaque groupe. Utile lorsque la taille du batch est petite.

En PyTorch, ces normalizers sont disponibles dans le module ``torch.nn``. Voici un exemple d'utilisation de la Batch Normalization :

.. code-block:: python
    import torch
    import torch.nn as nn

    class ImageNormalizerNetwork(nn.Module):
        def __init__(self):
            super().__init__()
            self.batch_norm = nn.BatchNorm2d(3)  # Normalisation des 3 canaux sur tout le batch
            self.layer_norm = nn.LayerNorm([3, 224, 224])  # Normalisation pour chaque √©chantillon du batch (tous les canaux)
            self.instance_norm = nn.InstanceNorm2d(3)  # Normalisation pour chaque canal de chaque √©chantillon
            self.group_norm = nn.GroupNorm(1, 3)  # Normalisation de groupe (1 groupe pour 3 canaux)

        def forward(self, x):
            x_batch_norm = self.batch_norm(x)  # Apply Batch Normalization
            x_layer_norm = self.layer_norm(x)  # Apply Layer Normalization
            x_instance_norm = self.instance_norm(x)  # Apply Instance Normalization
            x_group_norm = self.group_norm(x)  # Apply Group Normalization

            return x_batch_norm, x_layer_norm, x_instance_norm, x_group_norm
