
.. slide::

Chapitre 3 - Classification
================

üéØ Objectifs du Chapitre
----------------------


.. important::

   √Ä la fin de ce chapitre, vous saurez : 

   - D√©finir un probleme de classification.
   - Mettre en place un pipeline de classification avec PyTorch.
   - √âvaluer les performances d'un mod√®le de classification.   

.. slide::
üìñ 1. Classification - D√©finition
----------------------
La classification est une t√¢che fondamentale en apprentissage supervis√© o√π l'objectif est de pr√©dire une cat√©gorie ou une classe √† laquelle appartient une observation donn√©e, en se basant sur des donn√©es d'entr√©e. Contrairement √† la r√©gression, qui vise √† pr√©dire une valeur dans un domaine continu, la classification pr√©dit une valeur discr√®te.

L√† o√π la r√©gression revient √† trouver une courbe reliant tous les points, la classification revient √† trouver la (ou les) courbes permettant de s√©parer les diff√©rentes classes.

Discr√®te ? Pas tout √† fait ! En r√©alit√©, un mod√®le de classification ne pr√©dit pas directement une classe, mais plut√¥t une probabilit√© pour chaque classe possible. Par exemple, dans un probl√®me de classification binaire (deux classes), le mod√®le peut pr√©dire une probabilit√© de 0.8 pour la classe 1 et 0.2 pour la classe 0. La classe finale est ensuite d√©termin√©e en appliquant un seuil (par exemple, 0.5) : si la probabilit√© de la classe 1 est sup√©rieure √† 0.5, l'observation est class√©e dans la classe 1, sinon dans la classe 0.

.. slide::
üìñ 2. Pr√©dire une classe - One-Hot Encoding
----------------------
Imaginon un probl√®me de classification √† 3 classes... Comment repr√©senter la variable cible ?

Avec ce que nous connaissons d√©j√†, nous pourrions √™tre tent√©s d'encoder les classes arbitrairement comme suit, et demander au mod√®le de pr√©dire une unique valeur (par r√©gression) :

- Classe A : 0
- Classe B : 1
- Classe C : 2

Cependant, cette mod√©lisation comporte au moins deux probl√®mes majeurs : 

- Elle introduit une notion d'ordre entre les classes (0 < 1 < 2), ce qui n'a pas de sens dans un contexte de classification o√π les classes sont simplement des cat√©gories distinctes sans hi√©rarchie.
- On ne saurait pas comment interpr√©ter des valeurs flotantes interm√©diaires (1.5).

.. slide::
Le One-Hot Encoding est une technique de pr√©traitement des donn√©es utilis√©e pour convertir des variables cat√©gorielles en un format num√©rique que les algorithmes d'apprentissage automatique peuvent comprendre. Cette m√©thode est particuli√®rement utile lorsque les cat√©gories n'ont pas d'ordre intrins√®que, comme les couleurs, les types de fruits, ou les classes dans un probl√®me de classification (voir Figure 1).

Le principe du One-Hot Encoding est de cr√©er une nouvelle colonne pour chaque cat√©gorie unique dans la variable cat√©gorielle. Pour chaque observation, la colonne correspondant √† la cat√©gorie de cette observation est marqu√©e par un 1 (indiquant la pr√©sence de cette cat√©gorie), tandis que toutes les autres colonnes sont marqu√©es par un 0 (indiquant l'absence de ces cat√©gories). Par exemple, si nous avons une variable "Animal" avec les cat√©gories "Chien", "Oiseau", et "Chat", le One-Hot Encoding produira trois nouvelles colonnes : "Animal_Chien", "Animal_Oiseau", et "Animal_Chat".

.. figure:: images/one_hot.png
   :align: center
   :width: 400px
   :alt: Illustration du One-Hot Encoding

   **Figure 1** : Illustration du One-Hot Encoding.

.. slide::
On demande alors au mod√®le d'apprentissage de pr√©dire une probabilit√© qu'une donn√©e appartienne √† chaque classe. Par exemple, pour une observation donn√©e, le mod√®le pourrait pr√©dire les probabilit√©s suivantes :

.. figure:: images/classif.png
   :align: center
   :width: 400px
   :alt: Illustration d'une classification

   **Figure 2** : Exemple d'une classification pour un mod√®le d'apprentissage supervis√©.

Ici, le mod√®le pr√©dit une probabilit√© de 0.3 pour la classe Chien, 0.6 pour la classe Oiseau, et 0.1 pour la classe Chat. La classe finale *pr√©dite* est d√©termin√©e en choisissant la classe avec la probabilit√© la plus √©lev√©e (dans ce cas, Oiseau).

.. slide::
Pour s'assurer que les sorties du mod√®le sont bien des probabilit√©s, on applique souvent une fonction d'activation comme la softmax √† la couche de sortie du mod√®le. La fonction **softmax** convertit les scores bruts (appel√©s **logits**) en probabilit√©s en s'assurant que toutes les valeurs sont positives et que leur somme est √©gale √† 1.

.. math::
   softmax(z)_i = \frac{e^{z_i}}{\sum_{j=1}^{K} e^{z_j}}
o√π $$z$$ est le tenseur de sortie du mod√®le, $$z_i$$ est le score brut pour la classe $$i$$ dans ce tenseur, et $$K$$ est le nombre total de classes.

.. slide::
Dans la pratique, cette fonction d'activation finale n'est n√©cessaire que si la fonction de co√ªt utilis√©e pour entra√Æner le mod√®le ne l'inclut pas d√©j√† (comme c'est le cas avec la Cross-Entropy Loss en PyTorch).
Il suffit donc d'adapter la couche de sortie du mod√®le pour qu'elle produise un vecteur de taille √©gale au nombre de classes (sans appliquer de fonction d'activation comme la softmax en PyTorch).

.. code-block:: python
   import torch
   import torch.nn.functional as F

   class SimpleClassifMLP(torch.nn.Module):
      def __init__(self, input_dim, num_classes=3):
         super().__init__()
         self.fc1 = torch.nn.Linear(input_dim, 16)
         self.out_layer = torch.nn.Linear(16, num_classes) # couche de sortie pour "num_classes"

      def forward(self, x):
         x = F.relu(self.fc1(x))
         x = self.out_layer(x)
         return x  # logits pour "num_classes"
      

.. slide::
üìñ 3. Optimiser et √©valuer un mod√®le de classification supervis√©
----------------------

Traditionnellement, on dissocie les m√©triques d'optimisation de celles d'√©valuation en classification. En effet, les fonctions de co√ªt utilis√©es pour entra√Æner un mod√®le de classification ne sont pas n√©cessairement les m√™mes que celles utilis√©es pour √©valuer ses performances. 
Cette distinction est n√©cessaire car le mod√®le d'apprentissage a besoin d'une fonction de co√ªt diff√©rentiable pour ajuster ses poids via la r√©tropropagation, tandis que les m√©triques d'√©valuation peuvent √™tre non diff√©rentiables et plus adapt√©es √† la t√¢che sp√©cifique. En l'occurance, les m√©triques d'√©valuation en classification sont souvent bas√©es sur des seuils (par exemple, d√©terminer si une probabilit√© est sup√©rieure √† 0.5 pour classer une observation dans une classe particuli√®re), ce qui n'est pas diff√©rentiable.

.. slide::
3.1. Optimiser un mod√®le de classification (fonction de co√ªt)
~~~~~~~~~~~~~~~~~~~

En classification, la fonction de co√ªt la plus couramment utilis√©e est la **Cross-Entropy Loss** (ou entropie crois√©e). Cette fonction continue mesure la diff√©rence entre les distributions de probabilit√© pr√©dites par le mod√®le et les vraies distributions (celles des √©tiquettes r√©elles).

.. math::
   CrossEntropy(y, \hat{y}) = - \sum_{i=1}^{C} y_i \log(\hat{y}_i)
o√π $$C$$ est le nombre de classes, $$y_i$$ est la valeur binairee (0 ou 1) indiquant si la classe $$i$$ est la vraie classe, et $$\hat{y}_i$$ est la probabilit√© pr√©dite par le mod√®le pour la classe $$i$$.

.. slide::
En PyTorch, la Cross-Entropy Loss est impl√©ment√©e dans la classe `torch.nn.CrossEntropyLoss`, qui combine √† la fois la fonction softmax et le calcul de l'entropie crois√©e en une seule √©tape pour des raisons d'efficacit√© num√©rique.
Voici comment l'utiliser dans un pipeline de classification :
.. code-block:: python
   import torch
   import torch.nn as nn
   import torch.optim as optim

   # Supposons que nous avons un mod√®le, des donn√©es d'entr√©e et des √©tiquettes
   model = SimpleClassifMLP(input_dim=10, num_classes=3)
   inputs = torch.randn(5, 10)  # 5 √©chantillons, 10 caract√©ristiques chacun
   labels = torch.tensor([0, 2, 1, 0, 2])  # √©tiquettes r√©elles pour chaque √©chantillon

   # D√©finir la fonction de co√ªt et l'optimiseur
   criterion = nn.CrossEntropyLoss()
   optimizer = optim.Adam(model.parameters(), lr=0.001)

   # Phase d'entra√Ænement
   model.train()
   optimizer.zero_grad()  # R√©initialiser les gradients
   outputs = model(inputs)  # Obtenir les logits du mod√®le
   loss = criterion(outputs, labels)  # Calculer la perte
   loss.backward()  # R√©tropropagation
   optimizer.step()  # Mise √† jour des poids

‚ö†Ô∏è Notez que les labels doivent √™tre fournis sous forme d'indices de classes (entiers) et non sous forme de vecteurs one-hot. La fonction *CrossEntropyLoss* de PyTorch s'occupe √† la fois de convertir les logits en probabilit√©s (softmax) et de convertir les labels en vecteurs one-hot.

.. slide::
3.2. √âvaluer les performances d'un mod√®le de classification
~~~~~~~~~~~~~~~~~~~

Etant donn√© un mod√®le d'apprentissage, on souhaite √©valuer ses performances sur des donn√©es qu'il n'a jamais vues auparavant. Pour chaque √©chantillon, il y a donc 4 possibilit√©s :

- Vrai Positif (VP) : Le mod√®le pr√©dit la classe positive, et c'est correct.
- Faux Positif (FP) : Le mod√®le pr√©dit la classe positive, mais c'est incorrect.
- Vrai N√©gatif (VN) : Le mod√®le pr√©dit la classe n√©gative, et c'est correct.
- Faux N√©gatif (FN) : Le mod√®le pr√©dit la classe n√©gative, mais c'est incorrect.

.. figure:: images/vpfn.png
   :align: center
   :width: 400px
   :alt: Illustration des possibilit√©s d'erreur en classification

   **Figure 3** : Illustration des possibilit√©s d'erreur en classification, Vrai Positif (VP), Faux Positif (FP), Vrai N√©gatif (VN), Faux N√©gatif (FN). 

C'est sur la base de ces 4 possibilit√©s que sont d√©finies les principales m√©triques d'√©valuation en classification.

.. slide::
üìà **Exactitude (Accuracy)** : La proportion de pr√©dictions correctes par rapport au nombre total de pr√©dictions.

- **Objectif :** Maximiser le nombre de pr√©dictions correctes.
- **Int√©r√™t :** Simple √† comprendre et √† calculer.
- **Limite :** Peut √™tre trompeuse en cas de classes d√©s√©quilibr√©es. Exemple : si 95% des √©chantillons appartiennent √† la classe n√©gative, un mod√®le qui pr√©dit toujours la classe n√©gative aura une exactitude de 95%, mais ne sera pas utile.

.. slide::
üìà **Pr√©cision (Precision)** : La proportion de vraies pr√©dictions positives par rapport au nombre total de pr√©dictions positives.

- **Objectif :** Minimiser les faux positifs.
- **Int√©r√™t :** Utile lorsque les faux positifs co√ªtent cher.
- **Limite :** Ne prend pas en compte les faux n√©gatifs. Exemple : dans un test de d√©pistage d'une maladie rare, un mod√®le avec une haute pr√©cision minimisera les faux positifs, mais pourrait manquer de nombreux cas r√©els (faux n√©gatifs).

.. slide::
üìà **Rappel (Recall)** : La proportion de vraies pr√©dictions positives par rapport au nombre total d'exemples positifs.

- **Objectif :** Maximiser les vrais positifs.
- **Int√©r√™t :** Utile lorsque les faux n√©gatifs co√ªtent cher.
- **Limite :** Ne prend pas en compte les faux positifs. Exemple : dans un test de d√©pistage d'une maladie grave, un mod√®le avec un haut rappel minimisera les faux n√©gatifs, mais pourrait g√©n√©rer de nombreux faux positifs (le mod√®le alerte "√† tort").

.. slide::
üìà **Ratio de faux positifs (FPR)** : La proportion de fausses pr√©dictions positives par rapport au nombre total d'exemples n√©gatifs.

- **Objectif :** Minimiser les faux positifs.
- **Int√©r√™t :** Utile pour √©valuer la performance du mod√®le sur la classe n√©gative.
- **Limite :** Ne prend pas en compte les vrais positifs. Exemple : dans un syst√®me de d√©tection de fraude, un faible FPR est crucial pour √©viter d'alerter √† tort les utilisateurs l√©gitimes.

.. slide::
üìà **F1-score** : La moyenne harmonique de la pr√©cision et du rappel, utile lorsque les classes sont d√©s√©quilibr√©es.

- **Objectif :** Trouver un √©quilibre entre pr√©cision et rappel.
- **Int√©r√™t :** Utile lorsque les classes sont d√©s√©quilibr√©es et qu'il faut trouver un compromis entre √©viter les faux positifs et rater les vrais positifs.
- **Limite :** Ne distingue les faux positifs des faux n√©gatifs. Exemple : dans un syst√®me de recommandation, un F1-score √©lev√© indique que le mod√®le est bon pour recommander des √©l√©ments pertinents tout en minimisant les recommandations non pertinentes.


.. slide::
.. figure:: images/classif_metrics.png
   :align: center
   :width: 800px
   :alt: Mesures de performance en classification

   **Figure 4** : Mesures de performance en classification bas√©es sur les concepts de Vrai Positif (VP), Faux Positif (FP), Vrai N√©gatif (VN), et Faux N√©gatif (FN).

.. slide::
‚äû **Matrice de confusion**

La terminologie VP, FP, VN, FN s'applique naturellement aux probl√®mes de classification binaire. Pour les probl√®mes de classification multi-classes, on peut √©tendre ces concepts en utilisant une approche "un contre tous" (one-vs-all) pour chaque classe.
Par exemple, pour une classe sp√©cifique, on peut consid√©rer cette classe comme la classe positive et toutes les autres classes comme la classe n√©gative. On calcule alors VP, FP, VN, FN pour cette classe sp√©cifique. En r√©p√©tant ce processus pour chaque classe, on peut obtenir des m√©triques d'√©valuation pour chaque classe individuelle.

Une m√©thode classique pour visualiser la performance globale en classification multi-classes est la matrice de confusion. Il s'agit d'un tableau qui r√©sume les performances du mod√®le en affichant le nombre de pr√©dictions correctes et incorrectes pour chaque classe.


.. slide::
.. figure:: images/cm.png
   :align: center
   :width: 600px
   :alt: Matrice de confusion

   **Figure 5** : Matrice de confusion d'un mod√®le d'apprentissage sur un probl√®me de classification d'images √† 10 classes, sur un jeu de donn√©es √©quilibr√© (avec 1000 images par classe).

Chaque ligne de la matrice repr√©sente les instances dans une classe r√©elle, tandis que chaque colonne repr√©sente les instances dans une classe pr√©dite. La diagonale principale (de haut en gauche √† bas en droite) montre le nombre d'instances correctement class√©es pour chaque classe, tandis que les autres cellules montrent les erreurs de classification. Dans la Figure 5, on voit donc que 337 images de "Chat" ont √©t√© incorrectement class√©es comme "Chien". En revanche, les chiens ne sont pas consid√©r√©s comme des chats.

Cette technique permet de voir les classes qui sont souvent confondues entre elles, ce qui peut aider √† identifier les faiblesses du mod√®le et √† orienter les efforts d'am√©lioration.

‚ö†Ô∏è Dans un jeu de donn√©es d√©s√©quilibr√©, la matrice de confusion peut √™tre biais√©e en faveur des classes majoritaires. Par exemple, si une classe n'est repr√©sent√©e que par quelques √©chantillons de donn√©es, il est difficile de voir le nombre de faux n√©gatifs pour cette classe dans la matrice de confusion dont les couleurs sont √©talonn√©es en fonction de toutes les classes.

.. slide::
**Projection en 2D**

Rappel : Classiquement, un r√©seau de neurones profond est compos√© de couches r√©parties en deux phases souvent repr√©sent√©es en double D : 

- Une phase d'extraction de caract√©ristiques (couches cach√©es, le nombre de caract√©ristiques grandit pour permettre une meilleure description des donn√©es dans l'espace latent) 
- Une phase de r√©solution de t√¢che (couches finales, le nombre de caract√©ristiques diminue jusqu'√† atteindre la dimension souhait√©e pour la t√¢che, par exemple 1 pour une r√©gression ou K pour une classification).

Une autre mani√®re de visualiser les performances d'un mod√®le de classification est de projeter les donn√©es dans un espace 2D avec des algorithmes de r√©duction de dimension. 
Cette √©tape est r√©alis√©e les caract√©ristiques extraites par le mod√®le (derni√®re couche avant la phase de r√©solution de la t√¢che dans le mod√®le) car c'est ici que les donn√©es sont le mieux s√©par√©es.

La projection en 2D, r√©alis√©e avec des algorithmes comme t-SNE (t-Distributed Stochastic Neighbor Embedding),  UMAP (Uniform Manifold Approximation and Projection) ou PCA (Principal Component Analysis), garantit (jusqu'√† une certaine limite) que les distances observ√©es en 2D correspondent aux distances dans l'espace des caract√©ristiques. Ainsi, si deux points sont proches en 2D, ils devraient √©galement √™tre proches dans l'espace des caract√©ristiques, et vice versa. Il est alors possible d'observer les donn√©es qui, d'apr√®s le mod√®le d'apprentissage, sont similaires ou diff√©rentes. Id√©alement, les donn√©es similaires doivent avoir la m√™me classe.

.. slide::
.. figure:: images/tsne.png
   :align: center
   :width: 600px
   :alt: Projection 2D des donn√©es

   **Figure 6** : Projection 2D des donn√©es d'un mod√®le d'apprentissage sur un probl√®me de classification d'images √† 10 classes, sur un jeu de donn√©es √©quilibr√© (avec 1000 images par classe).

Dans la Figure 6, chaque point correspond √† une image. La couleur du point d√©termine la classe r√©elle de l'image (v√©rit√© terrain). On peut ainsi observer des groupes de donn√©es bien s√©par√©s des autres, ainsi que des groupes qui ont tendance √† se m√©langer (par exemple "Chien" et "Chat"). Gr√¢ce √† cette visualisation, on peut identifier les classes les mieux discrimin√©es ainsi que les erreurs de classification.

.. slide::
üìñ 4. Jeux de donn√©es
----------------------
Dans tout apprentissage, supervis√© ou non, la qualit√© et la quantit√© des donn√©es jouent un r√¥le crucial dans la performance du mod√®le. En classification, plusieurs d√©fis sp√©cifiques li√©s aux jeux de donn√©es peuvent influencer les r√©sultats.

.. slide::
4.1. G√©n√©ralisation et Validation
~~~~~~~~~~~~~~~~~~~
Bien qu'un mod√®le d'apprentissage puisse atteindre de bonnes performances sur son jeu d'entra√Ænement, il est essentiel de s'assurer qu'il poss√®de √©galement une bonne capacit√© √† **g√©n√©raliser** son apprentissage √† de nouvelles donn√©es.
On distingue alors les donn√©es *In distribution* (que le mod√®le a d√©j√† vues pendant son entra√Ænement) des donn√©es *Out of distribution* (que le mod√®le n'a jamais vues auparavant). Un bon mod√®le de classification doit √™tre capable de bien performer sur les deux types de donn√©es.
Par exemple dans le cas d'une voiture autonome, il faut s'assurer qu'un mod√®le entra√Æn√© √† reconna√Ætre des pi√©tons dans une ville en √©t√©, sera √©galement capable de les reconna√Ætre en hiver, de nuit, ou dans une autre ville.

La validation crois√©e est une technique utilis√©e pour √©valuer la capacit√© de g√©n√©ralisation d'un mod√®le d'apprentissage. Elle consiste √† diviser le jeu de donn√©es en plusieurs sous-ensembles (ou "folds"), puis √† entra√Æner et √©valuer le mod√®le plusieurs fois, en utilisant un fold diff√©rent pour l'√©valuation √† chaque it√©ration.

Cette approche permet de s'assurer que le mod√®le est capable de g√©n√©raliser son apprentissage √† de nouvelles donn√©es, en le testant sur des exemples qu'il n'a pas vus pendant l'entra√Ænement. Cela aide √† d√©tecter les probl√®mes de surapprentissage (overfitting) et √† ajuster les hyperparam√®tres du mod√®le pour am√©liorer sa performance sur des donn√©es non vues.

.. slide::
4.1.1. K-fold, Leave-K-Out (LKO), Leave-One-Out (LOO)
~~~~~~~~~~~~~~~~~~~

Une premi√®re famille de m√©thodes de validation est appel√©e **validation crois√©e** (cross-validation). Elle consiste √† diviser le jeu de donn√©es en plusieurs sous-ensembles, puis √† entra√Æner et √©valuer le mod√®le plusieurs fois, en utilisant un sous-ensemble diff√©rent pour l'√©valuation √† chaque it√©ration. Voici les principales variantes :

**K-Fold** : Le jeu de donn√©es est divis√© en K sous-ensembles ("folds"). √Ä chaque it√©ration, un fold sert de jeu de test et les K-1 autres de jeu d'entra√Ænement. On r√©p√®te l'op√©ration K fois, chaque fold √©tant utilis√© une fois comme test.

**Leave-K-Out (LKO)** : √Ä chaque it√©ration, K exemples sont retir√©s du jeu de donn√©es pour servir de test, et le reste sert √† l'entra√Ænement. On r√©p√®te l'op√©ration en changeant les K exemples test√©s √† chaque fois.

**Leave-One-Out (LOO)** : Cas particulier du LKO o√π K=1. Chaque exemple du jeu de donn√©es est utilis√© une fois comme test, les autres servant √† l'entra√Ænement, ce qui donne autant d'it√©rations que d'exemples.

Ces m√©thodes sont notamment utilis√©es en Machine Learning, avec de petits mod√®les et faibles volumes de donn√©es. Cependant, elles sont rarement utilis√©es en Deep Learning, o√π les mod√®les sont plus complexes et les volumes de donn√©es plus importants. En effet, ces m√©thodes peuvent √™tre tr√®s co√ªteuses en temps de calcul, car elles n√©cessitent d'entra√Æner le mod√®le plusieurs fois.

En Deep Learning, on pr√©f√®rera plus souvent utiliser une validation Hold-Out.

.. slide::
4.1.2. Hold-Out
~~~~~~~~~~~~~~~~~~~

La validation Hold-Out est une m√©thode simple et largement utilis√©e pour √©valuer la performance d'un mod√®le d'apprentissage. Elle consiste √† diviser le jeu de donn√©es en deux √† troies parties distinctes : 

- Un ensemble d'**entra√Ænement** (train set) : utilis√© pour entra√Æner le mod√®le.
- Un ensemble de **validation** (validation set) : utilis√© pour ajuster les hyperparam√®tres du mod√®le et pr√©venir le surapprentissage.
- Un ensemble de **test** (test set) : utilis√© pour √©valuer la performance finale du mod√®le.

A chaque √©poque, un mod√®le est entra√Æn√© (i.e., calcul de la loss et backpropagation) sur les donn√©es du *train set*. 

A la fin de chaque √©poque, le mod√®le est √©valu√© (i.e., calcul de la loss et des m√©triques, **sans backpropagation**) sur les donn√©es du *validation set*. Le mod√®le n'ayant jamais vu ces donn√©es, on peut ainsi estimer sa capacit√© √† g√©n√©raliser son apprentissage. 

Enfin, une fois l'entra√Ænement termin√©, le mod√®le est √©valu√© une derni√®re fois sur les donn√©es du *test set* pour obtenir une mesure finale de sa performance. Lorsque l'on con√ßoit plusieurs variantes de mod√®le d'apprentissage pour r√©soudre une t√¢che, c'est sur les performances sur le *test set* que l'on se base pour choisir le meilleur mod√®le.

En PyTorch, cela se traduit par la cr√©ation de trois DataLoaders distincts, un pour chaque ensemble de donn√©es, et le chainage des phases dans la boucle d'entrainement 

.. code-block:: python
   # Prepare train data
    train_dataset = ...
    train_loader = ...

    # Prepare validation data
    val_dataset = ...
    val_loader = ...

    # Prepare test data
    test_dataset = ...
    test_loader = ...

    # Define the model
    model = ...
    optimizer = ...
    loss_fn = ...

    # Train
    for epoch in range(n_epochs):
        model.train() #! Important !
        for i_batch, batch in enumerate(train_loader):
            inputs, groundtruthes = batch
            optimizer.zero_grad()  #! Important !
            pred = model(inputs)
            loss = loss_fn(pred, groundtruthes)
            loss.backward() #! Backpropagation !
            optimizer.step() 
    
        # Validation
        model.eval()  #! Important !
        with torch.no_grad(): # Don't compute the gradient (we won't backpropagate anyway)
            for vi_batch, batch in enumerate(val_loader):
                inputs, groundtruthes = batch
                pred = model(inputs)
                loss = loss_fn(pred, groundtruthes)
                compute_metrics(pred, groundtruthes)
    # End of train
    
    # Test
    model.eval() #! Important !
    with torch.no_grad(): # Don't compute the gradient (we won't backpropagate anyway)
        for i, batch in enumerate(test_loader):
            inputs, groundtruthes = batch
            pred = model(inputs)
            loss = loss_fn(pred, groundtruthes)
            compute_metrics(pred, groundtruthes)


.. slide::
4.2. D√©s√©quilibrage des classes
~~~~~~~~~~~~~~~~~~~

Dans un probl√®me de classification, il peut arriver que certaines classes soient beaucoup plus repr√©sent√©es que d'autres dans le jeu de donn√©es. Par exemple, dans un jeu de donn√©es m√©dical, il peut y avoir beaucoup plus de patients en bonne sant√© que de patients atteints d'une maladie rare. Ce d√©s√©quilibre peut poser plusieurs probl√®mes lors de l'entra√Ænement d'un mod√®le de classification :

- Le mod√®le peut √™tre biais√© en faveur des classes majoritaires, car il verra plus souvent ces exemples pendant l'entra√Ænement.
- Les m√©triques d'√©valuation peuvent √™tre trompeuses, car un mod√®le qui pr√©dit toujours la classe majoritaire peut obtenir une haute exactitude, mais ne sera pas utile pour d√©tecter les classes minoritaires.

Pour g√©rer le d√©s√©quilibre des classes, plusieurs techniques peuvent √™tre utilis√©es :

- **R√©√©chantillonnage** : On peut sur√©chantillonner les classes minoritaires (en dupliquant des exemples ou en g√©n√©rant de nouveaux exemples synth√©tiques) ou sous-√©chantillonner les classes majoritaires (en supprimant des exemples) pour √©quilibrer le jeu de donn√©es.
- **Pond√©ration des classes** : On peut attribuer des poids plus √©lev√©s aux classes minoritaires dans la fonction de co√ªt, de sorte que les erreurs sur ces classes aient un impact plus important lors de l'entra√Ænement.
- **Utilisation de m√©triques adapt√©es** : On peut utiliser des m√©triques d'√©valuation qui tiennent compte du d√©s√©quilibre des classes, comme le F1-score.

.. slide::
4.3. Augmentation des donn√©es
~~~~~~~~~~~~~~~~~~~

L'augmentation des donn√©es est une technique utilis√©e pour augmenter la taille et la diversit√© d'un jeu de donn√©es en appliquant des transformations aux exemples existants. En classification, l'augmentation des donn√©es peut aider √† am√©liorer la performance du mod√®le en lui fournissant plus d'exemples vari√©s √† apprendre, ce qui peut r√©duire le surapprentissage et am√©liorer la capacit√© de g√©n√©ralisation.

Les techniques courantes d'augmentation des donn√©es incluent :

- **Transformations g√©om√©triques** : rotation, translation, mise √† l'√©chelle, retournement horizontal/vertical.
- **Transformations du domaine de valeur** : ajustement du domaine de valeurs num√©riques des caract√©ristiques d'une donn√©e pour enrichir la diversit√© des exemples.
- **Bruit** : ajout de bruit al√©atoire aux donn√©es.
- **Cutout** : suppression al√©atoire de parties d'une donn√©e.

Ces techniques peuvent √™tre appliqu√©es de mani√®re al√©atoire pendant l'entra√Ænement, de sorte que chaque √©poque voit une version l√©g√®rement diff√©rente des donn√©es. Cela permet au mod√®le d'apprendre des caract√©ristiques de mieux g√©n√©raliser √† de nouvelles donn√©es et d'√™tre plus robustes aux petites variations d'environnement communes lors de la mise en production.

.. slide::
üìñ 5. Classification avanc√©e
----------------------

Jusqu'√† pr√©sent, nous avons principalement abord√© les probl√®mes de classification binaire (une classe vraie parmi deux) et multi-classes (une classe vraie parmi plusieurs). Cependant, il existe d'autres types de probl√®mes de classification qui pr√©sentent des d√©fis suppl√©mentaires.

.. slide::
5.1. Classification multi-label
~~~~~~~~~~~~~~~~~~~

Dans un probl√®me de classification multi-label, chaque donn√©e peut √™tre associ√©e √† plusieurs classes simultan√©ment. Par exemple, dans la classification d'images, une image peut contenir √† la fois un chat et un chien. Pour traiter ce type de probl√®me, plusieurs approches peuvent √™tre utilis√©es :

- **Sortie binaire par classe** : On peut entra√Æner un classificateur binaire distinct pour chaque classe. Chaque classificateur pr√©dit la pr√©sence ou l'absence de la classe correspondante. S'il y a $$K$$ classes, la sortie est alors de taille $$(K, 2)$$, o√π chaque ligne correspond √† une classe et contient deux valeurs, la probabilit√© d'appartenance √† la classe et probabilit√© de non-appartenance. C'est sur cette derni√®re dimension que l'on applique la fonction *softmax*. Cette approche est simple √† mettre en ≈ìuvre, mais elle ne capture pas les d√©pendances entre les classes.
- **Sortie multi-label** : On peut utiliser une seule couche de sortie pour pr√©dire la probabilit√© de chaque classe. Cela permet de capturer les d√©pendances entre les classes car le mod√®le peut apprendre √† reconna√Ætre des combinaisons de classes. La sortie est alors de taille $$(K)$$, o√π chaque √©l√©ment correspond √† la probabilit√© d'appartenance √† une classe. On applique alors une fonction **sigmo√Øde** (et non pas *softmax*) sur la sortie pour obtenir des probabilit√©s ind√©pendantes pour chaque classe. Pour s√©lectionner les classes pr√©dites, on applique un seuil de confiance (par exemple, 0.5) : si la probabilit√© d'une classe est sup√©rieure √† ce seuil, l'observation est class√©e dans cette classe.

.. slide::
5.1. Classification hi√©rarchique
~~~~~~~~~~~~~~~~~~~

Dans un probl√®me de classification hi√©rarchique, les classes sont organis√©es en une structure arborescente o√π certaines classes sont des sous-classes d'autres. Par exemple, dans la classification d'images, une image peut √™tre class√©e comme "animal", puis comme "mammif√®re", puis comme "chien". Pour traiter ce type de probl√®me, plusieurs approches peuvent √™tre utilis√©es :
- **Sortie multi-niveau** : On peut utiliser une seule couche de sortie pour pr√©dire la probabilit√© de chaque classe √† chaque niveau de la hi√©rarchie. La sortie est alors de taille $$(K_1 + K_2 + ... + K_n)$$, o√π $$K_i$$ est le nombre de classes au niveau $$i$$ de la hi√©rarchie. On applique une fonction *softmax* sur chaque sous-ensemble de la sortie correspondant √† un niveau de la hi√©rarchie pour obtenir des probabilit√©s pour chaque niveau. Pour s√©lectionner les classes pr√©dites, on choisit la classe avec la probabilit√© la plus √©lev√©e √† chaque niveau.
- **Plusieurs sorties** : On peut utiliser plusieurs couches de sortie, une pour chaque niveau de la hi√©rarchie. Chaque couche pr√©dit la probabilit√© des classes √† son niveau respectif. Cette approche est plus simple √† mettre en ≈ìuvre que les mod√®les hi√©rarchiques, mais elle ne capture pas les relations entre les classes.
- **Mod√®les hi√©rarchiques** : On peut entra√Æner un mod√®le pour chaque niveau de la hi√©rarchie. Par exemple, un mod√®le pour classer les images en "animal" ou "non-animal", puis un autre mod√®le pour classer les "animaux" en "mammif√®res" ou "non-mammif√®res", et ainsi de suite. Cette approche permet de capturer les relations entre les classes, mais elle peut √™tre complexe √† mettre en ≈ìuvre.