üèãÔ∏è Travaux Pratiques 5
=========================
.. slide::
Sur cette page se trouvent des exercices de TP sur le Chapitre 5. Ils sont class√©s par niveau de difficult√© :
.. discoverList::
    * Facile : üçÄ
    * Moyen : ‚öñÔ∏è
    * Difficile : üå∂Ô∏è

.. slide::
üçÄ Exercice 1 : Premier CNN pour la classification de chiffres
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Dans cet exercice, vous allez cr√©er un r√©seau de neurones convolutif (CNN) pour classifier des chiffres manuscrits du dataset MNIST.

**Objectif :** Comprendre la diff√©rence entre un MLP classique et un CNN sur des images.

On va travailler avec les donn√©es MNIST :

.. code-block:: python

    import torch
    import torch.nn as nn
    import torch.nn.functional as F
    from torchvision import datasets, transforms
    from torch.utils.data import DataLoader

    # Charger MNIST
    transform = transforms.Compose([transforms.ToTensor()])
    
    train_dataset = datasets.MNIST(root='./data', train=True, 
                                   download=True, transform=transform)
    test_dataset = datasets.MNIST(root='./data', train=False, 
                                  download=True, transform=transform)

.. note::
    Pour simplifier cet exercice d'introduction, nous utilisons uniquement les datasets train et test, mais en pratique il faudrait aussi un dataset de validation pour surveiller l'overfitting pendant l'entra√Ænement.

**Consigne :** √âcrire un programme qui :

.. step:: 
    1) Cr√©e deux mod√®les diff√©rents :
    
    - **MLP classique** : aplatit l'image ($$28√ó28$$ ‚Üí 784), puis 2 couches fully-connected de 128 neurones avec ReLU, sortie 10 classes
    - **CNN simple** : 
        
        - 1 couche convolutive (1‚Üí16 filtres, kernel $$3√ó3$$, padding=1)
        - ReLU + MaxPooling $$2√ó2$$
        - 1 couche convolutive (16‚Üí32 filtres, kernel $$3√ó3$$, padding=1)
        - ReLU + MaxPooling $$2√ó2$$
        - Aplatir puis 1 couche fully-connected vers 10 classes

.. step:: 
    2) Entra√Æne les deux mod√®les pendant 5 epochs avec :
    
    - Batch size de 64
    - Optimiseur Adam avec learning rate 0.001
    - Loss CrossEntropyLoss

.. step:: 
    3) Compare le nombre de param√®tres de chaque mod√®le (utiliser ``sum(p.numel() for p in model.parameters())``)

.. step::
    4) √âvalue la pr√©cision (accuracy) sur le test set pour les deux mod√®les

.. step::
    5) Affiche quelques exemples de pr√©dictions (correctes et incorrectes) pour chaque mod√®le


**Questions :**

.. step::
    6) Quel mod√®le a le moins de param√®tres ?

.. step::
    7) Quel mod√®le obtient la meilleure accuracy ?

.. step::
    8) Pourquoi le CNN est-il plus efficace malgr√© moins de param√®tres ?


**Astuce :**
.. spoiler::
    .. discoverList::
        1. Pour le MLP, utiliser ``x.view(x.size(0), -1)`` pour aplatir l'image
        2. Pour calculer la taille apr√®s convolution et pooling : avec 2 pooling $$2√ó2$$, une image $$28√ó28$$ devient $$7√ó7$$  
        3. Pour l'accuracy : ``(predicted == labels).sum().item() / len(labels)``
        4. Utiliser ``torch.no_grad()`` lors de l'√©valuation pour √©conomiser la m√©moire
        5. Le CNN pr√©serve la structure spatiale de l'image, ce qui est crucial pour la reconnaissance

**Astuce avanc√©e :**        
.. spoiler::
    .. discoverList:: 
        **Voici le code pour visualiser les pr√©dictions correctes et incorrectes :**
        
        .. code-block:: python
        
            import matplotlib.pyplot as plt
            
            # Fonction pour afficher des exemples de pr√©dictions
            def visualize_predictions(images, labels, predictions, model_name, num_examples=5, correct=True):
                """
                Affiche des exemples de pr√©dictions correctes ou incorrectes
                
                Args:
                    images: liste d'images
                    labels: vrais labels
                    predictions: pr√©dictions du mod√®le
                    model_name: nom du mod√®le (pour le titre)
                    num_examples: nombre d'exemples √† afficher
                    correct: True pour afficher les pr√©dictions correctes, False pour les incorrectes
                """
                # Trouver les indices selon le crit√®re
                if correct:
                    indices = [i for i in range(len(predictions)) 
                              if predictions[i] == labels[i]]
                    title_color = 'green'
                    main_title = f'{model_name} - Pr√©dictions CORRECTES'
                else:
                    indices = [i for i in range(len(predictions)) 
                              if predictions[i] != labels[i]]
                    title_color = 'red'
                    main_title = f'{model_name} - Pr√©dictions INCORRECTES'
                
                # Cr√©er la figure
                fig, axes = plt.subplots(1, num_examples, figsize=(15, 3))
                fig.suptitle(main_title, fontsize=14, fontweight='bold')
                
                # Afficher les exemples
                for i in range(min(num_examples, len(indices))):
                    idx = indices[i]
                    axes[i].imshow(images[idx].squeeze(), cmap='gray')
                    axes[i].set_title(f'Vrai: {labels[idx]}\\nPr√©dit: {predictions[idx]}', 
                                    color=title_color, fontweight='bold')
                    axes[i].axis('off')
                
                plt.tight_layout()
                plt.show()
            
            # Utilisation apr√®s √©valuation :
            # visualize_predictions(mlp_images, mlp_labels, mlp_preds, "MLP", correct=True)
            # visualize_predictions(mlp_images, mlp_labels, mlp_preds, "MLP", correct=False)


**R√©sultat attendu :** 

- MLP : environ 100k param√®tres, accuracy ~97%
- CNN : environ 20k param√®tres, accuracy ~98-99%


.. slide::
‚öñÔ∏è Exercice 2 : Comprendre l'effet du padding et du stride
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Dans cet exercice, vous allez explorer comment les param√®tres ``padding`` et ``stride`` affectent la taille des feature maps dans un CNN.

**Objectif :**  
Comprendre l'impact du padding et du stride sur les dimensions spatiales et visualiser les feature maps.

On va cr√©er un mini-dataset synth√©tique avec des formes g√©om√©triques :

.. code-block:: python

    import torch
    import matplotlib.pyplot as plt
    
    # Cr√©er des images synth√©tiques avec des formes
    def create_shape_image(shape_type='square'):
        img = torch.zeros(1, 1, 28, 28)
        if shape_type == 'square':
            img[0, 0, 10:18, 10:18] = 1.0
        elif shape_type == 'cross':
            img[0, 0, 14, :] = 1.0
            img[0, 0, :, 14] = 1.0
        elif shape_type == 'diagonal':
            for i in range(28):
                img[0, 0, i, i] = 1.0
        return img


**Consigne :** √âcrire un programme qui :

.. step::
    1) Cr√©e 3 images : un carr√©, une croix, une diagonale

.. step::
    2) D√©finit 4 configurations de convolution diff√©rentes :
    
    - Config A : kernel=3, stride=1, padding=0
    - Config B : kernel=3, stride=1, padding=1
    - Config C : kernel=3, stride=2, padding=0
    - Config D : kernel=5, stride=1, padding=2

.. step::
    3) Pour chaque configuration :
    
    - Applique la convolution avec 8 filtres sur une des images
    - Calcule et affiche la taille de sortie
    - V√©rifie avec la formule : $$H_{out} = \lfloor \frac{H_{in} + 2 \times padding - kernel\_size}{stride} \rfloor + 1$$

.. step::
    4) Visualise les 8 feature maps obtenues pour chaque configuration

.. step::
    5) Applique ensuite un MaxPooling $$2√ó2$$ apr√®s la convolution et observe la nouvelle taille


**Questions :**

.. step::
    6) Quelle configuration pr√©serve la taille spatiale de l'image ?

.. step::
    7) Quelle configuration r√©duit le plus la taille ?

.. step::
    8) Que se passe-t-il si on applique plusieurs convolutions successives sans padding ?

.. step::
    9) Pourquoi utilise-t-on souvent padding=1 avec kernel=3 ?


**Astuce :**
.. spoiler::
    .. discoverList::
        1. Utiliser ``nn.Conv2d(1, 8, kernel_size=k, stride=s, padding=p)``
        2. Pour visualiser : ``plt.imshow(feature_map[0, i].detach(), cmap='viridis')``
        3. Config B (padding=1, stride=1, kernel=3) pr√©serve la taille : $$28√ó28$$ ‚Üí $$28√ó28$$
        4. Sans padding, chaque convolution r√©duit la taille de (kernel_size - 1)
        5. padding=1 avec kernel=3 est un choix standard car il pr√©serve la taille


**R√©sultat attendu :**

Les tailles de sortie attendues pour une image $$28√ó28$$ :

- Config A (k=3, s=1, p=0) : $$26√ó26$$
- Config B (k=3, s=1, p=1) : $$28√ó28$$
- Config C (k=3, s=2, p=0) : $$13√ó13$$
- Config D (k=5, s=1, p=2) : $$28√ó28$$

Apr√®s MaxPooling $$2√ó2$$, les tailles sont divis√©es par 2.


.. slide::
üå∂Ô∏è Exercice 3 : CNN et Data Augmentation
~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Cet exercice vous guide d'un CNN et l'utilisation de data augmentation pour am√©liorer les performances.

**Objectif :**

    - Cr√©er une architecture CNN profonde avec blocs convolutifs
    - Impl√©menter et comparer l'entra√Ænement avec/sans data augmentation
    - G√©rer les datasets avec train/validation/test splits
    - Sauvegarder le meilleur mod√®le bas√© sur la validation

**Consigne :** √âcrire un programme qui :

.. step:: 
    1) Charge le dataset CIFAR-10 avec deux types de transformations :
    
    - **Sans augmentation** : seulement ToTensor et Normalize
    - **Avec augmentation** : RandomHorizontalFlip, RandomCrop(32, padding=4), ToTensor, Normalize

.. step::
    2) Divise le training set en train (80%) et validation (20%) avec ``random_split``

.. step::
    3) Cr√©e un CNN avec l'architecture suivante :
    
    .. code-block:: python
    
        # Bloc 1
        Conv2d(3, 64, kernel=3, padding=1) + ReLU
        Conv2d(64, 64, kernel=3, padding=1) + ReLU
        MaxPool2d(2, 2)  # 32√ó32 ‚Üí 16√ó16
        
        # Bloc 2
        Conv2d(64, 128, kernel=3, padding=1) + ReLU
        Conv2d(128, 128, kernel=3, padding=1) + ReLU
        MaxPool2d(2, 2)  # 16√ó16 ‚Üí 8√ó8
        
        # Bloc 3
        Conv2d(128, 256, kernel=3, padding=1) + ReLU
        Conv2d(256, 256, kernel=3, padding=1) + ReLU
        MaxPool2d(2, 2)  # 8√ó8 ‚Üí 4√ó4
        
        # Classification
        Flatten
        Linear(256 * 4 * 4, 512) + ReLU
        Dropout(0.5)
        Linear(512, 10)

.. step::
    4) Entra√Æne deux versions du mod√®le (10 epochs chacune) :
    
    - Mod√®le A : sans data augmentation
    - Mod√®le B : avec data augmentation

.. step::
    5) Pour chaque epoch, calcule et stocke :
    
    - Train loss et train accuracy
    - Validation loss et validation accuracy

.. step::
    6) Impl√©mente un syst√®me de sauvegarde qui garde le meilleur mod√®le bas√© sur la validation accuracy

.. step::
    7) Trace 4 courbes sur un m√™me graphique :
    
    - Train et validation loss (un subplot)
    - Train et validation accuracy (un autre subplot)
    - Faire cela pour les deux mod√®les A et B

.. step::
    8) √âvalue le meilleur mod√®le (A et B) sur le test set et affiche :
    
    - Test accuracy finale
    - Matrice de confusion
    - Classification report


**Questions :**

.. step::
    9) Quel mod√®le (A ou B) g√©n√©ralise mieux ? Comment le voyez-vous sur les courbes ?

.. step::
    10) Observe-t-on de l'overfitting ? Sur quel mod√®le et comment ?

.. step::
    11) Comment la data augmentation aide-t-elle √† r√©duire l'overfitting ?

.. step::
    12) Quelle est la diff√©rence de performance sur le test set ?


**Astuce :**
.. spoiler::
    .. discoverList::
    1. Pour CIFAR-10 : ``transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])``
    2. Pour la matrice de confusion : ``from sklearn.metrics import confusion_matrix, classification_report``
    3. Utiliser ``model.train()`` avant l'entra√Ænement et ``model.eval()`` avant l'√©valuation
    4. Sauvegarder avec : ``torch.save({'model_state_dict': model.state_dict(), 'accuracy': best_acc}, 'best_model.pth')``
    5. Le dropout aide aussi √† √©viter l'overfitting en d√©sactivant al√©atoirement 50% des neurones pendant l'entra√Ænement


**R√©sultats attendus :**

- **Mod√®le A** (sans augmentation) : test accuracy ~76-77%, **mais fort overfitting** (gap train/val de ~12-13%)
- **Mod√®le B** (avec augmentation) : test accuracy ~76-77%, **excellente g√©n√©ralisation** (gap train/val de seulement ~2%)
- **Observation importante** : Avec seulement 10 epochs, les deux mod√®les peuvent avoir des test accuracy similaires, **mais le Mod√®le B g√©n√©ralise beaucoup mieux**
- Le Mod√®le A montre des signes clairs d'overfitting apr√®s l'epoch 5 (train accuracy continue √† monter, val accuracy stagne)
- Le Mod√®le B a une meilleure loss sur le test set (~0.70 vs ~0.79), indiquant des pr√©dictions plus confiantes
- **Avec plus d'epochs (20-30)**, le Mod√®le B d√©passerait clairement A en test accuracy
- **Le√ßon importante** : L'accuracy seule ne suffit pas ! Analysez toujours le gap train/val pour √©valuer la vraie qualit√© du mod√®le


.. slide::
üèãÔ∏è Exercices suppl√©mentaires 5
===============================
Dans cette section, il y a des exercices suppl√©mentaires pour vous entra√Æner. Ils suivent le m√™me classement de difficult√© que pr√©c√©demment.


.. slide::
‚öñÔ∏è Exercice suppl√©mentaire 1 : Visualisation des filtres appris
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Cet exercice propose de visualiser ce que les filtres convolutifs ont appris apr√®s l'entra√Ænement.

**Objectif :** Comprendre ce que les filtres convolutifs d√©tectent dans les premi√®res couches d'un CNN.

**Consignes** :

.. step::
    1) Entra√Æner un CNN simple sur MNIST pendant 3 epochs :

    .. code-block:: python
    
        class SimpleCNN(nn.Module):
            def __init__(self):
                super().__init__()
                self.conv1 = nn.Conv2d(1, 16, kernel_size=3, padding=1)
                self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)
                self.fc = nn.Linear(32 * 7 * 7, 10)
            
            def forward(self, x):
                x = F.max_pool2d(F.relu(self.conv1(x)), 2)
                x = F.max_pool2d(F.relu(self.conv2(x)), 2)
                x = x.view(x.size(0), -1)
                x = self.fc(x)
                return x

.. step::
    2) Apr√®s l'entra√Ænement, extraire les poids de la premi√®re couche convolutive :

    .. code-block:: python
    
        filters = model.conv1.weight.data  # shape: [16, 1, 3, 3]

.. step::
    3) Visualiser les 16 filtres $$3√ó3$$ de la premi√®re couche sur une grille $$4√ó4$$

.. step::
    4) Prendre une image de test et visualiser les feature maps produites par la premi√®re couche convolutive :
    
    - Appliquer ``model.conv1(image)`` puis ``F.relu()``
    - Afficher les 16 feature maps obtenues

.. step::
    5) Faire de m√™me pour la deuxi√®me couche convolutive (afficher 32 feature maps sur une grille 4√ó8)


**Questions :**

.. step::
6) Que d√©tectent les filtres de la premi√®re couche ? (contours, textures, ...)

.. step::
    7) Les feature maps de la deuxi√®me couche sont-elles plus abstraites que celles de la premi√®re ?

.. step::
    8) Comment √©voluent les patterns d√©tect√©s entre les couches ?


**Astuce :**
.. spoiler::
    .. discoverList::
        1. Pour visualiser : ``plt.imshow(filters[i, 0].cpu(), cmap='gray')``
        2. Pour obtenir les feature maps : ``with torch.no_grad(): features = F.relu(model.conv1(image))``
        3. Les premiers filtres d√©tectent souvent des contours horizontaux, verticaux, diagonaux
        4. Les couches profondes d√©tectent des motifs plus complexes et abstraits
        5. Utiliser ``plt.subplots()`` pour cr√©er une grille de visualisation


**R√©sultats attendus :**

- Une grille montrant les 16 filtres $$3√ó3$$ de la premi√®re couche
- Une grille montrant les 16 feature maps activ√©es par une image
- Les filtres de la premi√®re couche devraient ressembler √† des d√©tecteurs de contours
- Les feature maps montrent quelles parties de l'image activent chaque filtre


.. slide::
üå∂Ô∏è Exercice suppl√©mentaire 2 : Transfer Learning avec un mod√®le pr√©-entra√Æn√©
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Dans cet exercice, vous allez utiliser un mod√®le pr√©-entra√Æn√© (ResNet18) et le fine-tuner sur un nouveau dataset.

.. warning::
    ‚è∞ **Attention : Temps d'entra√Ænement tr√®s long !**
    
    Cet exercice n√©cessite d'entra√Æner 2 mod√®les ResNet18 sur des images $$224√ó224$$, ce qui prend **beaucoup de temps** (plusieurs heures sans GPU, et reste long m√™me avec GPU). **Il n'est PAS possible de terminer l'entra√Ænement pendant la s√©ance de TP**. Il est recommand√© de :
    
    - üè† Lancer l'entra√Ænement √† la maison ou utiliser Google Colab avec GPU
    - ‚ö° R√©duire drastiquement √† 3-5 epochs pour tester rapidement (r√©sultats moins concluants)
    - üíæ Sauvegarder r√©guli√®rement les mod√®les pour reprendre plus tard

.. note::
    **ResNet18** est une architecture CNN de 18 couches qui a gagn√© le concours ImageNet en 2015. **ImageNet** est une immense base de 1.2 million d'images r√©parties en 1000 classes (animaux, v√©hicules, objets du quotidien). Le **transfer learning** consiste √† r√©utiliser les filtres appris sur ImageNet (qui d√©tectent contours, textures, formes g√©n√©riques) pour classifier CIFAR-10 (10 classes seulement) : au lieu de tout r√©apprendre depuis z√©ro, on adapte juste la derni√®re couche !

**Objectif** :

- Comprendre le concept de transfer learning
- Charger un mod√®le pr√©-entra√Æn√© et modifier sa derni√®re couche
- Comparer l'entra√Ænement from scratch vs transfer learning

**Consignes** :

.. step::
    1) Charger le dataset CIFAR-10 avec des transformations appropri√©es :

    .. code-block:: python
    
        from torchvision import models
        
        transform = transforms.Compose([
            transforms.Resize(224),  # ResNet attend du 224√ó224
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], 
                                std=[0.229, 0.224, 0.225])
        ])

.. step::
    2) Cr√©er deux mod√®les :
    
    - **Mod√®le A (from scratch)** : ResNet18 initialis√© al√©atoirement
    - **Mod√®le B (transfer learning)** : ResNet18 pr√©-entra√Æn√© sur ImageNet, on g√®le toutes les couches sauf la derni√®re

    .. code-block:: python
    
        # Mod√®le A
        model_scratch = models.resnet18(pretrained=False)
        model_scratch.fc = nn.Linear(model_scratch.fc.in_features, 10)
        
        # Mod√®le B
        model_transfer = models.resnet18(pretrained=True)
        # Geler toutes les couches
        for param in model_transfer.parameters():
            param.requires_grad = False
        # Remplacer la derni√®re couche et la d√©geler
        model_transfer.fc = nn.Linear(model_transfer.fc.in_features, 10)

.. step::
    3) Entra√Æner les deux mod√®les pendant 10 epochs avec :
    
    - Batch size 64
    - Adam optimizer, learning rate 0.001
    - Diviser train en train (80%) et validation (20%)

.. step::
    4) Pour chaque mod√®le, tracer :
    
    - Evolution de la train loss et validation loss
    - Evolution de la train accuracy et validation accuracy

.. step::           
    5) Comparer le temps d'entra√Ænement par epoch pour les deux mod√®les

.. step::
    6) √âvaluer les deux mod√®les sur le test set

.. step::
    7) Afficher une matrice de confusion pour chaque mod√®le


**Questions :**

.. step::
    8) Quel mod√®le converge le plus rapidement ?

.. step::
    9) Quel mod√®le atteint la meilleure accuracy finale ?

.. step::
    10) Pourquoi le transfer learning est-il plus efficace ?

.. step::
    11) Que se passerait-il si on d√©gelait aussi les couches interm√©diaires ?


**Astuce :**
.. spoiler::
    .. discoverList:: 
        1. Pour mesurer le temps : ``import time; start = time.time(); ... ; elapsed = time.time() - start``
        2. Le mod√®le pr√©-entra√Æn√© a d√©j√† appris des features g√©n√©riques sur ImageNet
        3. En gelant les couches, on a moins de param√®tres √† entra√Æner ‚Üí plus rapide
        4. Le transfer learning devrait donner ~80-85% accuracy en quelques epochs
        5. From scratch atteindra peut-√™tre ~70-75% apr√®s 10 epochs
        6. Si on d√©g√®le tout, on risque de d√©truire les features pr√©-apprises (sauf si learning rate tr√®s faible)


**R√©sultats attendus :**

- Mod√®le from scratch : convergence lente, accuracy finale ~70-75%
- Mod√®le transfer learning : convergence rapide (2-3 epochs), accuracy finale ~80-85%
- Temps par epoch : similaire pour les deux, mais transfer learning n√©cessite moins d'epochs
- Le gap train/validation devrait √™tre plus petit pour le transfer learning


.. slide::
üå∂Ô∏è Exercice suppl√©mentaire 3 : Early Stopping et Learning Rate Scheduler
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Dans cet exercice, vous allez impl√©menter un syst√®me complet d'entra√Ænement avec early stopping et ajustement dynamique du learning rate.

.. warning::
    ‚è∞ **Attention : Temps d'entra√Ænement long !**
    
    Cet exercice n√©cessite d'entra√Æner **3 mod√®les diff√©rents** sur CIFAR-10, ce qui prend **un temps cons√©quent** (le temps varie beaucoup selon votre GPU). **Il n'est pas possible de terminer l'entra√Ænement pendant la s√©ance de TP**. Options recommand√©es :
    
    - üè† Lancer l'entra√Ænement √† la maison ou utiliser Google Colab avec GPU
    - ‚ö° R√©duire √† 10-15 epochs max pour tester rapidement (r√©sultats moins concluants)
    - üíæ Sauvegarder r√©guli√®rement les mod√®les pour reprendre plus tard

**Objectif** :

- Impl√©menter un early stopping robuste pour √©viter l'overfitting
- Utiliser un learning rate scheduler pour am√©liorer la convergence
- Comparer diff√©rentes strat√©gies de training

**Consignes** :

.. step::
    1) Utiliser CIFAR-10 avec data augmentation et diviser en train/val/test (70%/15%/15%)

.. step::
    2) Cr√©er un CNN de taille moyenne :

    .. code-block:: python
    
        class MediumCNN(nn.Module):
            def __init__(self):
                super().__init__()
                self.features = nn.Sequential(
                    nn.Conv2d(3, 64, 3, padding=1),
                    nn.ReLU(),
                    nn.Conv2d(64, 64, 3, padding=1),
                    nn.ReLU(),
                    nn.MaxPool2d(2),
                    
                    nn.Conv2d(64, 128, 3, padding=1),
                    nn.ReLU(),
                    nn.Conv2d(128, 128, 3, padding=1),
                    nn.ReLU(),
                    nn.MaxPool2d(2),
                )
                self.classifier = nn.Sequential(
                    nn.Linear(128 * 8 * 8, 256),
                    nn.ReLU(),
                    nn.Dropout(0.5),
                    nn.Linear(256, 10)
                )
            
            def forward(self, x):
                x = self.features(x)
                x = x.view(x.size(0), -1)
                x = self.classifier(x)
                return x

.. step::
    3) Impl√©menter une classe ``EarlyStopping`` avec les param√®tres :
    
    - ``patience`` : nombre d'epochs sans am√©lioration avant d'arr√™ter
    - ``min_delta`` : am√©lioration minimale pour consid√©rer un progr√®s
    - Sauvegarder le meilleur mod√®le automatiquement

.. step::
    4) Entra√Æner 3 versions du mod√®le (max 50 epochs) :
    
    - **Mod√®le A** : learning rate constant 0.001, pas d'early stopping
    - **Mod√®le B** : learning rate constant 0.001, early stopping (patience=5)
    - **Mod√®le C** : learning rate avec ``ReduceLROnPlateau`` + early stopping (patience=7)

.. step::
    5) Pour chaque mod√®le, tracer sur un m√™me graphique :
    
    - Train et validation loss
    - Train et validation accuracy
    - Marquer l'epoch o√π l'entra√Ænement s'arr√™te (si early stopping)
    - Marquer les epochs o√π le learning rate change (mod√®le C)

.. step::
    6) Comparer les r√©sultats finaux sur le test set

.. step::
    7) Afficher pour chaque mod√®le :
    
    - Nombre total d'epochs entra√Æn√©es
    - Meilleure validation accuracy
    - Test accuracy finale
    - Temps d'entra√Ænement total


**Questions :**

.. step::
    8) Quel mod√®le √©vite le mieux l'overfitting ?

.. step::
    9) Quel est l'impact du learning rate scheduler ?

.. step::
    10) L'early stopping permet-il de gagner du temps d'entra√Ænement ?

.. step::
    11) Quelle strat√©gie recommanderiez-vous pour un nouveau projet ?


**Astuce :**
.. spoiler::
    .. discoverList:: 
        **Impl√©mentation de la classe EarlyStopping :**
        
        .. code-block:: python
        
            class EarlyStopping:
                def __init__(self, patience=5, min_delta=0, path='checkpoint.pth'):
                    self.patience = patience
                    self.min_delta = min_delta
                    self.path = path
                    self.counter = 0
                    self.best_loss = None
                    self.early_stop = False
                
                def __call__(self, val_loss, model):
                    if self.best_loss is None:
                        self.best_loss = val_loss
                        self.save_checkpoint(model)
                    elif val_loss > self.best_loss - self.min_delta:
                        self.counter += 1
                        if self.counter >= self.patience:
                            self.early_stop = True
                    else:
                        self.best_loss = val_loss
                        self.save_checkpoint(model)
                        self.counter = 0
                
                def save_checkpoint(self, model):
                    torch.save(model.state_dict(), self.path)
        
        **Pour le scheduler :**
        
        .. code-block:: python
        
            from torch.optim.lr_scheduler import ReduceLROnPlateau
            
            scheduler = ReduceLROnPlateau(optimizer, mode='min', 
                                         factor=0.5, patience=3, verbose=True)
            
            # Dans la boucle d'entra√Ænement, apr√®s validation :
            scheduler.step(val_loss)


**R√©sultats attendus :**

- Mod√®le A : overfitting apr√®s ~20 epochs, test accuracy ~75%
- Mod√®le B : s'arr√™te √† ~15-20 epochs, test accuracy ~78%
- Mod√®le C : s'arr√™te √† ~25-30 epochs avec LR r√©duit, test accuracy ~79-80%
- Mod√®le C devrait avoir la meilleure g√©n√©ralisation
- Le temps total d'entra√Ænement est r√©duit pour B et C par rapport √† A
