üèãÔ∏è Travaux Pratiques 6
=========================

.. slide::

Sur cette page se trouvent des exercices de TP sur le Chapitre 6 (D√©tection d'objets). Ils sont class√©s par niveau de difficult√© :

.. discoverList::
    * Facile : üçÄ
    * Moyen : ‚öñÔ∏è
    * Difficile : üå∂Ô∏è



############################

.. slide::

üçÄ Exercice 1 : Dataset avec pr√©sence/absence d'objet
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Dans cet exercice, vous allez cr√©er un dataset particulier o√π certaines images contiennent votre objet et d'autres **ne le contiennent pas**. Le mod√®le devra apprendre √† ne rien d√©tecter quand l'objet est absent.

**Objectif :** Entra√Æner un d√©tecteur robuste qui ne produit pas de faux positifs sur des images sans l'objet cible.

**Mat√©riel n√©cessaire :**

- Votre smartphone ou webcam
- Un objet √† d√©tecter (cube, balle, tasse, etc.)
- Un environnement vari√©

**Partie A : Cr√©ation du dataset**

**Consigne :** Cr√©er un dataset de 150 images r√©parties ainsi :

1) **100 images AVEC l'objet** :
   
   - Filmez une vid√©o de 30 secondes avec l'objet visible
   - Variez les angles, distances et positions
   - Extrayez 100 frames √©quidistantes avec OpenCV

2) **50 images SANS l'objet** :
   
   - Filmez le m√™me environnement sans l'objet (arri√®re-plans vari√©s)
   - OU : prenez des photos de sc√®nes al√©atoires (bureau, table, √©tag√®re...)
   - Extrayez/sauvegardez 50 images

3) **Annotation dans Label Studio** :
   
   - Cr√©ez un projet et importez les 150 images
   - Pour les images AVEC l'objet : dessinez la bo√Æte englobante
   - Pour les images SANS l'objet : soumettez l'image sans annotation (important !)
   - Exportez au format JSON

4) **V√©rification du dataset** :

.. code-block:: python

   import json
   import os
   from pathlib import Path

   def verify_dataset(json_path, images_dir):
       """
       V√©rifie le dataset et affiche les statistiques.
       
       Returns:
           dict avec statistiques
       """
       # TODO: Charger le JSON
       # TODO: Parcourir les annotations
       # TODO: Compter images avec/sans objet
       # TODO: Afficher les statistiques
       pass
   
   # V√©rification
   stats = verify_dataset('project-annotations.json', 'images/')

**Questions Partie A :**

1) Pourquoi est-il important d'avoir des images sans l'objet dans le dataset ?
2) Que se passerait-il si on entra√Ænait uniquement sur des images avec l'objet ?
3) Quel ratio pr√©sence/absence recommandez-vous ? (ex: 70/30, 50/50, 80/20 ?)

**Astuce Partie A :**

.. spoiler::
    .. discoverList::
        1. Pensez √† l'importance des exemples n√©gatifs pour √©viter les faux positifs
        2. R√©fl√©chissez au ratio optimal entre images avec et sans objet
        3. Dans Label Studio, une image vide doit √™tre soumise sans annotation

.. slide::

**Partie B : CNN Custom avec gestion de l'absence**

**Consigne :** Adapter le CNN simple du chapitre pour g√©rer l'absence d'objet.

**Approche :** Le mod√®le pr√©dit maintenant 5 valeurs :

- ``objectness`` : probabilit√© qu'un objet soit pr√©sent (0-1)
- ``x_center, y_center, width, height`` : coordonn√©es si objet pr√©sent

.. code-block:: python

   import torch
   import torch.nn as nn
   import torch.nn.functional as F

   class SimpleBBoxRegressorWithObjectness(nn.Module):
       """
       CNN qui pr√©dit la pr√©sence d'un objet + sa bo√Æte.
       Sortie : [objectness, x_center, y_center, width, height]
       """
       
       def __init__(self):
           super().__init__()
           
           # TODO: D√©finir les couches de convolution (backbone)
           # TODO: D√©finir les couches fully connected (head)
           # Rappel: 5 sorties [objectness, x, y, w, h]
           pass
       
       def forward(self, x):
           # TODO: Impl√©menter le forward pass
           # TODO: S√©parer objectness (sigmoid) et bbox (sigmoid)
           pass

**Fonction de pr√©paration des targets :**

.. code-block:: python

   def prepare_targets_with_objectness(targets, img_size=224):
       """
       Convertit les targets en format [objectness, x_c, y_c, w, h].
       Si aucune bo√Æte : objectness=0, bbox=[0, 0, 0, 0]
       """
       # TODO: Parcourir les targets
       # TODO: Si boxes vide : objectness=0
       # TODO: Sinon : objectness=1 + normaliser bbox
       pass

**Loss combin√©e :**

.. code-block:: python

   class DetectionLoss(nn.Module):
       """Loss pour d√©tection avec objectness."""
       
       def __init__(self):
           super().__init__()
           # TODO: D√©finir les losses (BCE pour objectness, MSE pour bbox)
           pass
       
       def forward(self, predictions, targets):
           """
           predictions: [B, 5] = [obj, x, y, w, h]
           targets: [B, 5] = [obj_gt, x_gt, y_gt, w_gt, h_gt]
           """
           # TODO: Calculer loss_obj (BCE sur objectness)
           # TODO: Calculer loss_bbox (MSE uniquement si objet pr√©sent)
           # TODO: Pond√©rer et combiner les losses
           pass

**Entra√Ænement :**

.. code-block:: python

   # TODO: Cr√©er le mod√®le, criterion, optimizer
   # TODO: Impl√©menter la boucle d'entra√Ænement
   # TODO: Sauvegarder le meilleur mod√®le
   pass

**√âvaluation et visualisation :**

.. code-block:: python

   @torch.no_grad()
   def evaluate_with_objectness(model, dataset, threshold=0.5, img_size=224):
       """√âvalue avec d√©tection de pr√©sence."""
       # TODO: Parcourir le dataset
       # TODO: Compter TP, FP, TN, FN
       # TODO: Calculer precision, recall, accuracy
       pass

**Questions Partie B :**

4) Pourquoi utilise-t-on une loss BCE pour objectness et MSE pour bbox ?
5) Pourquoi pond√©rer la loss_bbox par un facteur 5.0 ?
6) Que se passe-t-il si on met threshold=0.3 ? Et 0.8 ?
7) Comment interpr√©ter un mod√®le avec haute pr√©cision mais faible recall ?

**Astuce Partie B :**

.. spoiler::
    .. discoverList::
        1. R√©fl√©chissez aux types de losses appropri√©es pour classification vs r√©gression
        2. Pensez √† l'√©quilibrage entre les deux composantes de la loss
        3. Analysez l'impact du threshold sur les m√©triques (TP/FP/TN/FN)
        4. Interpr√©tez le trade-off entre pr√©cision et recall

.. slide::

**Partie C : YOLO avec images n√©gatives**

**Consigne :** Entra√Æner YOLOv11 sur le m√™me dataset et comparer.

1) **Exporter au format YOLO** depuis Label Studio :
   
   - Cliquez sur "Export" ‚Üí "YOLO"
   - T√©l√©chargez le ZIP

2) **Organiser le dataset** :

.. code-block:: python

   from pathlib import Path
   import shutil
   import random
   
   def organize_yolo_dataset(images_dir, labels_dir, output_dir, split=(0.7, 0.15, 0.15)):
       """
       Organise le dataset pour YOLO avec split train/val/test.
       G√®re automatiquement les images sans labels (n√©gatives).
       """
       # TODO: Cr√©er la structure de dossiers YOLO
       # TODO: Lister et m√©langer les images
       # TODO: Faire le split train/val/test
       # TODO: Copier images et labels (si existent)
       pass

3) **Cr√©er le fichier YAML** :

.. code-block:: yaml

   # data.yaml
   # TODO: Compl√©ter avec vos chemins
   path: /chemin/absolu/vers/data_yolo
   train: images/train
   val: images/val
   test: images/test
   
   nc: 1
   names: ['mon_objet']

4) **Entra√Æner YOLO** :

.. code-block:: python

   from ultralytics import YOLO
   
   # TODO: Charger YOLOv11n et entra√Æner
   # TODO: Choisir epochs, imgsz, batch appropri√©s
   pass

5) **√âvaluer et comparer** :

.. code-block:: python

   # TODO: Charger le meilleur mod√®le YOLO
   # TODO: √âvaluer sur le test set
   # TODO: Comparer avec les m√©triques du CNN custom
   pass

**Questions Partie C :**

8) Comment YOLO g√®re-t-il les images sans objet ?
9) Quel mod√®le est le plus rapide ? Le plus pr√©cis ?
10) Lequel utiliseriez-vous en production ? Pourquoi ?

**Astuce Partie C :**

.. spoiler::
    .. discoverList::
        1. Analysez comment YOLO traite les images sans annotation
        2. Comparez la vitesse d'inf√©rence entre CNN custom et YOLO
        3. R√©fl√©chissez aux avantages/inconv√©nients pour la production
        4. Consid√©rez le pr√©-entra√Ænement et son impact sur les performances

**R√©sultat attendu :**

- Dataset de 150 images (100 avec objet, 50 sans)
- Mod√®le CNN avec objectness entra√Æn√©
- Mod√®le YOLO entra√Æn√©
- Comparaison des m√©triques (accuracy, precision, recall)

.. slide::

‚öñÔ∏è Exercice 2 : D√©tection de deux objets diff√©rents
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Dans cet exercice, vous allez cr√©er un d√©tecteur capable de distinguer deux objets diff√©rents sur la m√™me image.

**Objectif :** Ma√Ætriser la d√©tection multi-classe et g√©rer plusieurs objets simultan√©s.

**Mat√©riel n√©cessaire :**

- Deux objets distincts visuellement (ex: cube rouge + balle bleue, tasse + bouteille)
- Smartphone ou webcam
- Environnement vari√©

**Partie A : Dataset multi-objets**

**Consigne :** Cr√©er un dataset de 200 images avec la r√©partition suivante :

1) **60 images avec l'objet 1 uniquement**
2) **60 images avec l'objet 2 uniquement**
3) **50 images avec les DEUX objets simultan√©ment**
4) **30 images sans aucun objet**

**Script de capture automatis√© :**

.. code-block:: python

   import cv2
   import os
   from pathlib import Path
   
   def capture_scenario(output_dir, scenario_name, num_images=60):
       """
       Capture des images depuis la webcam avec indicateur visuel.
       
       Args:
           output_dir: dossier de sortie
           scenario_name: nom du sc√©nario (obj1, obj2, both, none)
           num_images: nombre d'images √† capturer
       """
       # TODO: Cr√©er le dossier de sortie
       # TODO: Ouvrir la webcam avec cv2.VideoCapture(0)
       # TODO: Boucle de capture:
       #   - Afficher la frame avec compteur
       #   - ESPACE = capturer et sauvegarder
       #   - ESC = quitter
       # TODO: Lib√©rer la webcam
       pass
   
   # Programme de capture complet
   if __name__ == "__main__":
       output_base = "images_multi_objects"
       
       print("üì∑ CAPTURE MULTI-OBJETS")
       
       # TODO: Capturer les 4 sc√©narios:
       # - Sc√©nario 1: obj1 (60 images)
       # - Sc√©nario 2: obj2 (60 images)
       # - Sc√©nario 3: both (50 images)
       # - Sc√©nario 4: none (30 images)

**Annotation dans Label Studio :**

Configuration avec 2 classes :

.. code-block:: xml

   <View>
     <Image name="image" value="$image"/>
     <RectangleLabels name="label" toName="image">
       <Label value="objet_1" background="red"/>
       <Label value="objet_2" background="blue"/>
     </RectangleLabels>
   </View>

**Consignes d'annotation :**

- Images avec objet_1 : dessinez une bo√Æte rouge autour de objet_1
- Images avec objet_2 : dessinez une bo√Æte bleue autour de objet_2
- Images avec les deux : dessinez les deux bo√Ætes (rouge + bleue)
- Images sans objet : soumettez sans annotation

**Questions Partie A :**

11) Pourquoi capturer des images avec les deux objets ensemble ?
12) Quelle est la difficult√© principale de ce dataset compar√© √† l'exercice 1 ?
13) Comment √©quilibrer le dataset si un objet est plus difficile √† d√©tecter ?

**Astuce Partie A :**

.. spoiler::
    .. discoverList::
        1. Pensez √† l'importance d'avoir des images avec les deux objets simultan√©ment
        2. Identifiez les difficult√©s sp√©cifiques d'un dataset multi-classe
        3. R√©fl√©chissez aux strat√©gies d'√©quilibrage si une classe est plus difficile
        4. Variez les configurations spatiales des objets

.. slide::

**Partie B : YOLO multi-classe**

**Consigne :** Entra√Æner YOLOv11 pour d√©tecter les 2 objets.

1) **Organiser le dataset YOLO** :

.. code-block:: python

   import shutil
   from pathlib import Path
   import random
   
   def organize_multiclass_yolo(images_dir, labels_dir, output_dir):
       """Organise le dataset multi-classe pour YOLO."""
       # TODO: Cr√©er la structure des dossiers
       # TODO: Lister et m√©langer les images
       # TODO: Split 70/15/15 pour train/val/test
       # TODO: Copier images et labels
       # TODO: Analyser et afficher les statistiques par sc√©nario
       pass

2) **Cr√©er le fichier YAML** :

.. code-block:: yaml

   # data_multiclass.yaml
   # TODO: Compl√©ter avec vos chemins
   path: /chemin/absolu/vers/data_yolo_multiclass
   train: images/train
   val: images/val
   test: images/test
   
   nc: 2
   names: ['objet_1', 'objet_2']

3) **Entra√Æner YOLO** :

.. code-block:: python

   from ultralytics import YOLO
   
   # TODO: Charger yolo11n.pt
   # TODO: Entra√Æner avec data_multiclass.yaml
   # TODO: Choisir epochs, imgsz, batch appropri√©s
   pass

4) **√âvaluer par classe** :

.. code-block:: python

   # TODO: Charger le meilleur mod√®le
   # TODO: √âvaluer avec model.val()
   # TODO: Afficher mAP global
   # TODO: Afficher m√©triques par classe (precision, recall, mAP)
   pass

5) **Tester sur images avec les 2 objets** :

.. code-block:: python

   import cv2
   from pathlib import Path
   import matplotlib.pyplot as plt
   
   # TODO: R√©cup√©rer les images 'both_*.jpg' du test set
   # TODO: Faire les pr√©dictions
   # TODO: Visualiser en grille 2x3
   # TODO: Sauvegarder le r√©sultat
   pass

**Questions Partie B :**

14) Comment YOLO g√®re-t-il plusieurs objets de classes diff√©rentes sur une m√™me image ?
15) Que se passe-t-il si les deux objets se chevauchent beaucoup ?
16) Comment am√©liorer la d√©tection si un objet est syst√©matiquement mieux d√©tect√© que l'autre ?
17) Quelle est la diff√©rence entre mAP@0.5 et mAP@0.5:0.95 ?

**Astuce Partie B :**

.. spoiler::
    .. discoverList::
        1. Analysez le m√©canisme de d√©tection multi-classe de YOLO (grille + NMS)
        2. R√©fl√©chissez aux probl√®mes de chevauchement d'objets
        3. Pensez aux strat√©gies pour g√©rer le d√©s√©quilibre entre classes
        4. Comprenez la diff√©rence entre mAP@0.5 et mAP@0.5:0.95
        5. Utilisez la visualisation pour d√©boguer

**R√©sultat attendu :**

- Dataset de 200 images organis√© (train/val/test)
- Mod√®le YOLO entra√Æn√© avec 2 classes
- mAP@0.5 > 0.7 pour chaque classe
- Visualisation des pr√©dictions sur images avec 2 objets

.. slide::

üå∂Ô∏è Exercice 3 : Tracking vid√©o en temps r√©el
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Dans cet exercice, vous allez cr√©er un syst√®me de tracking qui d√©tecte et suit vos objets dans une vid√©o, en temps r√©el si possible.

**Objectif :** Impl√©menter un syst√®me complet de tracking avec d√©tection, suivi d'identit√© et comptage des apparitions/disparitions.

**Mat√©riel n√©cessaire :**

- Vid√©o de 30-60 secondes avec vos objets qui entrent/sortent du champ
- Mod√®le YOLO entra√Æn√© (exercice 2)
- (Optionnel) Webcam pour test en temps r√©el

**Partie A : Tracking simple avec d√©tection frame par frame**

**Consigne :** Cr√©er un script de base qui d√©tecte les objets sur chaque frame.

1) **Cr√©er une vid√©o de test** :

.. code-block:: python

   """
   SC√âNARIO DE LA VID√âO (30-60 secondes) :
   
   - 0-10s  : Aucun objet visible
   - 10-20s : Objet 1 entre dans le champ, se d√©place
   - 20-30s : Objet 2 entre aussi (les 2 sont visibles)
   - 30-40s : Objet 1 sort du champ (seul objet 2 reste)
   - 40-50s : Objet 2 sort aussi
   - 50-60s : Aucun objet visible
   
   Filmez avec votre smartphone !
   """

2) **D√©tection frame par frame** :

.. code-block:: python

   import cv2
   from ultralytics import YOLO
   import numpy as np
   from collections import defaultdict
   import time
   
   def detect_on_video(model_path, video_path, output_path, conf_threshold=0.5):
       """
       D√©tecte les objets sur chaque frame et sauvegarde la vid√©o.
       
       Returns:
           dict avec statistiques de d√©tection
       """
       # TODO: Charger le mod√®le YOLO
       # TODO: Ouvrir la vid√©o et r√©cup√©rer fps, dimensions, nombre de frames
       # TODO: Cr√©er VideoWriter pour la sortie
       # TODO: Initialiser un dict de statistiques
       # TODO: Boucle sur les frames:
       #   - Lire frame
       #   - Faire la pr√©diction YOLO
       #   - Mesurer le temps de traitement
       #   - Dessiner les d√©tections + info (frame, objets, FPS)
       #   - Sauvegarder dans la vid√©o de sortie
       #   - Accumuler les statistiques
       # TODO: Lib√©rer les ressources
       # TODO: Afficher les statistiques finales
       pass

3) **Analyser les d√©tections** :

.. code-block:: python

   import matplotlib.pyplot as plt
   
   def plot_detection_stats(stats, class_names):
       """Visualise les statistiques de d√©tection."""
       # TODO: Cr√©er figure 2x2
       # TODO: Graphique 1: D√©tections par frame (ligne)
       # TODO: Graphique 2: D√©tections par classe (barres)
       # TODO: Graphique 3: Distribution du nombre d'objets (histogramme)
       # TODO: Graphique 4: Temps de traitement (ligne + moyenne)
       # TODO: Sauvegarder la figure
       pass

**Questions Partie A :**

18) Quel est le FPS moyen de votre syst√®me ? Est-ce suffisant pour du temps r√©el (>30 fps) ?
19) Pourquoi le temps de traitement varie-t-il d'une frame √† l'autre ?
20) Comment pourriez-vous am√©liorer la vitesse si elle est trop lente ?

**Astuce Partie A :**

.. spoiler::
    .. discoverList::
        1. √âvaluez si votre FPS est suffisant pour le temps r√©el (seuil ~30 fps)
        2. Analysez les causes de variation du temps de traitement
        3. R√©fl√©chissez aux optimisations possibles (r√©solution, mod√®le, fr√©quence)
        4. Pensez √† adapter pour une webcam en direct

.. slide::

**Partie B : Tracking avec identit√© (Object ID)**

**Consigne :** Ajouter un syst√®me de suivi qui assigne un ID unique √† chaque objet.

1) **Impl√©menter un tracker simple** :

.. code-block:: python

   from scipy.spatial import distance
   import numpy as np
   
   class SimpleTracker:
       """
       Tracker simple bas√© sur la distance entre d√©tections.
       """
       
       def __init__(self, max_distance=50, max_disappeared=30):
           """
           Args:
               max_distance: distance max (pixels) pour associer d√©tection √† objet existant
               max_disappeared: nombre de frames max avant de supprimer un objet
           """
           # TODO: Initialiser next_object_id, objects dict, param√®tres
           pass
       
       def update(self, detections):
           """
           Met √† jour le tracker avec nouvelles d√©tections.
           
           Args:
               detections: list of dict {'bbox': [x1, y1, x2, y2], 'class': int, 'conf': float}
           
           Returns:
               dict {object_id: {'centroid': (x, y), 'class': int, 'bbox': [x1, y1, x2, y2]}}
           """
           # TODO: Si pas de d√©tections, incr√©menter disappeared et supprimer si > max
           # TODO: Calculer centroids des nouvelles d√©tections
           # TODO: Si pas d'objets existants, cr√©er tous les nouveaux
           # TODO: Sinon:
           #   - Calculer matrice de distances (distance.cdist)
           #   - Associer par plus proche voisin
           #   - V√©rifier distance max et m√™me classe
           #   - Mettre √† jour les objets associ√©s
           #   - Marquer les non-associ√©s comme disparus
           #   - Cr√©er les nouveaux objets
           # TODO: Retourner objects dict
           pass
       
       def _register(self, centroid, class_id, bbox):
           """Enregistre un nouvel objet."""
           # TODO: Cr√©er nouvel objet avec next_object_id
           # TODO: Incr√©menter next_object_id
           pass

2) **Appliquer le tracker sur la vid√©o** :

.. code-block:: python

   def track_video(model_path, video_path, output_path, class_names, conf_threshold=0.6):
       """Tracking avec identit√©s sur vid√©o."""
       # TODO: Charger YOLO et cr√©er SimpleTracker
       # TODO: Ouvrir vid√©o et cr√©er VideoWriter
       # TODO: Initialiser dict events (appeared/disappeared) et previous_ids
       # TODO: Boucle sur frames:
       #   - Faire la d√©tection YOLO
       #   - Convertir en format tracker
       #   - Mettre √† jour tracker
       #   - D√©tecter apparitions/disparitions
       #   - Dessiner bo√Ætes avec IDs et couleurs uniques
       #   - Dessiner centroids
       #   - Afficher info frame
       #   - Sauvegarder frame
       # TODO: Afficher statistiques finales
       # TODO: Retourner events
       pass

3) **Analyser les √©v√©nements** :

.. code-block:: python

   from collections import defaultdict
   
   def analyze_events(events, class_names):
       """Analyse les √©v√©nements d'apparition/disparition."""
       # TODO: Afficher tableau des apparitions (frame, objet, ID)
       # TODO: Afficher tableau des disparitions (frame, ID)
       # TODO: Calculer et afficher statistiques par classe
       pass

**Questions Partie B :**

21) Comment le tracker g√®re-t-il deux objets de la m√™me classe proches l'un de l'autre ?
22) Que se passe-t-il si un objet est temporairement occult√© (cach√©) ?
23) Comment am√©liorer le tracker pour g√©rer les occlusions ?
24) Pourquoi utiliser la distance euclidienne entre centroids plut√¥t que l'IoU entre bo√Ætes ?

**Astuce Partie B :**

.. spoiler::
    .. discoverList::
        1. Analysez les risques de confusion d'ID entre objets proches
        2. R√©fl√©chissez au r√¥le du param√®tre `max_disappeared` pour les occlusions
        3. Pensez aux am√©liorations possibles (IoU, features visuelles, pr√©diction)
        4. Comparez distance euclidienne vs IoU pour l'association
        5. Explorez les trackers avanc√©s (DeepSORT, ByteTrack)

**R√©sultat attendu :**

- Vid√©o `output_tracking.mp4` avec IDs affich√©s et couleurs uniques
- Liste des √©v√©nements d'apparition/disparition
- Statistiques du tracking (nombre d'objets uniques, dur√©es de vie)

.. slide::

**Partie C : Tracking en temps r√©el sur webcam (Bonus)**

**Consigne :** Adapter le syst√®me pour fonctionner sur webcam en temps r√©el.

.. code-block:: python

   def track_webcam_realtime(model_path, class_names, conf_threshold=0.6):
       """Tracking en temps r√©el sur webcam."""
       # TODO: Charger YOLO et cr√©er SimpleTracker
       # TODO: Ouvrir webcam avec cv2.VideoCapture(0)
       # TODO: Initialiser couleurs dict et fps_list
       # TODO: Boucle infinie:
       #   - Lire frame webcam
       #   - Mesurer temps de traitement
       #   - Faire d√©tection YOLO
       #   - Mettre √† jour tracker
       #   - Dessiner bo√Ætes avec IDs, centroids
       #   - Calculer et afficher FPS moyen (sur 30 frames)
       #   - Afficher nombre d'objets
       #   - G√©rer touches: 'q' = quitter, 's' = screenshot
       # TODO: Lib√©rer webcam et afficher statistiques
       pass

**Questions Partie C :**

25) Quelle est la latence (d√©lai) entre le mouvement r√©el et l'affichage ?
26) Comment optimiser pour atteindre 60 FPS sur webcam ?
27) Quelles sont les applications pratiques d'un tel syst√®me ?

**Astuce Partie C :**

.. spoiler::
    .. discoverList::
        1. Mesurez la latence entre mouvement r√©el et affichage
        2. Explorez les optimisations pour atteindre 60 FPS
        3. Identifiez des applications pratiques d'un tel syst√®me
        4. Pensez aux strat√©gies de r√©duction de latence

**R√©sultat attendu :**

- Syst√®me de tracking temps r√©el sur webcam
- FPS > 20 (minimum pour fluidit√©)
- D√©tection et suivi corrects des objets qui entrent/sortent
- Screenshots possibles pendant l'ex√©cution

.. slide::

üéØ Conclusion du TP
~~~~~~~~~~~~~~~~~~~

**Bilan des comp√©tences acquises :**

1. **D√©tection avec gestion de l'absence** :
   - Annotation de cas n√©gatifs (images sans objet)
   - Ajout d'un score d'objectness (CNN custom)
   - Utilisation d'images n√©gatives avec YOLO
   - M√©triques : TP, FP, TN, FN pour √©valuer les faux positifs

2. **D√©tection multi-classe** :
   - Dataset √©quilibr√© avec plusieurs objets
   - Annotation de multiple classes dans Label Studio
   - Entra√Ænement YOLO multi-classe
   - √âvaluation par classe (mAP, Precision, Recall)
   - Gestion des cas avec plusieurs objets simultan√©s

3. **Tracking vid√©o en temps r√©el** :
   - D√©tection frame par frame sur vid√©o
   - Association d'identit√©s aux objets (tracking)
   - Comptage des apparitions/disparitions
   - Performance temps r√©el sur webcam
   - Gestion des occlusions temporaires

**Comparaison CNN custom vs YOLO :**


   +------------------------+--------------------------------+--------------------------------+
   | **Crit√®re**            | **CNN Custom**                 | **YOLO**                       |
   +========================+================================+================================+
   | **Facilit√©**           | N√©cessite impl√©mentation       | Pr√™t √† l'emploi                |
   |                        | compl√®te (loss, training loop) | (``model.train()``)            |
   +------------------------+--------------------------------+--------------------------------+
   | **Performance**        | Plus lent (pas optimis√©)       | Tr√®s rapide (optimis√© C++)     |
   +------------------------+--------------------------------+--------------------------------+
   | **Flexibilit√©**        | Total contr√¥le sur             | Architecture fix√©e             |
   |                        | architecture et loss           |                                |
   +------------------------+--------------------------------+--------------------------------+
   | **Multi-objets**       | Difficile (NMS manuel)         | Natif (d√©tections multiples)   |
   +------------------------+--------------------------------+--------------------------------+
   | **Dataset requis**     | Petit dataset suffit           | Pr√©f√®re grands datasets        |
   |                        | (100-200 images)               | (500+ images)                  |
   +------------------------+--------------------------------+--------------------------------+
   | **Cas d'usage**        | - Apprentissage                | - Production                   |
   |                        | - Preuve de concept            | - Applications r√©elles         |
   |                        | - Recherche                    | - Temps r√©el                   |
   +------------------------+--------------------------------+--------------------------------+

**Pour aller plus loin :**

1. **Augmentation de donn√©es avanc√©e** :
   - Mixup, Cutout, Mosaic
   - Augmentation sp√©cifique au domaine (ex: conditions d'√©clairage)

2. **Tracking avanc√©** :
   - DeepSORT (avec features d'apparence)
   - ByteTrack (gestion d'occlusions)
   - Multi-object tracking avec r√©identification

3. **Optimisation pour production** :
   - Export ONNX pour d√©ploiement
   - Quantization (INT8) pour edge devices
   - TensorRT pour GPU NVIDIA

4. **Applications avanc√©es** :
   - D√©tection d'anomalies (objets inhabituels)
   - Estimation de pose (keypoints)
   - Segmentation d'instances (masques pr√©cis)

