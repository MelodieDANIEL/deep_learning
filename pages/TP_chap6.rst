üèãÔ∏è Travaux Pratiques 6 : D√©tection d'Objets
==============================================

.. slide::

Sur cette page se trouvent des exercices de TP sur le Chapitre 6. Ils sont class√©s par niveau de difficult√© suivant :

.. discoverList::
    * Facile : üçÄ
    * Moyen : ‚öñÔ∏è
    * Difficile : üå∂Ô∏è

.. slide::

üçÄ Exercice 1 : D√©tection avec pr√©sence/absence - CNN Custom vs YOLO
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Dans cet exercice, vous allez cr√©er un dataset o√π certaines images contiennent votre objet et d'autres non. Vous comparerez ensuite un mod√®le CNN custom avec YOLO, avec et sans augmentation de donn√©es.

**Objectif :** Ma√Ætriser la d√©tection d'objets avec gestion des cas n√©gatifs (images sans objet) et comparer les approches custom vs YOLO.

**Mat√©riel n√©cessaire :**

- Votre smartphone ou webcam
- Un objet √† d√©tecter (cube, cylindre, balle, tasse, etc.)
- Environnement vari√© pour les prises de vue

**Partie A : Cr√©ation du dataset (100 images avec objet, 40 sans objet)**

**Consigne :** √âcrire un programme qui :

.. step::
    1) Prend une vid√©o :
    
    - Avec l'objet visible, variez les angles, distances et orientations,
    - et avec le m√™me environnement SANS l'objet.

.. step::
    2) Extrait les frames de la vid√©o.

.. step::
    3) Annote les images au format YOLO :
    
    - Cr√©er les dossiers ``dataset/images/`` et ``dataset/labels/``
    - Pour les images AVEC l'objet : cr√©er un fichier .txt avec le format ``class_id x_center y_center width height`` (normalis√© 0-1)
    - Pour les images SANS l'objet : ne pas cr√©er de fichier .txt correspondant

.. step::
    4) (optionnel) V√©rifie le dataset.


**Questions Partie A :**

.. step::
    5) Pourquoi est-il crucial d'avoir des images sans l'objet dans le dataset ?

.. step::
    6) Que se passerait-il si on entra√Ænait uniquement sur des images positives ?

.. step::
    7) Quel ratio pr√©sence/absence est optimal ? (70/30, 80/20, 50/50 ?)


**Astuce Partie A :**

.. spoiler::
    .. discoverList::
        1. Les exemples n√©gatifs √©vitent les faux positifs (d√©tections fant√¥mes sur fond vide).
        2. Sans images n√©gatives, le mod√®le d√©tecte toujours quelque chose m√™me sur fond vide.
        3. Un ratio 70/30 ou 60/40 (avec objet / sans objet) est g√©n√©ralement optimal.
        4. Images sans label = pas de fichier .txt = image n√©gative pour YOLO (il apprend √† ne rien d√©tecter).

**R√©sultat attendu Partie A :**

- Dataset de ~140 images : 80 avec objet, 60 sans objet
- Structure correcte : ``dataset/images/`` et ``dataset/labels/``
- Fichiers .txt au format YOLO pour les images positives uniquement

.. slide::

**Partie B : CNN Custom avec gestion de l'objectness**

**Consigne :** √âcrire un programme qui :

.. step::
    1) Impl√©mente une architecture CNN simple pour la d√©tection que vous devez proposer.

.. step::
    2) Cr√©e un Dataset PyTorch pour charger les images et labels YOLO.

.. step::
    3) Impl√©mente la loss personnalis√©e pour la d√©tection que vous devez proposer.      

.. step::
    4) Entra√Æne le mod√®le SANS augmentation pendant 100 epochs avec un early stopping d'une patience = 15.

.. step::
    5) Sauvegarde le meilleur mod√®le dans ``best_model_no_aug.pth``.

.. step::
    6) Impl√©mente une fonction d'√©valuation avec m√©triques (TP, FP, TN, FN, IoU).

**Astuce Partie B :**

.. spoiler::
    .. discoverList::
        1. **Architecture CNN** : Votre CNN doit pr√©dire 5 valeurs : ``[has_object, x_center, y_center, width, height]``. Sortie finale avec ``nn.Sigmoid()`` pour borner entre 0 et 1. Proposez une architecture simple (3-4 conv layers + pooling) avec des couches Linear dont la derni√®re est ``nn.Linear(nb_features, 5)``.
        2. **Dataset PyTorch** : Pour les images SANS objet (pas de .txt), retournez ``target = torch.tensor([0.0, 0.0, 0.0, 0.0, 0.0])``. Pour les images AVEC objet, parsez le fichier .txt et retournez ``target = torch.tensor([1.0, x_c, y_c, w, h])``.
        3. **Loss combin√©e** : BCE (Binary Cross Entropy) pour has_object (classification binaire), MSE (Mean Squared Error) pour bbox (r√©gression). Utilisez un masque ``mask = target_obj > 0.5`` pour ne calculer loss_bbox QUE sur les images avec objet pr√©sent.
        4. **Entra√Ænement** : Adam optimizer avec lr=1e-4, batch_size=4 ou 8. Impl√©mentez early stopping avec patience=15 (arr√™ter si validation loss ne diminue pas pendant 15 epochs). Sauvegardez le meilleur mod√®le avec ``torch.save(model.state_dict(), 'best_model_no_aug.pth')``.
        5. **√âvaluation (IoU)** : Pour calculer l'IoU, convertissez YOLO (x_c, y_c, w, h) en coordonn√©es (x_min, y_min, x_max, y_max) avec ``x_min = x_c - w/2``, puis calculez l'intersection et l'union des rectangles.
        6. **M√©triques** : TP = GT a objet ET pred > threshold ET IoU > 0.5, FP = GT sans objet ET pred > threshold, TN = GT sans objet ET pred ‚â§ threshold, FN = GT a objet ET pred ‚â§ threshold.
        7. **Threshold de d√©tection** : Threshold bas (0.3) ‚Üí plus de d√©tections ‚Üí +recall, -precision. Threshold haut (0.8) ‚Üí moins de d√©tections ‚Üí +precision, -recall. Optimal g√©n√©ralement autour de 0.5-0.6.
        8. **Split du dataset** : 70% train, 15% validation, 15% test. Utilisez ``torch.utils.data.random_split()`` pour diviser le dataset de mani√®re reproductible.

**R√©sultat attendu Partie B :**

- Mod√®le CNN custom entra√Æn√© (sauvegard√© dans ``best_model_no_aug.pth``)
- Accuracy sur test set : ~70% (d√©pend de la qualit√© du dataset)
- Mean IoU : ~0.5-0.6 pour les d√©tections correctes

.. slide::

**Partie C : YOLO11 sans augmentation - Comparaison**

**Consigne :** √âcrire un programme qui :

.. step::
    1) Cr√©e un fichier ``dataset.yaml`` pour YOLO :

    .. code-block:: yaml

       # data_cube/dataset.yaml
       path: /chemin/absolu/vers/data_cube
       train: images
       val: images
       test: images
       
       nc: 1
       names: ['nom_label']

.. step::
    2) Entra√Æne YOLO11 sur le dataset.

.. step::
    3) √âvalue YOLO11 sur le test set avec les m√™mes m√©triques que le CNN custom.

.. step::
    4) Compare les r√©sultats dans un tableau comme par exemple :

    .. code-block:: python

       # TODO: Afficher tableau comparatif
       # +------------+---------------+-------------+
       # | M√©trique   | CNN Custom    | YOLO11      |
       # +------------+---------------+-------------+
       # | Accuracy   | XX.XX%        | XX.XX%      |
       # | Precision  | XX.XX%        | XX.XX%      |
       # | Recall     | XX.XX%        | XX.XX%      |
       # | Mean IoU   | 0.XXXX        | 0.XXXX      |
       # | FPS        | X.X           | XX.X        |
       # | Params     | 3.3M          | 2.6M        |
       # +------------+---------------+-------------+

**Questions Partie C :**

.. step::
    5) Quel mod√®le est le plus rapide en inf√©rence ? Le plus pr√©cis ?

.. step::
    6) Pourquoi YOLO est-il meilleur malgr√© moins de param√®tres ?


**Astuce Partie C :**

.. spoiler::
    .. discoverList::
        1. YOLO apprend automatiquement √† ne rien d√©tecter sur les images sans .txt
        2. YOLO est g√©n√©ralement plus rapide gr√¢ce √† son architecture optimis√©e
        3. YOLO b√©n√©ficie du pr√©-entra√Ænement sur COCO (80 classes, 1M+ images)
        4. Transfer learning : YOLO a d√©j√† appris des features g√©n√©riques (contours, textures)
        5. YOLO est pr√©f√©rable (rapidit√©, robustesse, communaut√©)

**R√©sultat attendu Partie C :**

- YOLO11 entra√Æn√© (mod√®le dans ``runs/detect/yolo_cube_detect/weights/best.pt``)
- Accuracy : ~90-98% (bien meilleure que CNN custom)
- Mean IoU : ~0.85-0.95 (localisation tr√®s pr√©cise)
- FPS : 20-50 fps sur CPU (vs 5-10 fps pour CNN custom)
- YOLO clairement sup√©rieur en production

############################

.. slide::

‚öñÔ∏è Exercice 2 : D√©tection multi-objets (2 classes)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Dans cet exercice, vous allez cr√©er un d√©tecteur capable de distinguer deux objets diff√©rents sur la m√™me image.

**Objectif :** Ma√Ætriser la d√©tection multi-classe et g√©rer plusieurs objets simultan√©s.

**Mat√©riel n√©cessaire :**

- Deux objets distincts visuellement (ex: cube + cylindre)
- Smartphone ou webcam

**Partie A : Dataset multi-objets**

**Consigne :** √âcrire un programme qui :

.. step::
    1) Cr√©e un dataset augment√© avec r√©√©quilibrage :

    .. code-block:: python

       import torchvision.transforms as T
       import random
       
       class AugmentedYOLODataset(Dataset):
           """Dataset avec augmentation adapt√©e √† la d√©tection."""
           
           def __init__(self, image_dir, label_dir, augment=False):
               # TODO: Initialiser comme YOLODetectionDataset
               
               # TODO: Ajouter transformations d'augmentation
               # self.color_jitter = T.ColorJitter(brightness=0.3, contrast=0.3)
               pass
           
           def horizontal_flip(self, image, bbox):
               """Flip horizontal avec ajustement bbox."""
               image = T.functional.hflip(image)
               if bbox is not None:
                   bbox = bbox.copy()  # IMPORTANT: copier !
                   bbox[0] = 1.0 - bbox[0]  # x_center invers√©
               return image, bbox
           
           def __getitem__(self, idx):
               # TODO: Charger image et label
               # TODO: Si augment=True:
               #   - 50% flip horizontal
               #   - 80% color jitter
               # TODO: Appliquer normalisation
               pass

.. step::
    2) R√©√©quilibre le dataset (dupliquer les images sans objet pour avoir 50/50).

.. step::
    3) Entra√Æne le CNN custom AVEC augmentation :

    .. code-block:: python

       # TODO: Cr√©er AugmentedYOLODataset avec augment=True pour train
       # TODO: Entra√Æner avec m√™mes hyperparam√®tres que Partie B
       # TODO: Comparer train loss et val loss (v√©rifier overfitting r√©duit)
       # TODO: Sauvegarder dans 'best_model_with_aug.pth'
    
    4) Entra√Æne YOLO avec AVEC augmentation.

.. step::
    5) Compare les 4 mod√®les finaux :

    .. code-block:: python

       # TODO: Charger les 4 mod√®les
       # TODO: √âvaluer sur le M√äME test set
       # TODO: Afficher tableau comparatif complet
       # +---------------------+---------------+---------------+-------------+-------------+
       # | M√©trique            | CNN No Aug    | CNN + Aug     | YOLO11      |YOLO11 + Aug |
       # +---------------------+---------------+---------------+-------------+-------------+
       # | Accuracy            | XX.XX%        | XX.XX%        | XX.XX%      |XX.XX%       |
       # | Precision           | XX.XX%        | XX.XX%        | XX.XX%      |XX.XX%       |
       # | Recall              | XX.XX%        | XX.XX%        | XX.XX%      |XX.XX%       |
       # | Mean IoU            | 0.XXXX        | 0.XXXX        | 0.XXXX      |0.XXXX       |
       # | Gap Train/Val (%)   | XX            | XX            | XX          | XX          |
       # +---------------------+---------------+---------------+-------------+-------------+


**Questions Partie D :**

.. step::
    6) L'augmentation am√©liore-t-elle les performances du CNN custom ?

.. step::
    7) Le gap train/val est-il r√©duit avec l'augmentation ?

.. step::
    8) Pourquoi l'augmentation seule ne suffit pas √† rattraper YOLO ?

.. step::
    9) Quelle est l'importance du r√©√©quilibrage (50/50) ?


**Astuce Partie D :**

.. spoiler::
    .. discoverList::
        1. L'augmentation r√©duit l'overfitting (gap train/val plus petit)
        2. Le r√©√©quilibrage √©vite le biais : sans lui, le mod√®le d√©tecte trop souvent
        3. YOLO reste sup√©rieur car il combine pr√©-entra√Ænement + architecture optimis√©e
        4. Flip horizontal : x_center ‚Üí 1 - x_center, y_center inchang√©, w et h inchang√©s
        5. Augmentation adapt√©e d√©tection : √©viter rotations fortes (change orientation objet)
        6. Color jitter OK car n'affecte pas les coordonn√©es spatiales

**R√©sultat attendu Partie D :**

- CNN sans aug : 60-70% accuracy, gap train/val ~15-20%
- CNN avec aug : 68-75% accuracy, gap train/val ~5-10% (meilleure g√©n√©ralisation)
- YOLO11 : 90-98% accuracy, gap train/val ~2-3% (le meilleur)
- Conclusion : Augmentation aide, mais YOLO reste imbattable

############################

.. slide::

‚öñÔ∏è Exercice 2 : D√©tection multi-objets (2 classes)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Dans cet exercice, vous allez cr√©er un d√©tecteur capable de distinguer deux objets diff√©rents sur la m√™me image.

**Objectif :** Ma√Ætriser la d√©tection multi-classe et g√©rer plusieurs objets simultan√©s.

**Mat√©riel n√©cessaire :**

- Deux objets distincts visuellement (ex: cube + cylindre)
- Smartphone ou webcam

**Partie A : Dataset multi-objets**

**Consigne :** √âcrire un programme qui :

.. step::
    1) Capture ~200 images r√©parties ainsi par exemple :
    
    - **60 images** : objet 1 uniquement
    - **60 images** : objet 2 uniquement
    - **50 images** : les DEUX objets simultan√©ment
    - **30 images** : aucun objet

.. step::
    2) Annote le dataset :
    
    - Classe 0 : objet_1 (ex: cube)
    - Classe 1 : objet_2 (ex: cylindre)
    - Format YOLO : ``class_id x_center y_center width height``

.. step::
    3) Organise et importe le dataset pour YOLO.


**Questions Partie A :**

.. step::
    4) Quelle est la difficult√© principale de ce dataset compar√© √† l'exercice 1 ?

.. step::
    5) Comment √©quilibrer le dataset si un objet est plus difficile √† d√©tecter ?

.. step::
    6) Que se passe-t-il si les deux objets se chevauchent beaucoup ?


**Astuce Partie A :**

.. spoiler::
    .. discoverList::
        1. Les images avec les deux objets apprennent au mod√®le √† les distinguer simultan√©ment
        2. Difficult√© : confusion entre classes si objets similaires, gestion multi-d√©tections
        3. √âquilibrage : augmenter les donn√©es de la classe sous-repr√©sent√©e
        4. Chevauchement : YOLO g√®re bien gr√¢ce au NMS (Non-Maximum Suppression)

**R√©sultat attendu Partie A :**

- Dataset de 200 images organis√© en train/val/test
- Annotations multi-classe correctes (classe 0 et 1)
- Distribution √©quilibr√©e entre les 4 sc√©narios

.. slide::

**Partie B : YOLO multi-classe**

**Consigne :** √âcrire un programme qui :

.. step::
    1) Cr√©e le fichier ``dataset.yaml`` pour 2 classes :

    .. code-block:: yaml

       # data_multiclass/dataset.yaml
       path: /chemin/absolu/vers/data_multiclass
       train: images/train
       val: images/val
       test: images/test
       
       nc: 2
       names: ['label1', 'label2']

.. step::
    2) Entra√Æne YOLO11 multi-classe.

.. step::
    3) √âvalue par classe (mAP, precision, recall, etc.).


**Questions Partie B :**

.. step::
    4) Comment YOLO g√®re-t-il plusieurs objets de classes diff√©rentes sur une m√™me image ?

.. step::
    5) Que se passe-t-il si les deux objets se chevauchent fortement ?

.. step::
    6) Comment am√©liorer la d√©tection si objet_1 est syst√©matiquement mieux d√©tect√© qu'objet_2 ?

.. step::
    7) Pourquoi utiliser NMS (Non-Maximum Suppression) ?


**Astuce Partie B :**

.. spoiler::
    .. discoverList::
        1. YOLO pr√©dit plusieurs bo√Ætes par grille, chacune avec une classe
        2. NMS √©limine les d√©tections redondantes (IoU > seuil avec m√™me classe)
        3. Si chevauchement fort : risque de supprimer une d√©tection valide avec NMS agressif
        4. Am√©liorer d√©tection d√©s√©quilibr√©e : augmenter weight de la classe faible, plus de donn√©es
        5. mAP@0.5 : moyenne precision avec IoU‚â•0.5 (localisation tol√©rante)
        6. mAP@0.5:0.95 : moyenne sur plusieurs seuils IoU (plus strict, meilleure m√©trique)
        7. NMS garde la bo√Æte avec le plus haut score de confiance parmi les overlaps
        8. Analyser les confusions avec une matrice de confusion (classe pr√©dite vs GT)

**R√©sultat attendu Partie B :**

- YOLO multi-classe entra√Æn√©
- mAP@0.5 global : ~0.75-0.85
- mAP@0.5 par classe : ~0.70-0.90 pour chaque objet

############################

.. slide::

üèãÔ∏è Exercices suppl√©mentaires 6
===============================
Dans cette section, il y a des exercices suppl√©mentaires pour vous entra√Æner. Ils suivent le m√™me classement de difficult√© que pr√©c√©demment.


.. slide::

‚öñÔ∏è Exercice suppl√©mentaire 1 : Visualisation des r√©sultats de d√©tection
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Cet exercice propose de visualiser et analyser les r√©sultats de d√©tection de vos mod√®les.

**Objectif :** Comprendre les forces et faiblesses de vos mod√®les de d√©tection √† travers la visualisation.

**Consignes** :

.. step::
    1) Utiliser le mod√®le CNN custom ou YOLO entra√Æn√© dans l'exercice 1

.. step::
    2) Cr√©er une fonction qui affiche les meilleurs r√©sultats de d√©tection :

    .. code-block:: python
    
        def visualize_best_detections(model, test_images, test_labels, num_examples=6):
            """
            Affiche les meilleures d√©tections (IoU > 0.8)
            """
            # TODO: Pr√©dire sur test set
            # TODO: Filtrer les d√©tections avec has_object > 0.5 ET IoU > 0.8
            # TODO: Afficher en grille 2x3:
            #   - Image originale avec bbox GT (vert)
            #   - Image avec bbox pr√©dite (bleu)
            #   - Titre: IoU = X.XX, Conf = X.XX
            pass

.. step::
    3) Cr√©er une fonction pour visualiser les images sans objet (vrais n√©gatifs) :

    .. code-block:: python
    
        def visualize_empty_images(model, test_images, test_labels, num_examples=6):
            """
            Affiche les images correctement identifi√©es comme vides
            """
            # TODO: Filtrer images o√π GT n'a pas d'objet (target[0] == 0)
            # TODO: Filtrer o√π pr√©diction < 0.5 (correct)
            # TODO: Afficher avec titre: "Conf vide: X.XX"
            pass

.. step::
    4) Cr√©er une fonction pour visualiser les erreurs :

    .. code-block:: python
    
        def visualize_errors(model, test_images, test_labels, num_examples=6):
            """
            Affiche les cas probl√©matiques
            """
            # TODO: Cas 1: Faux positifs (d√©tection sur fond vide)
            # TODO: Cas 2: Faux n√©gatifs (objet non d√©tect√©)
            # TODO: Cas 3: Mauvaise localisation (IoU < 0.5 mais objet d√©tect√©)
            # TODO: Afficher en 3 lignes
            pass

.. step::
    5) Comparer les visualisations entre CNN custom et YOLO11


**Questions :**

.. step::
    6) Quel type d'erreur est le plus fr√©quent pour chaque mod√®le ?

.. step::
    7) Dans quelles conditions les mod√®les ont-ils le plus de difficult√©s ?

.. step::
    8) YOLO fait-il des erreurs diff√©rentes du CNN custom ?


**Astuce :**
.. spoiler::
    .. discoverList::
        1. Pour dessiner les bbox : ``cv2.rectangle(img, (x_min, y_min), (x_max, y_max), color, 2)``
        2. Convertir YOLO ‚Üí xyxy : ``x_min = int((x_c - w/2) * img_width)``
        3. Couleur GT : vert ``(0, 255, 0)``, pr√©diction : bleu ``(255, 0, 0)``
        4. Pour les faux positifs : ``(GT sans objet) AND (pred > 0.5)``
        5. Pour les faux n√©gatifs : ``(GT avec objet) AND (pred < 0.5)``
        6. Utilisez ``plt.subplots()`` pour cr√©er des grilles de visualisation


**R√©sultats attendus :**

- 3 grilles de visualisation : meilleures d√©tections, images vides, erreurs
- Analyse comparative CNN vs YOLO : types d'erreurs, fr√©quence
- Compr√©hension des cas difficiles (occlusion partielle, faible luminosit√©, etc.)


.. slide::

üå∂Ô∏è Exercice suppl√©mentaire 2 : Augmentation de donn√©es et comparaison finale
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Dans cet exercice, vous allez impl√©menter l'augmentation de donn√©es adapt√©e √† la d√©tection et comparer les r√©sultats.

**Objectif :**  
Comprendre l'impact de l'augmentation sur la g√©n√©ralisation et r√©duire l'overfitting.

**Consignes** :

.. step::
    1) Cr√©er un dataset augment√© avec r√©√©quilibrage :

    .. code-block:: python

       import torchvision.transforms as T
       import random
       
       class AugmentedYOLODataset(Dataset):
           """Dataset avec augmentation adapt√©e √† la d√©tection."""
           
           def __init__(self, image_dir, label_dir, augment=False):
               # TODO: Initialiser comme YOLODetectionDataset
               
               # TODO: Ajouter transformations d'augmentation
               # self.color_jitter = T.ColorJitter(brightness=0.3, contrast=0.3)
               pass
           
           def horizontal_flip(self, image, bbox):
               """Flip horizontal avec ajustement bbox."""
               image = T.functional.hflip(image)
               if bbox is not None:
                   bbox = bbox.copy()  # IMPORTANT: copier !
                   bbox[0] = 1.0 - bbox[0]  # x_center invers√©
               return image, bbox
           
           def __getitem__(self, idx):
               # TODO: Charger image et label
               # TODO: Si augment=True:
               #   - 50% flip horizontal
               #   - 80% color jitter
               # TODO: Appliquer normalisation
               pass

.. step::
    2) R√©√©quilibrer le dataset (dupliquer les images sans objet pour avoir 50/50)

.. step::
    3) Entra√Æner le CNN custom AVEC augmentation :

    .. code-block:: python

       # TODO: Cr√©er AugmentedYOLODataset avec augment=True pour train
       # TODO: Entra√Æner avec m√™mes hyperparam√®tres que Partie B
       # TODO: Comparer train loss et val loss (v√©rifier overfitting r√©duit)
       # TODO: Sauvegarder dans 'best_model_with_aug.pth'
    
.. step::
    4) Entra√Æner YOLO AVEC augmentation

.. step::
    5) Comparer les 4 mod√®les finaux :

    .. code-block:: python

       # TODO: Charger les 4 mod√®les
       # TODO: √âvaluer sur le M√äME test set
       # TODO: Afficher tableau comparatif complet
       # +---------------------+---------------+---------------+-------------+-------------+
       # | M√©trique            | CNN No Aug    | CNN + Aug     | YOLO11      |YOLO11 + Aug |
       # +---------------------+---------------+---------------+-------------+-------------+
       # | Accuracy            | XX.XX%        | XX.XX%        | XX.XX%      |XX.XX%       |
       # | Precision           | XX.XX%        | XX.XX%        | XX.XX%      |XX.XX%       |
       # | Recall              | XX.XX%        | XX.XX%        | XX.XX%      |XX.XX%       |
       # | Mean IoU            | 0.XXXX        | 0.XXXX        | 0.XXXX      |0.XXXX       |
       # | Gap Train/Val (%)   | XX            | XX            | XX          | XX          |
       # +---------------------+---------------+---------------+-------------+-------------+


**Questions :**

.. step::
    6) L'augmentation am√©liore-t-elle les performances du CNN custom ?

.. step::
    7) Le gap train/val est-il r√©duit avec l'augmentation ?

.. step::
    8) Pourquoi l'augmentation seule ne suffit pas √† rattraper YOLO ?

.. step::
    9) Quelle est l'importance du r√©√©quilibrage (50/50) ?


**Astuce :**

.. spoiler::
    .. discoverList::
        1. L'augmentation r√©duit l'overfitting (gap train/val plus petit)
        2. Le r√©√©quilibrage √©vite le biais : sans lui, le mod√®le d√©tecte trop souvent
        3. YOLO reste sup√©rieur car il combine pr√©-entra√Ænement + architecture optimis√©e
        4. Flip horizontal : x_center ‚Üí 1 - x_center, y_center inchang√©, w et h inchang√©s
        5. Augmentation adapt√©e d√©tection : √©viter rotations fortes (change orientation objet)
        6. Color jitter OK car n'affecte pas les coordonn√©es spatiales

**R√©sultat attendu :**

- CNN sans aug : 60-70% accuracy, gap train/val ~15-20%
- CNN avec aug : 68-75% accuracy, gap train/val ~5-10% (meilleure g√©n√©ralisation)
- YOLO11 : 90-98% accuracy, gap train/val ~2-3% (le meilleur)
- Conclusion : Augmentation aide, mais YOLO reste imbattable


.. slide::

üå∂Ô∏è Exercice suppl√©mentaire 3 : Introduction au tracking basique (d√©tection frame par frame)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Cet exercice est une introduction au tracking d'objets en d√©tectant frame par frame dans une vid√©o.

**Objectif :**  
Comprendre les bases de la d√©tection vid√©o et mesurer les performances en temps r√©el.

.. note::
    **Qu'est-ce que le tracking ?**
    
    Le tracking (suivi d'objets) consiste √† d√©tecter et identifier le **m√™me objet** √† travers les frames d'une vid√©o. Dans cet exercice simplifi√©, vous allez uniquement **d√©tecter** l'objet sur chaque frame ind√©pendamment, **sans associer d'identit√©** entre les frames.
    
    **Limitation de cette approche :**
    
    - Si l'objet sort puis revient, vous ne savez pas que c'est le m√™me
    - Impossible de compter combien d'objets distincts sont apparus
    - Pas de trajectoire ou d'historique de mouvement
    
    Cette approche est utile pour :
    
    - D√©tecter la pr√©sence/absence d'un objet en temps r√©el
    - Mesurer les performances (FPS) de votre syst√®me
    - Pr√©parer le terrain pour un vrai tracking avec identit√©s

**Mat√©riel n√©cessaire :**

- Vid√©o MP4 de test (30-60 secondes)
- Mod√®le YOLO entra√Æn√© (exercice 1 ou 2)

**Consignes** :

.. step::
    1) Cr√©er ou utiliser une vid√©o de test (30-60 secondes) :
    
    **Sc√©nario recommand√©** :
    - 0-10s : Aucun objet visible
    - 10-20s : Objet entre dans le champ, se d√©place
    - 20-30s : Objet sort du champ
    - 30-40s : Objet r√©appara√Æt

.. step::
    2) Impl√©menter la d√©tection frame par frame sur vid√©o :

    .. code-block:: python

       import cv2
       from ultralytics import YOLO
       import time
       
       def detect_on_video(model_path, video_path, output_path, conf_threshold=0.5):
           """D√©tecte les objets sur chaque frame et sauvegarde la vid√©o."""
           # TODO: Charger le mod√®le YOLO
           # TODO: Ouvrir la vid√©o (fps, dimensions, nb_frames)
           # TODO: Cr√©er VideoWriter pour la sortie
           # TODO: Boucle sur les frames:
           #   - Lire frame
           #   - Mesurer temps de traitement
           #   - Pr√©diction YOLO
           #   - Dessiner d√©tections + info (frame, FPS)
           #   - Sauvegarder frame
           # TODO: Lib√©rer ressources et afficher stats
           pass

.. step::
    3) Analyser les statistiques de d√©tection :

    .. code-block:: python

       import matplotlib.pyplot as plt
       
       def plot_detection_stats(stats):
           """Visualise les statistiques."""
           # TODO: Graphique 1: D√©tections par frame (ligne)
           # TODO: Graphique 2: Distribution nb objets (histogramme)
           # TODO: Graphique 3: Temps de traitement (ligne + moyenne)
           # TODO: Sauvegarder la figure
           pass


**Questions :**

.. step::
    4) Quel est le FPS moyen de votre syst√®me ? Est-ce suffisant pour du temps r√©el (>30 fps) ?

.. step::
    5) Pourquoi le temps de traitement varie-t-il d'une frame √† l'autre ?

.. step::
    6) Comment pourriez-vous am√©liorer la vitesse si elle est trop lente ?

.. step::
    7) Quelles sont les limitations de cette approche sans identit√© d'objets ?


**Astuce :**
.. spoiler::
    .. discoverList::
        1. FPS r√©el = 1 / temps_traitement_moyen
        2. Temps r√©el n√©cessite g√©n√©ralement >25-30 FPS pour fluidit√©
        3. Variation temps : complexit√© variable de l'image, nombre d'objets
        4. Optimisation du temps d'entra√Ænement : r√©duire le nombre de pixels par exemple 640‚Üí320
        5. VideoWriter : m√™me fps que la vid√©o source pour synchronisation
        6. Utiliser ``model.predict(frame, verbose=False)`` pour √©viter logs
        7. Pour mesurer FPS : ``fps = 1.0 / (time.time() - start_time)``


**Pour aller plus loin : Tracking avec identit√©s**

.. note::
    **Limitations du tracking frame par frame :**
    
    Cette approche simple ne permet pas de :
    
    - Savoir si c'est le **m√™me objet** d'une frame √† l'autre
    - **Compter** combien d'objets distincts sont apparus dans la vid√©o
    - Suivre les **trajectoires** et analyser les mouvements
    - G√©rer les **occlusions** temporaires (objet cach√© puis r√©appara√Æt)
    
    **Solution : Tracking avec identit√©s (Object ID)**
    
    Pour un vrai syst√®me de tracking, il faut :
    
    1. **Assigner un ID unique** √† chaque objet d√©tect√©
    2. **Associer les d√©tections** entre frames successives :
       - Comparer les positions (distance euclidienne)
       - Si deux d√©tections sont proches ‚Üí m√™me objet
       - Si d√©tection loin de tous les objets connus ‚Üí nouvel objet
    3. **G√©rer les disparitions** :
       - Garder l'ID en m√©moire pendant N frames
       - Si l'objet r√©appara√Æt ‚Üí retrouver son ID
       - Sinon ‚Üí supprimer l'ID apr√®s N frames

    
    **Utilisation de YOLO pour le tracking avanc√© : https://docs.ultralytics.com/modes/track/**
    
    .. code-block:: python
    
        from ultralytics import YOLO
        model = YOLO('yolo11n.pt')
        results = model.track(source='video.mp4', persist=True)  # Track avec IDs !
        