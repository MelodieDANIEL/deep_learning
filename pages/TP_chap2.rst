üèãÔ∏è Travaux Pratiques 2
=========================
.. slide::
Sur cette page se trouvent des exercices de TP sur le Chapitre 2. Ils sont class√©s par niveau de difficult√© :
.. discoverList::
    * Facile : üçÄ
    * Moyen : ‚öñÔ∏è
    * Difficile : üå∂Ô∏è




.. slide::
üçÄ Exercice 1 : Approximations d‚Äôune fonction non lin√©aire
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Dans cet exercice, vous allez impl√©menter une boucle d'entra√Ænement simple pour ajuster les param√®tres d‚Äôun mod√®le polyn√¥mial comme dans le chapitre 1, puis comparer les r√©sultats avec ceux d'un mod√®le MLP.

On vous donne les donn√©es suivantes :

.. code-block:: python

    torch.manual_seed(0)

    X = torch.linspace(-3, 3, 100).unsqueeze(1)
    y_true = torch.sin(X) + 0.1 * torch.randn(X.size())  # fonction sinuso√Ødale bruit√©e

**Objectif :** Comparer deux mod√®les pour approximer la fonction :

1. Polyn√¥me cubique : $$y = f(x) = a x^3 + b x^2 + c x + d$$, o√π $$a, b, c, d$$ sont des param√®tres appris automatiquement en minimisant l‚Äôerreur entre les pr√©dictions et les donn√©es r√©elles comme dans le chapitre 1.

2. MLP simple :  

    - Impl√©ment√© sous forme de classe ``nn.Module``  
    - 2 couches cach√©es de 10 neurones chacune avec `ReLU`` pour l'activation
    - Entr√©e : 1 feature, sortie : 1 pr√©diction

**Consigne :** √âcrire un programme qui :

1) Ajuste les param√®tres du polyn√¥me cubique aux donn√©es en utilisant PyTorch.  
2) Affiche les param√®tres appris $$a, b, c, d$$.  
3) Impl√©mente ensuite un MLP et entra√Æne-le sur les m√™mes donn√©es pendant 5000 epochs avec un learning rate de 0.01.  
4) Compare visuellement les deux mod√®les avec les donn√©es r√©elles sur un m√™me graphique. 
5) Que remarquez-vous sur les performances des deux mod√®les ?
6) Que se passe-t-il si vous augmentez le nombre du polyn√¥me ?


**Astuce :**
.. spoiler::
    .. discoverList::
        1. Initialiser les param√®tres du polyn√¥me avec ``torch.randn(1, requires_grad=True)``.  
        2. Utiliser ``nn.MSELoss()`` comme fonction de perte pour les deux mod√®les.  
        3. Pour le MLP, cr√©er une classe h√©ritant de ``nn.Module`` et d√©finir ``forward``.  
        4. Utiliser ``optimizer.zero_grad()``, ``loss.backward()``, ``optimizer.step()`` √† chaque it√©ration.  
        5. On voit que le MLP parvient √† mieux s'adapter aux donn√©es, car il peut capturer des relations non lin√©aires plus complexes.

**R√©sultat attendu :** Vous devez obtenir un graphique similaire √† celui ci-dessous o√π :  

- les points bleus correspondent aux donn√©es r√©elles (``y_true``)  
- la courbe rouge correspond au polyn√¥me cubique  
- la courbe verte correspond au MLP  

.. image:: images/chap2_exo_1_resultat.png
    :alt: R√©sultat Exercice 1
    :align: center


.. slide::
‚öñÔ∏è Exercice 2 : Comparaison de l'entra√Ænement d'un MLP sur donn√©es brutes et standardis√©es
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Dans cet exercice, vous allez entra√Æner un MLP simple sur un jeu de donn√©es synth√©tiques avec deux features ayant des √©chelles diff√©rentes. Vous comparerez les performances lorsque les donn√©es sont brutes ou standardis√©es.

On vous donne les donn√©es suivantes :

.. code-block:: python

    # Donn√©es synth√©tiques
    N = 500
    X1 = torch.linspace(0, 1, N).unsqueeze(1)      # petite amplitude
    X2 = torch.linspace(0, 100, N).unsqueeze(1)    # grande amplitude
    X = torch.cat([X1, X2], dim=1)
    y = 3*X1 + 0.05*X2**2 + torch.randn(N,1) * 0.5


**Objectif :**  
Comprendre l‚Äôimportance de la standardisation des donn√©es pour l‚Äôentra√Ænement d‚Äôun r√©seau de neurones et observer l‚Äô√©volution de la loss.


**Consigne :** √âcrire un programme qui :  

1) D√©finit une classe MLP simple sans couches cach√©es avec : 

   - une couche lin√©aire d‚Äôentr√©e (2 features) vers 20 neurones  
   - une fonction d‚Äôactivation ``ReLU``  
   - une couche de sortie avec 1 pr√©diction 

2) Cr√©e deux mod√®les : un pour les donn√©es brutes, un pour les donn√©es standardis√©es.  

3) Entra√Æne les deux mod√®les avec Adam et une fonction de perte MSE pendant 1000 epochs avec un learning rate de 0.01.

4) Stocke et trace l‚Äô√©volution de la loss pour les deux mod√®les.  

5) Trace les pr√©dictions finales des deux mod√®les sur le m√™me graphique que les donn√©es r√©elles.  

6) Comparez les performances des deux mod√®les et notez lequel converge plus vite et donne de meilleures pr√©dictions.

7) A quelle epoch peut-on consid√©rer que le mod√®le sur donn√©es standardis√©es a converg√© et comment on peut faire pour le d√©terminer ?


**Astuce :**
.. spoiler::
    .. discoverList::
        1. N‚Äôoubliez pas d initialiser les poids du mod√®le avec ``torch.randn()`` pour un d√©marrage al√©atoire et de  mettre ``optimizer.zero_grad()`` avant ``loss.backward()``.  
        2. Pour standardiser, utilisez ``(X - X_mean)/X_std``.  
        3. Pour visualiser la loss : stockez ``loss.item()`` √† chaque epoch et utilisez ``matplotlib.pyplot.plot()``.  
        4. Pour visualiser les pr√©dictions, utilisez un scatter plot avec les donn√©es r√©elles et les pr√©dictions des deux mod√®les.
        5. Pour savoir quand stopper l'entra√Ænement, vous pouvez faire du Early Stopping.
        6. Pour que l‚Äôearly stopping fonctionne correctement avec ce type de donn√©es, il est conseill√© de :

            - Mettre le param√®tre ``patience`` √† 20.  
            - Comparer la perte actuelle avec la meilleure perte pr√©c√©dente en utilisant un seuil de tol√©rance. Par exemple, arrondir la perte √† 5 pour consid√©rer une am√©lioration significative (``if loss.item() < best_loss - 5``)

**R√©sultat attendu :**  
Le graphique montre les pr√©dictions du MLP sur les donn√©es brutes (rouge) et standardis√©es (bleu) par rapport aux donn√©es r√©elles (noir).  Vous devez obtenir un r√©sultat similaire √† celui-ci avant de r√©duire le nombre d'epochs :

.. image:: images/chap2_exo_2_resultat.png
    :alt: Comparaison MLP brutes vs standardis√©es
    :align: center


.. slide::
üèãÔ∏è Exercices suppl√©mentaires 2
===============================
Dans cette section, il y a des exercices suppl√©mentaires pour vous entra√Æner. Ils suivent le m√™me classement de difficult√© que pr√©c√©demment.

.. slide::
‚öñÔ∏è Exercice suppl√©mentaire 1 : Approximation d'une fonction 2D avec un MLP 
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Cet exercise propose l'entra√Ænement d'un MLP avec des donn√©es en 2D.

**Objectif :** Entra√Æner un MLP pour approximer la fonction suivante :

.. math::

    y = \sin(X_1) + \cos(X_2)

o√π $(X_1, X_2) \in [-2,2]^2$, et visualiser la pr√©diction du mod√®le par rapport √† la fonction r√©elle.

**Consigne :**  

1. G√©n√©rer ``N = 800`` points al√©atoires $$(X_1, X_2)$$ dans $$[-2,2]$$ et calculer $$y$$ en suivant la fonction.

2. Standardiser les entr√©es pour le MLP.

3. Cr√©er un MLP simple :

   - Entr√©e : 2 features  
   - 2 couches cach√©es de 64 neurones avec activation ``Tanh``  
   - Sortie : 1 pr√©diction

4. Entra√Æner le mod√®le avec Adam et MSE loss pendant 1000 epochs.

5. Ajouter early stopping avec ``patience = 20`` et ``tolerance = 0.1``.

6. Pr√©parer une grille 2D pour visualiser la fonction r√©elle et la pr√©diction du mod√®le.

7. Afficher sur une seule figure 3D* :

   - Surface r√©elle en vert transparent  
   - Surface pr√©dite par le MLP en orange semi-transparent  
   - Ajouter une l√©gende pour distinguer les surfaces

8. Tracer l'√©volution de la loss pendant l'entra√Ænement pour v√©rifier la convergence.

9. Refaire un test avec des donn√©es bruit√©es (ajouter un bruit gaussien de moyenne 0 et √©cart-type 0.6 √† y) et observer l'impact sur la pr√©diction du MLP.

**Questions :**  

- Que remarquez-vous sur la capacit√© du MLP √† approximer la fonction sous-jacente malgr√© le bruit‚ÄØ?  
- Que se passe-t-il si vous augmentez ou diminuez le niveau de bruit‚ÄØ?  
- Comment l‚Äôearly stopping impacte-t-il l‚Äôapprentissage‚ÄØ?

**Astuce :**
.. spoiler::
    .. discoverList::
        1. Pour l'early stopping, stocker la meilleure loss et un compteur d'epochs sans am√©lioration.  
        2. Pour la visualisation, utiliser ``ax.plot_surface`` pour les surfaces et ``Patch`` pour la l√©gende.  
        3. La standardisation permet au MLP de mieux converger.  
        4. V√©rifier la loss finale pour s'assurer que le mod√®le a appris correctement la fonction.
        5. Pour g√©n√©rer le bruit, utilisez ``0.6 * torch.randn_like(y_clean)``.

**Astuce avanc√©e :**        
.. spoiler::
    .. discoverList:: 
        **Voici le code pour la visualisation 3D avec Matplotlib :**
        .. code-block:: python
            x1g, x2g = torch.meshgrid(
            torch.linspace(-2, 2, 80),
            torch.linspace(-2, 2, 80),
            indexing="ij"
            )

            Xg = torch.cat([x1g.reshape(-1,1), x2g.reshape(-1,1)], dim=1)
            Xg_std = (Xg - X_mean) / X_std

            with torch.no_grad():
                y_true_grid = (torch.sin(x1g) + torch.cos(x2g))
                y_pred_grid = model(Xg_std).reshape(80, 80)
                y_pred_train = model(X_stdized)

            fig = plt.figure(figsize=(9,7))
            ax = fig.add_subplot(111, projection='3d')
            ax.set_title("MLP 2D avec Early Stopping")
            ax.set_xlabel("X1"); ax.set_ylabel("X2"); ax.set_zlabel("y")
            ax.set_xlim(-2, 2); ax.set_ylim(-2, 2); ax.set_zlim(-2, 2)
            try:
                ax.set_box_aspect((1,1,1))
            except Exception:
                pass
            ax.view_init(elev=25, azim=35)

            ax.plot_surface(x1g.numpy(), x2g.numpy(), y_true_grid.numpy(),
                            cmap="Greens", alpha=0.45, linewidth=0)
            ax.plot_surface(x1g.numpy(), x2g.numpy(), y_pred_grid.numpy(),
                            cmap="Oranges", alpha=0.70, linewidth=0)
            legend_elements = [
                Patch(facecolor="tab:green", alpha=0.45, label="Surface vraie"),
                Patch(facecolor="tab:orange", alpha=0.70, label="Surface MLP")
            ]
            ax.legend(handles=legend_elements, loc="upper left")


            plt.tight_layout()
            plt.show()


**R√©sultat attendu :**

- Voici un exemple de la figure 3D attendues pour les points 1 √† 8 de la consigne avec la surface r√©elle (verte) et la surface pr√©dite par le MLP (orange) :

.. image:: images/chap2_exo_sup_1_resultat.png
    :alt: R√©sultat attendu MLP 2D
    :align: center

- Voici un exemple de la figure 3D attendues pour le point 9 de la consigne avec la surface r√©elle (verte) et la surface pr√©dite par le MLP (orange) :

.. image:: images/chap2_exo_sup_1_suite_resultat.png
    :alt: R√©sultat attendu MLP 2D
    :align: center