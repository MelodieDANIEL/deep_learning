.. slide::

Chapitre 6 ‚Äî D√©tection d'objets avec des bo√Ætes englobantes
================

üéØ Objectifs du Chapitre
----------------------

.. important::

   √Ä la fin de ce chapitre, vous saurez : 

   - Comprendre la diff√©rence entre classification et d√©tection d'objets.
   - Extraire des images depuis une vid√©o.
   - Utiliser Label Studio pour annoter des objets avec des bo√Ætes englobantes de mani√®re collaborative.
   - Comprendre et manipuler les formats d'annotations.
   - Cr√©er un dataset PyTorch pour la d√©tection d'objets.
   - Entra√Æner un d√©tecteur custom.
   - Comparer avec YOLO et choisir le bon mod√®le selon le contexte.
   - Effectuer l'inf√©rence sur des images en temps r√©el.

.. slide::

üìñ 1. Classification vs D√©tection : comprendre la diff√©rence
----------------------

1.1. Classification d'images (chapitres pr√©c√©dents)
~~~~~~~~~~~~~~~~~~~

Dans les chapitres pr√©c√©dents, nous avons travaill√© sur la **classification d'images** : le mod√®le devait r√©pondre √† la question *"Qu'est-ce qu'il y a dans cette image ?"*

**Exemple** : 

- Entr√©e : une image $$224√ó224$$ pixels
- Sortie : une classe parmi N possibles (ex : "chat", "chien", "voiture")
- Une seule pr√©diction par image

.. code-block:: python

   # Classification : une image ‚Üí une classe
   output = model(image)  # Shape: [batch_size, num_classes]
   predicted_class = torch.argmax(output, dim=1)
   print(f"Cette image contient : {classes[predicted_class]}")

.. slide::

1.2. D√©tection d'objets : localiser ET classifier
~~~~~~~~~~~~~~~~~~~

La **d√©tection d'objets** va plus loin : le mod√®le doit r√©pondre √† *"Qu'est-ce qu'il y a dans cette image ET o√π se trouve chaque objet ?"*

**Pour chaque objet d√©tect√©, le mod√®le doit fournir** :

1. **La classe** de l'objet (ex : "personne", "voiture", "chien")
2. **La bo√Æte englobante** (bounding box en anglais) : 4 coordonn√©es d√©finissant un rectangle autour de l'objet
3. **Un score de confiance** : probabilit√© que la d√©tection soit correcte (0 √† 1)

**Exemple de sortie** :

.. code-block:: python

   # D√©tection : une image ‚Üí plusieurs objets localis√©s
   outputs = model(image)
   # outputs[0]['boxes']: tensor([[x1, y1, x2, y2], [x1, y1, x2, y2], ...])
   # outputs[0]['labels']: tensor([1, 3, 1, ...])  # IDs des classes
   # outputs[0]['scores']: tensor([0.95, 0.87, 0.76, ...])  # Confiances

üí° **Intuition** : imaginez que vous regardez une photo de famille. La classification dirait "photo de groupe", tandis que la d√©tection indiquerait "3 personnes aux positions (x1,y1,x2,y2), (x3,y3,x4,y4), (x5,y5,x6,y6)".

.. slide::

1.3. Qu'est-ce qu'une bo√Æte englobante ?
~~~~~~~~~~~~~~~~~~~

Une **bo√Æte englobante** est un rectangle d√©fini par 4 valeurs. Il existe plusieurs fa√ßons de repr√©senter ces coordonn√©es :

**Format 1 : (x1, y1, x2, y2)** ‚Äî> coins de la bo√Æte (PyTorch/torchvision)

- ``x1, y1`` : coordonn√©es du coin sup√©rieur gauche
- ``x2, y2`` : coordonn√©es du coin inf√©rieur droit

**Format 2 : (x, y, w, h)** ‚Äî> coin + dimensions (Label Studio format standard)

- ``x, y`` : coordonn√©es du coin sup√©rieur gauche (en % )
- ``w`` : largeur de la bo√Æte
- ``h`` : hauteur de la bo√Æte

**Format 3 : (x_center, y_center, w, h) normalis√©** (Label Studio format utilis√© pour YOLO)

- ``x_center, y_center`` : coordonn√©es du centre (normalis√©es entre 0 et 1)
- ``w, h`` : largeur et hauteur (normalis√©es entre 0 et 1)

.. code-block:: text

   Exemple d'une image $$640√ó480$$ pixels avec un objet :
   
   Format PyTorch : [100, 50, 300, 250]
   ‚Üí Rectangle du pixel (100,50) au pixel (300,250)
   
   Format Label Studio : [15.625, 10.417, 31.25, 41.67]
   ‚Üí Coin en (15.625%, 10.417%), taille 31.25%√ó41.67% de l'image
   
   Format YOLO : [0.3125, 0.3125, 0.3125, 0.4167]
   ‚Üí Centre √† 31.25% de la largeur/hauteur, bo√Æte de 31.25%√ó41.67% de l'image

.. slide::

1.4. Applications concr√®tes de la d√©tection
~~~~~~~~~~~~~~~~~~~

La d√©tection d'objets est au c≈ìur de nombreuses applications :

- **V√©hicules autonomes** : d√©tecter pi√©tons, voitures, panneaux
- **Surveillance vid√©o** : compter les personnes, d√©tecter des comportements suspects
- **Commerce** : compter les produits en rayon, d√©tecter les vols
- **M√©dical** : localiser des tumeurs, anomalies sur des radiographies
- **R√©alit√© augment√©e** : d√©tecter des objets pour y superposer des informations

üí° Dans ce chapitre, nous allons apprendre √† cr√©er notre propre d√©tecteur d'objets personnalis√©, de A √† Z !

.. slide::

üìñ 2. Pr√©parer les donn√©es : de la vid√©o aux images annot√©es
----------------------

Le pipeline complet pour cr√©er un dataset de d√©tection :

1. **Capturer une vid√©o** de l'objet √† d√©tecter
2. **Extraire des images** (frames) depuis la vid√©o
3. **Annoter** les objets sur chaque image
4. **Exporter** les annotations dans un format standard
5. **Organiser** le dataset pour l'entra√Ænement

Voyons chaque √©tape en d√©tail.

.. slide::

2.1. Capturer une vid√©o
~~~~~~~~~~~~~~~~~~~

**Objectif** : filmer l'objet que vous voulez d√©tecter sous diff√©rents angles et conditions.

**Conseils pratiques** :

- Dur√©e : 30 secondes √† 2 minutes suffisent
- Vari√©t√© : filmez l'objet sous diff√©rents angles, distances, √©clairages
- Stabilit√© : √©vitez les mouvements trop brusques
- Qualit√© : r√©solution HD ($$1280√ó720$$ ou $$1920√ó1080$$) recommand√©e

üí° **Astuce** : plus vous capturez de vari√©t√©, meilleur sera votre d√©tecteur !

.. note::

   **üí° Vous pouvez commencer avec beaucoup moins !**
   
   - ~50-100 photos extraites d'une vid√©o faite avec un smartphone suffisent pour d√©buter
   - R√©solution modeste ($$640√ó480$$ ou $$224√ó224$$) acceptable pour un prototype
   - M√™me avec peu de vari√©t√©, vous obtiendrez d√©j√† des r√©sultats !

.. slide::

2.2. Installation d'OpenCV
~~~~~~~~~~~~~~~~~~~

**OpenCV (cv2)** est une biblioth√®que Python tr√®s puissante pour manipuler des vid√©os. Elle s'utilise directement en Python sans installer d'outils externes.

.. code-block:: bash

   # Installer OpenCV dans votre environnement virtuel
   pip install opencv-python

.. slide::

2.3. Script d'extraction de base
~~~~~~~~~~~~~~~~~~~

Voici un script complet pour extraire toutes les frames d'une vid√©o :

.. code-block:: python

   import cv2
   import os

   def extraire_frames(video_path, output_dir):
       """
       Extrait toutes les frames d'une vid√©o.
       
       Args:
           video_path: chemin vers la vid√©o
           output_dir: dossier o√π sauvegarder les images
       """
       # Cr√©er le dossier de sortie (exist_ok=True √©vite l'erreur si le dossier existe d√©j√†)
       os.makedirs(output_dir, exist_ok=True)
       
       # Ouvrir la vid√©o
       cap = cv2.VideoCapture(video_path)
       
       # V√©rifier que la vid√©o s'ouvre correctement
       if not cap.isOpened():
           print(f"‚ùå Erreur : impossible d'ouvrir {video_path}")
           return

       # Obtenir les propri√©t√©s de la vid√©o (fps, nombre total de frames)
       fps = cap.get(cv2.CAP_PROP_FPS)
       total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
       
       print(f"üìπ Vid√©o : {total_frames} frames √† {fps:.2f} fps")
       
       frame_count = 0
       
       while True:
           # Lire la frame suivante
           ret, frame = cap.read()
           
           # Si plus de frames, sortir de la boucle
           if not ret:
               break
           
           # Sauvegarder la frame en jpg pour compression
           output_path = os.path.join(output_dir, f'frame_{frame_count:05d}.jpg')
           cv2.imwrite(output_path, frame)
           
           frame_count += 1
       
       # Lib√©rer les ressources
       cap.release()
       
       print(f"‚úì {frame_count} frames extraites dans {output_dir}")

   # Utilisation
   extraire_frames('ma_video.mp4', 'frames/')

.. warning::

   ‚ö†Ô∏è **Attention √† la quantit√© !**
   
   Une vid√©o de 30 secondes √† 30 fps g√©n√®re **900 images**. C'est souvent trop pour annoter manuellement !

.. slide::

2.4. Extraction intelligente (sous-√©chantillonnage)
~~~~~~~~~~~~~~~~~~~

Pour r√©duire le nombre d'images √† annoter, on extrait seulement certaines frames :

.. code-block:: python

   import cv2
   import os

   def extraire_frames_espacees(video_path, output_dir, intervalle=10):
       """
       Extrait 1 frame tous les N frames.
       
       Args:
           video_path: chemin vers la vid√©o
           output_dir: dossier de sortie
           intervalle: extraire 1 frame tous les N frames (ex: 10)
       """
       os.makedirs(output_dir, exist_ok=True)
       
       cap = cv2.VideoCapture(video_path)
       
       if not cap.isOpened():
           print(f"‚ùå Erreur : impossible d'ouvrir {video_path}")
           return
       
       fps = cap.get(cv2.CAP_PROP_FPS)
       total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
       
       print(f"üìπ Extraction de 1 frame tous les {intervalle} frames")
       print(f"   Total attendu : ~{total_frames // intervalle} images")
       
       frame_count = 0
       saved_count = 0
       
       while True:
           ret, frame = cap.read()
           if not ret:
               break
           
           # Sauvegarder seulement toutes les N frames
           if frame_count % intervalle == 0:
               output_path = os.path.join(output_dir, f'frame_{saved_count:05d}.jpg')
               cv2.imwrite(output_path, frame)
               saved_count += 1
           
           frame_count += 1
       
       cap.release()
       
       print(f"‚úì {saved_count} frames extraites sur {frame_count} totales")

   # Exemple : extraire 1 frame toutes les 10 frames
   extraire_frames_espacees('ma_video.mp4', 'frames/', intervalle=10)

**Recommandation pratique** : pour d√©buter, extraire 50-200 images est un bon compromis entre travail d'annotation et qualit√© du mod√®le.

**R√®gles de calcul de l'intervalle** :

- Vid√©o √† 30 fps, 1 frame/seconde ‚Üí ``intervalle=30``
- Vid√©o √† 30 fps, 1 frame toutes les 10 frames ‚Üí ``intervalle=10``
- Extraire ~100 images d'une vid√©o de 900 frames ‚Üí ``intervalle=9``

.. slide::

2.5. Redimensionner les images √† l'extraction
~~~~~~~~~~~~~~~~~~~

Pour √©conomiser l'espace disque et acc√©l√©rer le traitement, on peut redimensionner directement.

.. warning::

   ‚ö†Ô∏è **Attention √† la d√©formation !**
   
   Si votre vid√©o n'est **pas carr√©e** (ex : $$1920√ó1080$$) et que vous redimensionnez en **carr√©** (ex : $$224√ó224$$), l'image sera **d√©form√©e** (√©cras√©e ou √©tir√©e).
   
   **Deux solutions** :
   
   1. **Crop au centre** (RECOMMAND√â) : d√©couper un carr√© au centre avant de redimensionner
   2. **Padding** : ajouter des bordures noires pour garder le ratio

.. slide::

Voici les deux approches :

**Approche 1 : Crop au centre (recommand√©e - pas de d√©formation)**

.. code-block:: python

   import cv2
   import os

   def extraire_frames_crop_redimensionner(video_path, output_dir, intervalle=10, 
                                           target_size=224):
       """
       Extrait, crop au centre en carr√©, puis redimensionne.
       √âVITE la d√©formation en d√©coupant l'image.
       
       Args:
           video_path: chemin vers la vid√©o
           output_dir: dossier de sortie
           intervalle: extraire 1 frame tous les N frames
           target_size: taille finale du carr√© (ex: 224 pour 224√ó224)
       """
       os.makedirs(output_dir, exist_ok=True)
       
       cap = cv2.VideoCapture(video_path)
       
       if not cap.isOpened():
           print(f"‚ùå Erreur : impossible d'ouvrir {video_path}")
           return
       
       # Obtenir les dimensions originales
       original_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
       original_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
       
       print(f"üìπ R√©solution originale : {original_width}√ó{original_height}")
       print(f"üìê Nouvelle r√©solution : {target_size}√ó{target_size} (carr√©)")
       print(f"‚úÇÔ∏è  M√©thode : Crop au centre (pas de d√©formation)")
       
       frame_count = 0
       saved_count = 0
       
       while True:
           ret, frame = cap.read()
           if not ret:
               break
           
           if frame_count % intervalle == 0:
               # √âTAPE 1 : Crop au centre pour obtenir un carr√©
               h, w = frame.shape[:2]
               size = min(h, w)  # Prendre la plus petite dimension
               
               # Calculer les coordonn√©es du crop au centre
               start_y = (h - size) // 2
               start_x = (w - size) // 2
               
               # D√©couper le carr√© au centre
               cropped = frame[start_y:start_y+size, start_x:start_x+size]
               
               # √âTAPE 2 : Redimensionner le carr√© √† la taille souhait√©e
               resized = cv2.resize(cropped, (target_size, target_size))
               
               # Sauvegarder
               output_path = os.path.join(output_dir, f'frame_{saved_count:05d}.jpg')
               cv2.imwrite(output_path, resized)
               saved_count += 1
           
           frame_count += 1
       
       cap.release()
       
       print(f"‚úì {saved_count} frames extraites, cropp√©es et redimensionn√©es")

   # Exemple : extraire 1 frame/seconde en 224√ó224 (format standard CNN)
   extraire_frames_crop_redimensionner('ma_video.mp4', 'frames/', 
                                       intervalle=30, target_size=224)


.. slide::

**Approche 2 : Redimensionnement direct (D√âCONSEILL√â si ratio diff√©rent)**

.. code-block:: python

   def extraire_frames_redimensionner_simple(video_path, output_dir, intervalle=10, 
                                             target_width=640, target_height=480):
       """
       Redimensionne directement sans crop.
       ‚ö†Ô∏è ATTENTION : d√©forme l'image si le ratio change !
       """
       os.makedirs(output_dir, exist_ok=True)
       
       cap = cv2.VideoCapture(video_path)
       
       if not cap.isOpened():
           print(f"‚ùå Erreur : impossible d'ouvrir {video_path}")
           return
       
       original_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
       original_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
       
       print(f"üìπ R√©solution originale : {original_width}√ó{original_height}")
       print(f"üìê Nouvelle r√©solution : {target_width}√ó{target_height}")
       
       # V√©rifier si le ratio va changer
       original_ratio = original_width / original_height
       target_ratio = target_width / target_height
       
       if abs(original_ratio - target_ratio) > 0.01:
           print(f"‚ö†Ô∏è  ATTENTION : Le ratio va changer !")
           print(f"    Original : {original_ratio:.2f}")
           print(f"    Cible : {target_ratio:.2f}")
           print(f"    ‚Üí L'image sera d√©form√©e !")
       
       frame_count = 0
       saved_count = 0
       
       while True:
           ret, frame = cap.read()
           if not ret:
               break
           
           if frame_count % intervalle == 0:
               # Redimensionner directement (PEUT D√âFORMER !)
               resized = cv2.resize(frame, (target_width, target_height))
               
               output_path = os.path.join(output_dir, f'frame_{saved_count:05d}.jpg')
               cv2.imwrite(output_path, resized)
               saved_count += 1
           
           frame_count += 1
       
       cap.release()
       
       print(f"‚úì {saved_count} frames extraites et redimensionn√©es")

   # Exemple : ‚ö†Ô∏è Vid√©o 16:9 ‚Üí carr√© = D√âFORMATION !
   # extraire_frames_redimensionner_simple('ma_video.mp4', 'frames/', 
   #                                        intervalle=30, 
   #                                        target_width=224, target_height=224)

üí° **Recommandations** :

- **Pour la d√©tection d'objets** : utilisez le crop au centre pour √©viter les d√©formations.
- **Pour la classification** : le crop au centre est aussi pr√©f√©rable.
- **R√©solutions recommand√©es** : $$224√ó224$$ (standard CNN), $$640√ó480$$ (compromis vitesse/qualit√©), $$800√ó600$$ (bonne qualit√©).

.. slide::

üìñ 3. Annotation avec Label Studio
----------------------

**Label Studio** est un outil open-source d'annotation collaborative qui permet de cr√©er des bo√Ætes englobantes, de g√©rer plusieurs annotateurs et d'exporter dans diff√©rents formats.

3.1. Installation et premier lancement
~~~~~~~~~~~~~~~~~~~

.. code-block:: bash

   # Installer Label Studio (dans votre environnement virtuel)
   pip install label-studio
   
   # Lancer Label Studio
   label-studio start
   
   # L'interface web s'ouvre automatiquement sur http://localhost:8080

**Premier lancement : cr√©ation du compte**

Au premier lancement, Label Studio vous demande de cr√©er un compte.

.. note::

   üí° **Travail collaboratif**
   
   Si vous souhaitez travailler en √©quipe, vous pourrez inviter vos coll√®gues via **"Invite People"** (voir section 3.5). Label Studio leur enverra automatiquement un email d'invitation.

**Si vous avez d√©j√† un compte** : entrez simplement votre email et mot de passe pour vous connecter.

**En cas de probl√®me** : si Label Studio ne s'ouvre pas automatiquement, ouvrez manuellement votre navigateur et allez sur ``http://localhost:8080``

.. slide::

3.2. Cr√©er un projet d'annotation
~~~~~~~~~~~~~~~~~~~

**√âtapes dans l'interface web** :

1. Cliquer sur "Create Project"

2. Donner un nom au projet (ex : "Detection_Bouteille")

3. **Import des donn√©es** :
   
   - Onglet "Data Import"
   - S√©lectionner tous les fichiers du dossier ``frames/`` (ou le dossier o√π vous avez extrait les images)
   - Cliquer sur "Import"

4. **Configuration de l'annotation** :
   
   - Cliquez sur votre projet pour l'ouvrir
   - Cliquez sur "Settings" (en haut √† droite ou dans le menu du projet)
   - Allez dans l'onglet "Labeling Interface"
   - Cliquez sur "Browse Templates"
   - S√©lectionnez "Computer Vision"
   - Choisissez "Object Detection with Bounding Boxes"
   - Une page s'ouvre pour d√©finir les labels (classes d'objets)

.. slide::

3.3. D√©finir les classes d'objets
~~~~~~~~~~~~~~~~~~~

Apr√®s avoir choisi le template, une interface appara√Æt o√π vous pouvez d√©finir vos labels (classes d'objets).

**M√©thode simple : ajouter des labels via l'interface**

1. Dans le champ "Add Label Name", entrez le nom de votre premi√®re classe (ex : "bouteille")
2. Cliquez sur "Add" ou appuyez sur Entr√©e
3. R√©p√©tez pour chaque classe d'objet √† d√©tecter
4. Cliquez sur "Save" pour valider

**Si vous pr√©f√©rez √©diter le code XML directement**, vous pouvez voir/modifier le code de configuration :

**Exemple pour d√©tecter des bouteilles et des gobelets** :

.. code-block:: xml

   <View>
     <Image name="image" value="$image"/>
     <RectangleLabels name="label" toName="image">
       <Label value="bouteille" background="green"/>
       <Label value="gobelet" background="blue"/>
     </RectangleLabels>
   </View>

üí° **Astuce** : commencez avec une seule classe pour simplifier. Vous pourrez toujours ajouter des classes plus tard.

.. slide::

3.4. Annoter les images
~~~~~~~~~~~~~~~~~~~

**Pour cr√©er une annotation** :

1. Cliquer sur une t√¢che (image) dans la liste
2. S√©lectionner la classe dans le panneau en bas de l'image (ex : "bouteille")
3. Dessiner un rectangle autour de l'objet :
   
   - Cliquer et maintenir le bouton de la souris
   - D√©placer pour cr√©er le rectangle
   - Rel√¢cher quand l'objet est bien encadr√©

4. R√©p√©ter pour tous les objets de l'image
5. Cliquer sur "Submit" pour valider l'annotation

**Modifier une annotation existante** :

- **Double-cliquer** sur un rectangle pour le s√©lectionner
- Vous pouvez alors :
  
  - **D√©placer** le rectangle en le faisant glisser
  - **Redimensionner** en tirant sur les coins ou les bords
  - **Changer la classe** dans le panneau de droite
  - **Supprimer** avec la touche ``Suppr``

**Bonnes pratiques d'annotation** :

- La bo√Æte doit englober **tout l'objet** visible (pas trop serr√©e, pas trop large)
- Si un objet est **partiellement visible** (coup√© par le bord), l'annoter quand m√™me
- Si un objet est **tr√®s petit** (<10 pixels), c'est optionnel (difficiles √† d√©tecter)
- **Coh√©rence** : gardez le m√™me style d'annotation d'une image √† l'autre

.. slide::

3.5. Annotation collaborative : inviter des personnes
~~~~~~~~~~~~~~~~~~~

Pour travailler en √©quipe sur l'annotation, suivez ces √©tapes :

**√âtape 1 : Inviter des personnes**

1. Dans Label Studio, cliquez sur l'ic√¥ne **Organization** (en haut √† droite, ic√¥ne avec plusieurs personnes)

2. Allez dans l'onglet **"People"**

3. Cliquez sur le bouton **"Invite People"**

4. Entrez les adresses email de vos coll√®gues (ex : ``marie.dubois@exemple.com``, ``paul.martin@exemple.com``)

5. Choisissez le r√¥le pour chaque personne :
   
   - **Annotator** : peut uniquement annoter les images
   - **Reviewer** : peut annoter ET valider/corriger les annotations des autres
   - **Manager** : peut g√©rer les projets et les param√®tres

6. Cliquez sur **"Send Invitations"**

7. Vos coll√®gues recevront un email avec un lien pour cr√©er leur compte

.. slide::

**√âtape 2 : Ajouter les membres √† votre projet**

Une fois les invitations accept√©es :

1. Ouvrez votre projet d'annotation
2. Allez dans **"Settings"** ‚Üí **"Members"**
3. Cliquez sur **"Add Member"**
4. S√©lectionnez les personnes dans la liste
5. Assignez-leur le r√¥le appropri√© pour ce projet

**√âtape 3 : R√©partir le travail (optionnel mais recommand√©)**

Pour √©viter que deux personnes annotent les m√™mes images :

1. Dans le projet, onglet **"Tasks"** (liste des images)
2. S√©lectionnez un groupe d'images (ex : images 1-50)
3. Menu **"Actions"** ‚Üí **"Assign Annotators"**
4. Choisissez la personne
5. R√©p√©tez pour les autres groupes d'images

**Exemple de workflow collaboratif** :

- **Marie** (Annotator) : images 1-50
- **Paul** (Annotator) : images 51-100
- **Sophie** (Reviewer) : v√©rifie et corrige toutes les annotations
- **Vous** (Manager) : supervise et exporte les donn√©es finales

üí° **Astuce qualit√©** : faites annoter 10 images par deux personnes diff√©rentes et comparez. Un IoU (Intersection over Union) > 0.7 indique une bonne coh√©rence entre annotateurs.

.. slide::

3.6. Raccourcis clavier utiles
~~~~~~~~~~~~~~~~~~~

Pour acc√©l√©rer l'annotation :

- ``1, 2, 3...`` : s√©lectionner la classe 1, 2, 3...
- ``Ctrl + Enter`` ou ``Cmd + Enter`` : soumettre et passer √† l'image suivante
- ``Ctrl + Z`` : annuler la derni√®re action
- ``Suppr`` : supprimer la bo√Æte s√©lectionn√©e
- ``Fl√®ches`` : ajuster finement la position d'une bo√Æte

.. slide::

üìñ 4. Formats d'annotations : Label Studio JSON et YOLO
----------------------

Apr√®s l'annotation dans Label Studio, nous allons utiliser deux formats diff√©rents dans ce chapitre :

1. **Format JSON** : pour cr√©er notre d√©tecteur CNN custom (sections 5-7)
2. **Format YOLO** : pour entra√Æner YOLO sur notre dataset (section 9)

.. slide::

4.1. Format Label Studio JSON
~~~~~~~~~~~~~~~~~~~

Label Studio utilise son propre format JSON qui stocke toutes les annotations dans un seul fichier.

**Caract√©ristiques principales** :

- **Un fichier JSON** contenant toutes les images et leurs annotations
- **Coordonn√©es en pourcentages** : ``x, y, width, height`` exprim√©s en % de la taille de l'image (0-100)
- ``x, y`` : position du coin sup√©rieur gauche en %
- ``width, height`` : dimensions de la bo√Æte en %
- ``rectanglelabels`` : liste des classes d√©tect√©es

**Avantages** :

- Format complet avec m√©tadonn√©es (id, date, utilisateur...)
- Un seul fichier pour tout le dataset (facile √† manipuler)
- Structure Python-friendly (facile √† parser)

**Inconv√©nients** :

- Fichier unique qui peut devenir volumineux
- N√©cessite de cr√©er un Dataset PyTorch custom

.. note::

   üí° La structure d√©taill√©e du JSON sera pr√©sent√©e en **section 5** avec des exemples de parsing Python

.. slide::

4.2. Format YOLO (TXT) 
~~~~~~~~~~~~~~~~~~~

**YOLO** utilise un format ultra-simple : un fichier texte par image.

**Exemple de fichier** ``frame_00001.txt`` :

.. code-block:: text

   0 0.3125 0.2604 0.3125 0.3125
   1 0.6250 0.5417 0.1562 0.2500

**Format d'une ligne** : ``class_id x_center y_center width height``

**Toutes les valeurs sont normalis√©es entre 0 et 1** :

- ``class_id`` : entier (0, 1, 2...) correspondant √† l'index de la classe
- ``x_center`` : position X du centre / largeur de l'image
- ``y_center`` : position Y du centre / hauteur de l'image
- ``width`` : largeur de la bo√Æte / largeur de l'image
- ``height`` : hauteur de la bo√Æte / hauteur de l'image

**Exemple de calcul** (image $$640√ó480$$, objet de 100,50 √† 300,200) :

.. code-block:: python

   # Coordonn√©es en pixels
   x1, y1, x2, y2 = 100, 50, 300, 200
   img_width, img_height = 640, 480
   
   # Calcul des valeurs YOLO
   x_center = ((x1 + x2) / 2) / img_width  # (100+300)/2 / 640 = 0.3125
   y_center = ((y1 + y2) / 2) / img_height  # (50+200)/2 / 480 = 0.2604
   width = (x2 - x1) / img_width  # (300-100) / 640 = 0.3125
   height = (y2 - y1) / img_height  # (200-50) / 480 = 0.3125
   
   # Ligne YOLO : "0 0.3125 0.2604 0.3125 0.3125"

.. slide::

**Avantages du format YOLO** :

- Format ultra-compact et rapide √† parser
- Un fichier par image (facile √† parall√©liser)
- Coordonn√©es normalis√©es (insensible √† la r√©solution)
- Utilis√© nativement par tous les mod√®les YOLO

**Structure compl√®te d'un dataset YOLO** :

.. code-block:: text

   dataset_yolo/
   ‚îú‚îÄ‚îÄ images/           # Toutes les images
   ‚îÇ   ‚îú‚îÄ‚îÄ img1.jpg
   ‚îÇ   ‚îî‚îÄ‚îÄ img2.jpg
   ‚îú‚îÄ‚îÄ labels/           # Fichiers .txt (m√™mes noms que les images)
   ‚îÇ   ‚îú‚îÄ‚îÄ img1.txt
   ‚îÇ   ‚îî‚îÄ‚îÄ img2.txt
   ‚îî‚îÄ‚îÄ classes.txt       # Liste des noms de classes (1 par ligne)

.. slide::

4.3. Exporter depuis Label Studio
~~~~~~~~~~~~~~~~~~~

Label Studio peut exporter dans plusieurs formats. **Dans ce chapitre, nous allons utiliser :**

1. **Le format JSON natif de Label Studio** pour cr√©er un **d√©tecteur CNN custom** (sections 5-7)
2. **Le format YOLO** pour entra√Æner YOLO sur notre dataset custom (section 9)

**√âtapes pour exporter** :

1. Ouvrez votre projet dans Label Studio
2. Cliquez sur "Export" en haut de la liste des t√¢ches
3. Choisir le format :
   
   - **"JSON"** ‚Üí format natif Label Studio (pour sections 5-7)
   - **"YOLO"** ‚Üí fichiers .txt au format YOLO (pour section 9)

4. T√©l√©charger le fichier

.. slide::

üìñ 5. Comprendre le format JSON de Label Studio
----------------------

Nous allons utiliser le **format JSON natif de Label Studio** pour entra√Æner notre d√©tecteur custom. Voyons d'abord sa structure.

5.1. Structure du fichier JSON export√©
~~~~~~~~~~~~~~~~~~~

Apr√®s avoir cliqu√© sur "Export" ‚Üí "JSON" dans Label Studio, vous obtenez un fichier avec cette structure :

.. code-block:: json

   [
     {
       "id": 1,
       "annotations": [
         {
           "id": 1,
           "completed_by": 1,
           "result": [
             {
               "original_width": 224,
               "original_height": 224,
               "value": {
                 "x": 24.67,
                 "y": 45.99,
                 "width": 52.41,
                 "height": 54.01,
                 "rotation": 0,
                 "rectanglelabels": ["cube"]
               },
               "type": "rectanglelabels",
               "from_name": "label",
               "to_name": "image"
             }
           ]
         }
       ],
       "file_upload": "ad2a7904-frame_000000.jpg",
       "data": {
         "image": "/data/upload/1/ad2a7904-frame_000000.jpg"
       }
     },
     {
       "id": 2,
       "annotations": [...],
       "file_upload": "caed06ef-frame_000001.jpg",
       "data": {
         "image": "/data/upload/1/caed06ef-frame_000001.jpg"
       }
     }
   ]

.. slide::

**Points importants** :

- Chaque √©l√©ment du tableau JSON repr√©sente **une image**
- ``file_upload`` : nom original du fichier image
- ``data.image`` : chemin dans Label Studio (√† ignorer, on utilise ``file_upload``)
- ``annotations[0].result`` : liste des bo√Ætes englobantes
- ``value.x, y, width, height`` : **coordonn√©es en pourcentage** (0-100) de l'image
- ``value.rectanglelabels`` : liste des labels (ici un seul)
- ``original_width`` et ``original_height`` : dimensions de l'image (utile pour v√©rifier)

.. slide::

5.2. Extraire le nom de fichier depuis le JSON
~~~~~~~~~~~~~~~~~~~

Le champ ``file_upload`` contient le nom du fichier tel que stock√© par Label Studio (avec un pr√©fixe UUID ajout√© automatiquement). Voici comment l'utiliser :

.. code-block:: python

   import json

   # Charger le JSON
   with open('project-1-annotations.json', 'r', encoding='utf-8') as f: # ADAPTEZ : le nom de fichier
       data = json.load(f)

   # Examiner la premi√®re image
   first_item = data[0]
   
   # Le champ file_upload contient le nom avec le pr√©fixe UUID
   image_name = first_item['file_upload']
   print(f"Nom du fichier : {image_name}")
   # Exemple : "ad2a7904-frame_000000.jpg"
   
   # Vous pouvez aussi l'extraire depuis data.image (m√™me r√©sultat)
   import os
   image_path = first_item['data']['image']
   image_name_alt = os.path.basename(image_path)
   print(f"Nom du fichier (depuis path) : {image_name_alt}")
   # Exemple : "ad2a7904-frame_000000.jpg"
   
   # V√©rifier les dimensions de l'image dans les annotations
   result = first_item['annotations'][0]['result'][0]
   print(f"Dimensions : {result['original_width']}x{result['original_height']}")
   # Exemple : "Dimensions : 224x224"

.. warning::

   ‚ö†Ô∏è **Attention au pr√©fixe UUID !**
   
   Label Studio ajoute automatiquement un pr√©fixe UUID lors de l'upload (ex : ``ad2a7904-frame_000000.jpg``). 
   
   **Si vos fichiers images ont les noms originaux** (``frame_000000.jpg``), vous devrez extraire la partie originale du nom.

.. slide::

**Script complet : nettoyer le JSON et renommer les images** :

.. code-block:: python

   import json
   import os
   import shutil

   def clean_labelstudio_dataset(json_path, images_dir, output_json_path=None):
       """
       Nettoie compl√®tement un dataset Label Studio :
       - Enl√®ve les pr√©fixes UUID du JSON
       - Renomme les fichiers images correspondants
       
       Args:
           json_path: chemin vers le JSON Label Studio
           images_dir: dossier contenant les images
           output_json_path: chemin JSON de sortie (None = √©crase l'original)
       """
       
       def remove_prefix(filename):
           """Enl√®ve le pr√©fixe UUID (8 caract√®res hexa + tiret)."""
           if '-' in filename:
               parts = filename.split('-', 1)
               if len(parts[0]) == 8 and all(c in '0123456789abcdef' for c in parts[0].lower()):
                   return parts[1]
           return filename
       
       # 1. NETTOYER LE JSON
       print("üìÑ Nettoyage du JSON...")
       with open(json_path, 'r', encoding='utf-8') as f:
           data = json.load(f)
       
       json_changes = 0
       for item in data:
           # Nettoyer file_upload
           if 'file_upload' in item:
               original = remove_prefix(item['file_upload'])
               if original != item['file_upload']:
                   print(f"  ‚úì {item['file_upload']} ‚Üí {original}")
                   item['file_upload'] = original
                   json_changes += 1
           
           # Nettoyer data.image
           if 'data' in item and 'image' in item['data']:
               path = item['data']['image']
               filename = os.path.basename(path)
               cleaned = remove_prefix(filename)
               if '/' in path:
                   item['data']['image'] = path.rsplit('/', 1)[0] + '/' + cleaned
               else:
                   item['data']['image'] = cleaned
       
       # Sauvegarder le JSON nettoy√©
       output_path = output_json_path or json_path
       with open(output_path, 'w', encoding='utf-8') as f:
           json.dump(data, f, indent=2, ensure_ascii=False)
       
       print(f"  ‚úì {json_changes} noms nettoy√©s dans le JSON")
       print(f"  ‚úì JSON sauvegard√© : {output_path}\n")
       
       # 2. RENOMMER LES IMAGES
       print("üñºÔ∏è  Renommage des images...")
       if not os.path.exists(images_dir):
           print(f"  ‚ö†Ô∏è  Dossier introuvable : {images_dir}")
           return
       
       image_changes = 0
       for filename in os.listdir(images_dir):
           if not filename.lower().endswith(('.jpg', '.jpeg', '.png')):
               continue
           
           original = remove_prefix(filename)
           if original != filename:
               old_path = os.path.join(images_dir, filename)
               new_path = os.path.join(images_dir, original)
               
               if os.path.exists(new_path):
                   print(f"  ‚ö†Ô∏è  {original} existe d√©j√†, ignor√©")
                   continue
               
               shutil.move(old_path, new_path)
               print(f"  ‚úì {filename} ‚Üí {original}")
               image_changes += 1
       
       print(f"\n‚úÖ Termin√© ! {image_changes} images renomm√©es")

   # üéØ UTILISATION
   clean_labelstudio_dataset(
       json_path='project-1-annotations.json',
       images_dir='data/images/',
       output_json_path='project-1-annotations-clean.json'  # Ou None pour √©craser
   )

üí° **Astuce** : le format peut varier selon la configuration de Label Studio. Utilisez ``file_upload`` si disponible, sinon extrayez depuis ``data.image``.

.. slide::

5.3. V√©rifier que tout fonctionne
~~~~~~~~~~~~~~~~~~~

Apr√®s avoir nettoy√© le JSON, v√©rifiez que les donn√©es sont correctes :

.. code-block:: python

   import json
   import os
   import cv2

   def verify_labelstudio_dataset(json_path, images_dir, num_samples=5):
       """
       V√©rifie que le JSON et les images correspondent.
       Affiche les statistiques et dessine quelques exemples.
       
       Args:
           json_path: JSON Label Studio (nettoy√©)
           images_dir: dossier des images
           num_samples: nombre d'images √† visualiser
       """
       
       # Charger le JSON
       with open(json_path, 'r', encoding='utf-8') as f:
           data = json.load(f)
       
       print(f"üìä STATISTIQUES DU DATASET")
       print(f"   Nombre d'images : {len(data)}")
       
       # Compter les objets et classes
       total_objects = 0
       classes_count = {}
       missing_images = []
       
       for item in data:
           image_name = item['file_upload']
           full_path = os.path.join(images_dir, image_name)
           
           # V√©rifier que l'image existe
           if not os.path.exists(full_path):
               missing_images.append(image_name)
               continue
           
           # Compter les objets
           annotations = item.get('annotations', [])
           if annotations:
               result = annotations[-1].get('result', [])
               for ann in result:
                   if ann.get('type') == 'rectanglelabels':
                       total_objects += 1
                       label = ann['value']['rectanglelabels'][0]
                       classes_count[label] = classes_count.get(label, 0) + 1
       
       print(f"   Objets annot√©s : {total_objects}")
       print(f"   Classes : {list(classes_count.keys())}")
       for cls, count in classes_count.items():
           print(f"      - {cls}: {count} objets")
       
       if missing_images:
           print(f"\n‚ö†Ô∏è  {len(missing_images)} images manquantes :")
           for img in missing_images[:5]:
               print(f"      - {img}")
       else:
           print(f"\n‚úÖ Toutes les images sont pr√©sentes !")
       
       # Visualiser quelques exemples
       print(f"\nüñºÔ∏è  VISUALISATION DE {num_samples} EXEMPLES")
       os.makedirs('verification', exist_ok=True)
       
       for idx, item in enumerate(data[:num_samples]):
           image_name = item['file_upload']
           full_path = os.path.join(images_dir, image_name)
           
           if not os.path.exists(full_path):
               continue
           
           # Charger l'image
           img = cv2.imread(full_path)
           h, w = img.shape[:2]
           
           # Dessiner les bo√Ætes
           annotations = item.get('annotations', [])
           if annotations:
               result = annotations[-1].get('result', [])
               for ann in result:
                   if ann.get('type') != 'rectanglelabels':
                       continue
                   
                   value = ann['value']
                   label = value['rectanglelabels'][0]
                   
                   # Convertir % ‚Üí pixels
                   x1 = int(value['x'] * w / 100)
                   y1 = int(value['y'] * h / 100)
                   x2 = int((value['x'] + value['width']) * w / 100)
                   y2 = int((value['y'] + value['height']) * h / 100)
                   
                   # Dessiner
                   cv2.rectangle(img, (x1, y1), (x2, y2), (0, 255, 0), 2)
                   cv2.putText(img, label, (x1, y1-10),
                              cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)
           
           # Sauvegarder
           output_path = f'verification/check_{idx:02d}_{image_name}'
           cv2.imwrite(output_path, img)
           print(f"   ‚úì {output_path}")
       
       print(f"\n‚úÖ V√©rification termin√©e ! Consultez le dossier 'verification/'")

   # üéØ UTILISATION
   verify_labelstudio_dataset(
       json_path='project-1-annotations-clean.json',
       images_dir='data/images/',
       num_samples=5
   )

üí° **Conseil** : v√©rifiez toujours vos donn√©es avant de lancer l'entra√Ænement !

.. slide::

üìñ 6. Cr√©er un Dataset PyTorch pour la d√©tection
----------------------

Maintenant que nos annotations sont pr√™tes, cr√©ons un Dataset PyTorch personnalis√© qui charge directement le JSON de Label Studio.

6.1. Structure de dossiers recommand√©e
~~~~~~~~~~~~~~~~~~~

Organisez vos fichiers ainsi :

.. code-block:: text

   mon_projet_detection/
   ‚îú‚îÄ‚îÄ data/
   ‚îÇ   ‚îú‚îÄ‚îÄ images/           # Toutes les images
   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ frame_00001.jpg
   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ frame_00002.jpg
   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ...
   ‚îÇ   ‚îú‚îÄ‚îÄ annotations.json  # Export Label Studio
   ‚îÇ   ‚îî‚îÄ‚îÄ splits.json       # Split train/val/test (optionnel)
   ‚îî‚îÄ‚îÄ train.py              # Script d'entra√Ænement

**Fichier** ``splits.json`` **(optionnel)** : pour s√©parer train/val/test

.. code-block:: json

   {
     "train": ["frame_00001.jpg", "frame_00002.jpg", ...],
     "val": ["frame_00151.jpg", "frame_00152.jpg", ...],
     "test": ["frame_00181.jpg", "frame_00182.jpg", ...]
   }

.. note::

   üí° **Split automatique avec random_split**
   
   Pas besoin de cr√©er ``splits.json`` ! Vous pouvez s√©parer train/val/test directement dans le code avec ``random_split`` comme au chapitre 5.

.. slide::

6.2. Classe DetectionDataset compl√®te
~~~~~~~~~~~~~~~~~~~

Voici une impl√©mentation qui charge directement le JSON de Label Studio :

.. warning::

   ‚ö†Ô∏è **Pr√©requis : Nettoyage des UUID**
   
   Avant d'utiliser ce Dataset, assurez-vous d'avoir :
   
   1. ‚úÖ Nettoy√© le JSON avec ``clean_labelstudio_dataset()`` (section 5.2)
   2. ‚úÖ Renomm√© les images pour enlever les pr√©fixes UUID (section 5.2)
   3. ‚úÖ V√©rifi√© avec ``verify_labelstudio_dataset()`` (section 5.3)
   
   **Sinon**, le Dataset ne trouvera pas les images (erreur ``FileNotFoundError``) car les noms dans le JSON ne correspondront pas aux noms des fichiers sur le disque.

.. code-block:: python

   import torch
   from torch.utils.data import Dataset
   from PIL import Image
   import json
   import os
   from torchvision.transforms import functional as F

   class LabelStudioDetectionDataset(Dataset):
       """
       Dataset PyTorch qui charge directement les annotations Label Studio.
       """
       
       def __init__(self, json_path, images_dir, split_images=None, transforms=None):
           """
           Args:
               json_path: chemin vers le JSON export√© de Label Studio
               images_dir: dossier contenant les images
               split_images: liste de noms d'images √† utiliser (None = toutes)
               transforms: transformations √† appliquer (optionnel)
           """
           self.images_dir = images_dir
           self.transforms = transforms
           
           # Charger le JSON
           with open(json_path, 'r', encoding='utf-8') as f:
               all_data = json.load(f)
           
           # Filtrer selon split_images si fourni
           if split_images:
               split_set = set(split_images)
               self.data = [
                   item for item in all_data
                   if os.path.basename(item['data']['image']) in split_set
               ]
           else:
               self.data = all_data
           
           # Extraire les noms de classes uniques
           classes_set = set()
           for item in self.data:
               annotations = item.get('annotations', [])
               if annotations:
                   result = annotations[-1].get('result', [])
                   for ann in result:
                       if ann.get('type') == 'rectanglelabels':
                           labels = ann['value'].get('rectanglelabels', [])
                           classes_set.update(labels)
           
           self.classes = sorted(list(classes_set))
           self.class_to_idx = {cls: idx+1 for idx, cls in enumerate(self.classes)}
           
           print(f"Dataset initialis√© : {len(self.data)} images, "
                 f"{len(self.classes)} classes : {self.classes}")
       
       def __len__(self):
           return len(self.data)
       
       def __getitem__(self, idx):
           """
           Charge une image et ses annotations.
           
           Returns:
               img: tensor [3, H, W]
               target: dict avec 'boxes', 'labels', 'image_id'
           """
           item = self.data[idx]
           
           # Extraire le nom de l'image et la charger
           image_path_str = item['data']['image']
           image_name = os.path.basename(image_path_str)
           full_path = os.path.join(self.images_dir, image_name)
           
           img = Image.open(full_path).convert('RGB')
           img_width, img_height = img.size
           
           # R√©cup√©rer les annotations
           boxes = []
           labels = []
           
           annotations = item.get('annotations', [])
           if annotations:
               # Prendre la derni√®re version (plus r√©cente)
               result = annotations[-1].get('result', [])
               
               for ann in result:
                   if ann.get('type') != 'rectanglelabels':
                       continue
                   
                   value = ann['value']
                   
                   # Label Studio donne les coordonn√©es en pourcentages (0-100)
                   x_percent = value['x']
                   y_percent = value['y']
                   w_percent = value['width']
                   h_percent = value['height']
                   
                   # Convertir en pixels [x1, y1, x2, y2]
                   x1 = (x_percent / 100.0) * img_width
                   y1 = (y_percent / 100.0) * img_height
                   x2 = ((x_percent + w_percent) / 100.0) * img_width
                   y2 = ((y_percent + h_percent) / 100.0) * img_height
                   
                   boxes.append([x1, y1, x2, y2])
                   
                   # R√©cup√©rer la classe
                   class_name = value['rectanglelabels'][0]
                   class_idx = self.class_to_idx[class_name]
                   labels.append(class_idx)
           
           # Convertir en tenseurs
           boxes = torch.as_tensor(boxes, dtype=torch.float32)
           labels = torch.as_tensor(labels, dtype=torch.int64)
           
           # Cr√©er le dictionnaire target
           target = {}
           target['boxes'] = boxes
           target['labels'] = labels
           target['image_id'] = torch.tensor([idx])
           
           # Si aucune bo√Æte, cr√©er des tenseurs vides
           if len(boxes) == 0:
               target['boxes'] = torch.zeros((0, 4), dtype=torch.float32)
               target['labels'] = torch.zeros((0,), dtype=torch.int64)
           
           # Appliquer les transformations
           if self.transforms:
               img = self.transforms(img)
           else:
               img = F.to_tensor(img)
           
           return img, target
       
       def get_class_name(self, class_id):
           """Retourne le nom d'une classe depuis son ID."""
           return self.classes[class_id - 1]

.. note::

   üí° **Gestion des IDs de classes**
   
   - Les classes sont automatiquement extraites du JSON
   - Les IDs commencent √† **1** (0 est r√©serv√© au background dans torchvision)
   - ``class_to_idx`` : dictionnaire ``{'bouteille': 1, 'gobelet': 2}``

.. slide::

6.3. Cr√©er les DataLoaders avec split automatique
~~~~~~~~~~~~~~~~~~~

Si vous n'avez pas de fichier ``splits.json``, utilisez ``random_split`` comme au chapitre 5 :

.. code-block:: python

   from torch.utils.data import DataLoader, random_split

   # Charger le dataset complet
   full_dataset = LabelStudioDetectionDataset(
       json_path='data/annotations.json',
       images_dir='data/images/'
   )

   # Split : 70% train, 15% val, 15% test
   total_size = len(full_dataset)
   train_size = int(0.70 * total_size)
   val_size = int(0.15 * total_size)
   test_size = total_size - train_size - val_size

   train_dataset, val_dataset, test_dataset = random_split(
   full_dataset, 
   [train_size, val_size, test_size],
   generator=torch.Generator().manual_seed(42)
   )

   print(f"Train : {len(train_dataset)} images")
   print(f"Val   : {len(val_dataset)} images")
   print(f"Test  : {len(test_dataset)} images")

   # Cr√©er les dataloaders
   def collate_fn(batch):
       """Fonction n√©cessaire car chaque image a un nombre diff√©rent d'objets."""
       return tuple(zip(*batch))

   train_loader = DataLoader(
       train_dataset,
       batch_size=4,
       shuffle=True,
       num_workers=4,
       collate_fn=collate_fn
   )

   val_loader = DataLoader(
       val_dataset,
       batch_size=4,
       shuffle=False,
       num_workers=4,
       collate_fn=collate_fn
   )

üí° **Avantage** : tout en un ! Pas besoin de g√©rer des listes de noms de fichiers s√©par√©es.

.. slide::

6.5. Tester le chargement des donn√©es
~~~~~~~~~~~~~~~~~~~

Toujours v√©rifier que le Dataset charge correctement :

.. code-block:: python

   # Charger un exemple
   img, target = train_dataset[0]

   print(f"Image shape: {img.shape}")
   print(f"Nombre d'objets: {len(target['boxes'])}")
   print(f"Boxes:\n{target['boxes']}")
   print(f"Labels: {target['labels']}")

   # Visualiser quelques exemples
   import matplotlib.pyplot as plt
   import matplotlib.patches as patches

   def visualize_sample(dataset, idx):
       img, target = dataset[idx]
       
       # Convertir le tensor en numpy pour l'affichage
       img_np = img.permute(1, 2, 0).numpy()
       
       fig, ax = plt.subplots(1, figsize=(12, 8))
       ax.imshow(img_np)
       
       # Dessiner chaque bo√Æte
       for box, label in zip(target['boxes'], target['labels']):
           x1, y1, x2, y2 = box.tolist()
           width = x2 - x1
           height = y2 - y1
           
           rect = patches.Rectangle(
               (x1, y1), width, height,
               linewidth=2, edgecolor='r', facecolor='none'
           )
           ax.add_patch(rect)
           
           # Ajouter le label
           class_name = dataset.get_class_name(label.item())
           ax.text(x1, y1-5, class_name, 
                  bbox=dict(facecolor='red', alpha=0.5),
                  fontsize=12, color='white')
       
       plt.axis('off')
       plt.tight_layout()
       plt.savefig(f'check_sample_{idx}.png')
       print(f"‚úì Visualisation sauvegard√©e : check_sample_{idx}.png")

   # V√©rifier les 5 premiers exemples
   for i in range(5):
       visualize_sample(train_dataset, i)


.. slide::

üìñ 7. CNN ultra-simple : r√©gression directe de bo√Æte
----------------------

Pour des cas simples avec **1 seul objet par image**, on peut utiliser une approche beaucoup plus simple que YOLO ou Faster R-CNN : **r√©gression directe des coordonn√©es** de la bo√Æte. Le mod√®le pr√©dit directement 4 nombres : ``(x_center, y_center, width, height)`` normalis√©s dans [0,1].

.. note::

   üí° **Quand utiliser cette approche ?**
   
   ‚úÖ **OUI** : 1 objet par image, objet centr√©, peu de variations (ex: d√©tection de visage, logo)
   
   ‚ùå **NON** : plusieurs objets, positions variables, objets qui se chevauchent, etc.

.. slide::
    
7.1. Architecture ultra-simple
~~~~~~~~~~~~~~~~~~~

Le mod√®le est constitu√© d'un **backbone CNN** (4 couches Conv2D + MaxPool) suivi d'un **head de r√©gression** (2 couches FC) qui pr√©dit directement les 4 coordonn√©es normalis√©es. Dans l'exemple, l‚Äôentr√©e $$224√ó224$$ est r√©duite **4 fois** par MaxPool(2): $$224‚Üí112‚Üí56‚Üí28‚Üí14$$; la carte de features finale est donc $$14√ó14$$. Si vous changez la taille d‚Äôentr√©e ou le nombre de couches √† stride 2, la taille de la grille changera.

.. code-block:: python

   import torch
   import torch.nn as nn
   import torch.nn.functional as F
   from tqdm import tqdm # Pour les barres de progression

   class SimpleBBoxRegressor(nn.Module):
       """
       CNN ultra-simple qui r√©gresse directement UNE bo√Æte par image.
       Sortie : [x_center, y_center, width, height] normalis√©s dans [0,1]
       """
       
       def __init__(self):
           super().__init__()
           
           # Backbone simple : Conv2D + MaxPool (comme chapitre 5)
           self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)
           self.pool1 = nn.MaxPool2d(2)  # 224 -> 112
           
           self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)
           self.pool2 = nn.MaxPool2d(2)  # 112 -> 56
           
           self.conv3 = nn.Conv2d(32, 64, kernel_size=3, padding=1)
           self.pool3 = nn.MaxPool2d(2)  # 56 -> 28
           
           self.conv4 = nn.Conv2d(64, 128, kernel_size=3, padding=1)
           self.pool4 = nn.MaxPool2d(2)  # 28 -> 14
           
           # Apr√®s 4 MaxPool: 224‚Üí112‚Üí56‚Üí28‚Üí14
           # Taille finale: [B, 128, 14, 14]
           
           # Head de r√©gression : 4 sorties (x, y, w, h)
           self.fc1 = nn.Linear(128 * 14 * 14, 128)
           self.fc2 = nn.Linear(128, 4)  # x_center, y_center, width, height
       
       def forward(self, x):
           # Backbone
           x = self.pool1(F.relu(self.conv1(x)))
           x = self.pool2(F.relu(self.conv2(x)))
           x = self.pool3(F.relu(self.conv3(x)))
           x = self.pool4(F.relu(self.conv4(x)))
           
           # Flatten
           x = x.view(x.size(0), -1)  # [B, 128*14*14]
           
           # R√©gression
           x = F.relu(self.fc1(x))
           x = torch.sigmoid(self.fc2(x))  # Sortie dans [0, 1]
           
           return x  # [B, 4] : (x_center, y_center, w, h) normalis√©s


   # Cr√©er le mod√®le
   simple_model = SimpleBBoxRegressor().to(device)
   num_params = sum(p.numel() for p in simple_model.parameters() if p.requires_grad)
   print(f"‚úÖ Mod√®le cr√©√© : {num_params:,} param√®tres")
   print(f"üìä Architecture : Conv2D (16‚Üí32‚Üí64‚Üí128) + Flatten + FC(128‚Üí4)")

.. note::

   üìä **Taille du mod√®le**

   Ce mod√®le a environ **25 millions** de param√®tres (principalement dans la premi√®re couche FC ``128*14*14 ‚Üí 128``). C'est bien plus petit que Faster R-CNN (``>40M``) qui est plus g√©n√©rique.

.. slide::

7.2. Loss et optimiseur
~~~~~~~~~~~~~~~~~~~

**Loss MSE** pour les coordonn√©es normalis√©es (x_center, y_center, width, height) + **pr√©paration des targets**.

.. code-block:: python

   # Loss simple : MSE sur les coordonn√©es
   criterion = nn.MSELoss()
   optimizer = optim.Adam(simple_model.parameters(), lr=1e-3)
   
   # Fonction de pr√©paration des targets
   def prepare_single_box_target(targets, img_size=224):
       """
       Convertit les targets (dict) en format (x_center, y_center, w, h) normalis√©s.
       """
       batch_targets = []
       
       for target in targets:
           # Directement la premi√®re (et unique) bo√Æte
           box = target['boxes'][0]  # [x1, y1, x2, y2]
           
           x1, y1, x2, y2 = box
           x_center = ((x1 + x2) / 2) / img_size
           y_center = ((y1 + y2) / 2) / img_size
           width = (x2 - x1) / img_size
           height = (y2 - y1) / img_size
           
           # Clamp dans [0, 1]
           x_center = torch.clamp(x_center, 0, 1)
           y_center = torch.clamp(y_center, 0, 1)
           width = torch.clamp(width, 0, 1)
           height = torch.clamp(height, 0, 1)
           
           batch_targets.append(torch.tensor([x_center, y_center, width, height], device=device))
       
       return torch.stack(batch_targets)

.. note::

   üìê **Normalisation des coordonn√©es**
   
   - Entr√©e : bo√Ætes en pixels ``[x1, y1, x2, y2]`` dans ``[0, 224]``
   - Sortie : coordonn√©es normalis√©es ``[x_c, y_c, w, h]`` dans ``[0, 1]``
   - Le mod√®le pr√©dit directement ces 4 valeurs normalis√©es

.. slide::

7.3. Entra√Ænement (boucles train/val)
~~~~~~~~~~~~~~~~~~~

Boucles simples d'entra√Ænement et d'√©valuation.

.. code-block:: python
   
   # Fonctions d'entra√Ænement
   def train_simple_epoch(model, criterion, optimizer, loader):
       model.train()
       total_loss = 0
       
       for images, targets in tqdm(loader, desc="Training"):
           images = torch.stack([img.to(device) for img in images])
           
           # Pr√©parer les targets
           batch_targets = prepare_single_box_target(targets)
           
           # Forward
           preds = model(images)
           loss = criterion(preds, batch_targets)
           
           # Backward
           optimizer.zero_grad()
           loss.backward()
           optimizer.step()
           
           total_loss += loss.item() * images.size(0)
       
       return total_loss / len(loader.dataset)
   
   @torch.no_grad()
   def eval_simple_epoch(model, criterion, loader):
       model.eval()
       total_loss = 0
       
       for images, targets in loader:
           images = torch.stack([img.to(device) for img in images])
           batch_targets = prepare_single_box_target(targets)
           
           preds = model(images)
           loss = criterion(preds, batch_targets)
           
           total_loss += loss.item() * images.size(0)
       
       return total_loss / len(loader.dataset)
   
   # LANCER L'ENTRA√éNEMENT
   print("\nüöÄ Entra√Ænement du CNN simple...\n")
   
   num_epochs = 20
   best_val = float('inf')
   
   for epoch in range(num_epochs):
       train_loss = train_simple_epoch(simple_model, criterion, optimizer, train_loader)
       val_loss = eval_simple_epoch(simple_model, criterion, val_loader)
       
       print(f"Epoch {epoch+1:02d} | Train: {train_loss:.4f} | Val: {val_loss:.4f}")
       
       if val_loss < best_val:
           best_val = val_loss
           torch.save(simple_model.state_dict(), 'simple_bbox_regressor.pth')
           print("  ‚úÖ Meilleur mod√®le sauvegard√©!")
   
   print("\n‚úÖ Entra√Ænement termin√©!")
   print(f"üìÅ Mod√®le sauvegard√© : simple_bbox_regressor.pth")

.. note::
   
   Avec ce mod√®le simple, vous devriez voir la loss descendre rapidement (√† partir de l'epoch 5). Si la loss ne descend pas, v√©rifiez que vos donn√©es sont bien normalis√©es.

.. slide::

7.4. √âvaluation sur tout le test data
~~~~~~~~~~~~~~~~~~~

Calcul de l'**IoU moyen** (Intersection over Union) sur le test set.

.. code-block:: python

   # √âvaluation sur TOUT le test set
   print(f"\nüìä √âVALUATION SUR TOUT LE TEST SET ({len(test_dataset)} images)")
   print("="*60)
   
   @torch.no_grad()
   def evaluate_on_test(model, dataset, img_size=224):
       model.eval()
       
       total_iou = 0
       num_samples = len(dataset)
       
       for idx in tqdm(range(num_samples), desc="√âvaluation"):
           img, target = dataset[idx]
           img_tensor = img.unsqueeze(0).to(device)
           
           # Pr√©diction
           pred = model(img_tensor)[0].cpu()
           
           # Convertir en [x1, y1, x2, y2] pixels
           xc, yc, w, h = pred
           pred_x1 = (xc - w/2) * img_size
           pred_y1 = (yc - h/2) * img_size
           pred_x2 = (xc + w/2) * img_size
           pred_y2 = (yc + h/2) * img_size
           
           # GT (prendre la premi√®re bo√Æte)
           if len(target['boxes']) > 0:
               gt_box = target['boxes'][0]
               gt_x1, gt_y1, gt_x2, gt_y2 = gt_box.tolist()
               
               # Calculer IoU
               x1_inter = max(pred_x1.item(), gt_x1)
               y1_inter = max(pred_y1.item(), gt_y1)
               x2_inter = min(pred_x2.item(), gt_x2)
               y2_inter = min(pred_y2.item(), gt_y2)
               
               if x2_inter > x1_inter and y2_inter > y1_inter:
                   inter = (x2_inter - x1_inter) * (y2_inter - y1_inter)
                   pred_area = (pred_x2 - pred_x1) * (pred_y2 - pred_y1)
                   gt_area = (gt_x2 - gt_x1) * (gt_y2 - gt_y1)
                   union = pred_area + gt_area - inter
                   iou = inter / (union + 1e-6)
                   total_iou += iou.item()
       
       mean_iou = total_iou / num_samples
       print(f"\nüìà IoU moyen sur le test set : {mean_iou:.3f}")
       return mean_iou
   
   # √âvaluer
   mean_iou = evaluate_on_test(simple_model, test_dataset)

.. note::

   üìà **Interpr√©tation de l'IoU**

   - IoU $$> 0.5$$ : Bonne d√©tection
   - IoU $$> 0.75$$ : Tr√®s bonne d√©tection
   - IoU $$> 0.9$$ : D√©tection quasi-parfaite

   Un mod√®le bien entra√Æn√© sur ce dataset simple devrait obtenir un IoU moyen $$> 0.8$$.

.. slide::

7.5. Visualisation
~~~~~~~~~~~~~~~~~~~

Affichage des pr√©dictions sur une grille d'images avec GT (vert) et pr√©dictions (rouge).

.. code-block:: python

   # Visualisation de quelques exemples
   print(f"\nüñºÔ∏è  VISUALISATION D'EXEMPLES")
   print("="*60)
   
   import matplotlib.pyplot as plt
   import matplotlib.patches as patches
   import numpy as np
   
   # Afficher 9 exemples (3x3)
   num_to_show = min(9, len(test_dataset))
   indices = np.linspace(0, len(test_dataset)-1, num_to_show, dtype=int)
   
   fig, axes = plt.subplots(3, 3, figsize=(15, 15))
   axes = axes.flatten()
   
   simple_model.eval()
   
   for plot_idx, test_idx in enumerate(indices):
       img, target = test_dataset[test_idx]
       img_tensor = img.unsqueeze(0).to(device)
       
       # Pr√©diction
       with torch.no_grad():
           pred = simple_model(img_tensor)[0].cpu()
       
       # Convertir en pixels
       xc, yc, w, h = pred
       pred_x1 = (xc - w/2) * 224
       pred_y1 = (yc - h/2) * 224
       pred_x2 = (xc + w/2) * 224
       pred_y2 = (yc + h/2) * 224
       
       # Affichage
       ax = axes[plot_idx]
       img_np = img.permute(1, 2, 0).cpu().numpy()
       ax.imshow(img_np)
       ax.set_title(f'Test {test_idx}', fontsize=10)
       
       # GT en vert
       for box in target['boxes']:
           x1_gt, y1_gt, x2_gt, y2_gt = box.tolist()
           rect = patches.Rectangle(
               (x1_gt, y1_gt), x2_gt-x1_gt, y2_gt-y1_gt,
               linewidth=2, edgecolor='green', facecolor='none'
           )
           ax.add_patch(rect)
       
       # Pr√©diction en rouge
       rect = patches.Rectangle(
           (pred_x1.item(), pred_y1.item()), 
           (pred_x2 - pred_x1).item(), 
           (pred_y2 - pred_y1).item(),
           linewidth=2, edgecolor='red', facecolor='none', linestyle='--'
       )
       ax.add_patch(rect)
       
       ax.axis('off')
   
   plt.tight_layout()
   plt.suptitle('Pr√©dictions CNN Simple (Vert=GT, Rouge=Pred)', y=1.002, fontsize=14, weight='bold')
   plt.show()


.. note::

   üé® **L√©gende**
   
   - **Vert** : Ground truth (annotation r√©elle)
   - **Rouge** (pointill√©) : Pr√©diction du mod√®le
   
   Si les bo√Ætes se superposent bien, le mod√®le fonctionne correctement !

.. slide::

üìñ 8. Entra√Ænement avec YOLO sur dataset existant
----------------------

Nous allons maintenant utiliser **YOLOv11** (Ultralytics) pour entra√Æner un d√©tecteur sur un dataset standard. YOLO (You Only Look Once) est un mod√®le utilis√© pour la d√©tection d'objets rapide et efficace, parfait pour la d√©tection en temps r√©el.

8.1. Introduction √† YOLO
~~~~~~~~~~~~~~~~~~~

**YOLO** divise l'image en une **grille** (ex: $$7√ó7$$, $$13√ó13$$, etc.) et pour chaque **cellule** de la grille, pr√©dit :

- **Plusieurs bo√Ætes englobantes candidates** (typiquement 3-9 selon les versions) gr√¢ce aux **anchors**
- Chaque bo√Æte est repr√©sent√©e par : **(x, y, w, h)** relatives au centre de la cellule
- **Objectness** : probabilit√© qu'un objet soit pr√©sent dans cette bo√Æte
- **Classes** : probabilit√©s pour chaque classe (si objet d√©tect√©)

**Avantages de YOLO :**

- ‚úÖ **Rapide** : 30-80 FPS (temps r√©el)
- ‚úÖ **One-stage** : pr√©diction directe
- ‚úÖ **Pr√©cis** : performances sup√©rieures √† Faster R-CNN
- ‚úÖ **Facile √† utiliser** : librairie Ultralytics tr√®s simple

**YOLOv11** est la derni√®re version stable (2024) avec des am√©liorations significatives par rapport √† YOLOv8 (2023) : l'architecture est plus optimis√©e et la pr√©cision est am√©lior√©e tout en √©tant plus rapide. 

.. note::

   üìö **Ressources YOLO**
   
   - Documentation officielle : https://docs.ultralytics.com/
   - GitHub : https://github.com/ultralytics/ultralytics
   - Papier YOLOv11 (2024) : https://arxiv.org/abs/2410.17725
   - Papier YOLOv1 original (2015) : https://arxiv.org/abs/1506.02640

.. slide::

8.2. Concepts cl√©s : Anchors et NMS
~~~~~~~~~~~~~~~~~~~

**C'est quoi un anchor (ancre) ?**

Un anchor est une bo√Æte de r√©f√©rence pr√©d√©finie avec des proportions sp√©cifiques (largeur/hauteur).

**Exemple d'anchors :** 

- Anchor 1 : petit carr√© ($$0.2 √ó 0.2$$ de l'image) ‚Üí pour d√©tecter petits objets

- Anchor 2 : rectangle vertical ($$0.1 √ó 0.3$$) ‚Üí pour personnes debout

- Anchor 3 : rectangle horizontal ($$0.4 √ó 0.2$$) ‚Üí pour voitures

Le mod√®le **ajuste** ces anchors (d√©cale et redimensionne) pour coller aux objets r√©els. C'est plus efficace que de pr√©dire la taille depuis z√©ro !

‚û°Ô∏è **Au total** : Si grille $$13√ó13$$ avec 3 anchors par cellule = $$13√ó13√ó3$$ = **507 bo√Ætes candidates** par image !

.. slide::

**üßπ C'est quoi le NMS (Non-Maximum Suppression) ?**

Probl√®me : Plusieurs bo√Ætes d√©tectent souvent le **m√™me objet** (ex: 5 bo√Ætes qui se chevauchent sur une voiture).

**NMS √©limine les doublons en 3 √©tapes :**

1. **Trier** les bo√Ætes par score de confiance (objectness) d√©croissant

2. **Garder** la bo√Æte avec le meilleur score

3. **Supprimer** toutes les bo√Ætes qui se chevauchent trop (IoU > seuil, ex: 0.5) avec la bo√Æte gard√©e

4. R√©p√©ter pour les bo√Ætes restantes

**Exemple :**

- Avant NMS : 507 bo√Ætes candidates

- Apr√®s NMS : 3-10 d√©tections finales (les meilleures, sans doublons)

Le mod√®le filtre ainsi avec **NMS** pour garder les meilleures d√©tections sans redondance.

.. slide::

8.3. Installation de YOLOv11 (Ultralytics)
~~~~~~~~~~~~~~~~~~~

Installation simple via pip :

.. code-block:: python

   # Installer Ultralytics (inclut YOLOv11)
   !pip install ultralytics
   
   # Imports
   from ultralytics import YOLO
   import torch
   
   print(f"‚úÖ Ultralytics install√© !")
   print(f"üî• PyTorch version: {torch.__version__}")
   print(f"üéÆ CUDA disponible: {torch.cuda.is_available()}")

.. note::

   üí° **Versions compatibles**
   
   - Python ‚â• 3.8
   - PyTorch ‚â• 1.8
   - Ultralytics maintient automatiquement les d√©pendances

.. slide::

8.4. Dataset COCO (Common Objects in Context)
~~~~~~~~~~~~~~~~~~~

**COCO** est le dataset de r√©f√©rence pour la d√©tection d'objets :

- **80 classes** d'objets courants (personne, voiture, chien, etc.)
- **118 000 images** d'entra√Ænement (COCO complet)
- **5 000 images** de validation
- Annotations au format JSON (bo√Ætes + segmentation)

**Pour ce cours, nous utilisons COCO128**, une version r√©duite avec seulement 128 images, car :

- ‚úÖ T√©l√©chargement rapide (6.8 Mo au lieu de ~20 Go)
- ‚úÖ Entra√Ænement rapide (2-3 min au lieu de 6-10h)
- ‚úÖ Parfait pour apprendre et tester

.. note::

   üìä **Classes COCO (extrait)**
   
   0: person, 1: bicycle, 2: car, 3: motorcycle, ... 5: bus, ... 7: truck, ... 15: bird, 16: cat, 17: dog, ... 39: bottle, ... 41: cup, ... 56: chair, ...

.. note::

   üí° **Format COCO vs Format YOLO - Clarification importante**
   
   Le mot "COCO" d√©signe **deux choses diff√©rentes** :
   
   1. **COCO le dataset** : 118k images avec 80 classes d'objets (person, car, dog...)
   2. **COCO le format d'annotation** : fichier JSON avec bo√Ætes au format ``[x, y, width, height]`` en pixels
   
   **YOLO** utilise son **propre format** : fichiers TXT avec coordonn√©es normalis√©es ``[x_center, y_center, w, h]`` (section 4.3)
   
   **Quand on entra√Æne YOLO sur le dataset COCO :**
   
   - Ultralytics t√©l√©charge les annotations COCO (format JSON)
   - Les convertit **automatiquement** en format YOLO (TXT) en interne
   - Vous n'avez **rien √† faire manuellement** !
   
   **Pour un dataset custom (section 9) :**
   
   - Vous annotez dans Label Studio
   - Vous exportez directement au **format YOLO** (section 9.1)
   - Pas besoin de passer par le format COCO


.. slide::

8.5. Entra√Ænement YOLOv11 sur COCO128
~~~~~~~~~~~~~~~~~~~

**8.5.1. Choisir et charger le mod√®le**

YOLOv11 propose plusieurs tailles. Nous utilisons **YOLOv11n (Nano)** pour le cours car il est rapide :

.. code-block:: python
   
   # Charger YOLOv11 Nano (le plus rapide)
   model = YOLO('yolo11n.pt')
   
   print(f"‚úÖ Mod√®le YOLOv11n charg√© (3M param√®tres, 80+ FPS)")

.. note::

   üì¶ **Autres mod√®les disponibles** (pour information)
   
   - ``yolo11n.pt`` : Nano (3M params, 80+ FPS) ‚Üê **on utilise celui-ci**
   - ``yolo11s.pt`` : Small (9M params, 60 FPS)
   - ``yolo11m.pt`` : Medium (20M params, 45 FPS)
   - ``yolo11l.pt`` : Large (26M params, 35 FPS)
   - ``yolo11x.pt`` : XLarge (57M params, 30 FPS)

.. slide::

**8.5.2. T√©l√©charger COCO128**

T√©l√©chargez le dataset COCO128 via Ultralytics :

.. code-block:: python

   from ultralytics.data.utils import check_det_dataset
   
   # T√©l√©charger COCO128 (6.8 Mo, 128 images)
   print("üì• T√©l√©chargement de COCO128 (6.8 Mo)...")
   data_dict = check_det_dataset('coco128.yaml', autodownload=True)
   print(f"‚úÖ Dataset t√©l√©charg√© : {data_dict['path']}")

.. note::

   üíæ **COCO128 : 128 images, 80 classes possibles**
   
   - **128 images** : le nombre d'images dans le dataset
   - **80 classes** : les types d'objets que le mod√®le peut d√©tecter (person, car, dog, etc.)
   - **~20-30 classes pr√©sentes** : seulement ces classes apparaissent dans les 128 images
   - Dataset t√©l√©charg√© dans : ``./datasets/coco128/``

.. slide::

**8.5.3. Lancer l'entra√Ænement**

.. code-block:: python

   # Entra√Æner YOLOv11n sur COCO128
   results = model.train(
       data='coco128.yaml',        # COCO128 (128 images)
       epochs=3,                   # 3 epochs pour le cours (rapide)
       imgsz=640,                  # Taille des images
       batch=16,                   # Batch size (ajuster selon votre GPU)
       device=0,                   # GPU 0 (ou 'cpu' sans GPU)
       project='runs/detect',      # Dossier de sortie
       name='yolo11_coco128'       # Nom de l'exp√©rience
   )
   
   print(f"‚úÖ Entra√Ænement termin√© !")
   print(f"üìÅ R√©sultats : runs/detect/yolo11_coco128/")

.. note::

   ‚è±Ô∏è **Temps d'entra√Ænement**
   
   - **COCO128** (128 images, 3 epochs) : ~5-10 minutes sur GPU
   - **COCO complet** (118k images, 50 epochs) : ~5-10 heures sur GPU
   
   Pour ce cours, COCO128 suffit amplement pour comprendre le fonctionnement !

.. slide::

**8.5.4. Visualiser les r√©sultats de l'entra√Ænement**

Ultralytics g√©n√®re automatiquement plusieurs fichiers de r√©sultats dans ``runs/detect/yolo11_coco128/`` :

- **results.png** : graphiques avec toutes les courbes (loss, mAP, etc.)
- **Courbes de loss** (train/val)
- **M√©triques mAP** (mean Average Precision)
- **Exemples de pr√©dictions**

.. code-block:: python

   # Afficher les r√©sultats de l'entra√Ænement
   from IPython.display import Image, display
   
   # Afficher la courbe de loss
   results_path = 'runs/detect/yolo11_coco128/results.png'
   try:
       print(f"üìä Affichage des courbes d'entra√Ænement YOLO\n")
       display(Image(filename=results_path))
       print(f"\n‚úÖ Graphiques charg√©s depuis : {results_path}")
   except FileNotFoundError:
       print(f"‚ö†Ô∏è Fichier non trouv√© : {results_path}")
       print("   Les r√©sultats seront disponibles apr√®s l'entra√Ænement.")

.. slide::

**8.5.5. Pour aller plus loin : COCO complet (optionnel)**

Si vous voulez entra√Æner sur le dataset complet apr√®s avoir test√© avec COCO128 :

.. code-block:: python

   # T√©l√©charger COCO complet (~20 Go, peut prendre 30-60 min)
   # print("üì• T√©l√©chargement de COCO complet (~20 Go)...")
   # data_dict = check_det_dataset('coco.yaml', autodownload=True)
   
   # Entra√Æner sur COCO complet (plusieurs heures)
   # results = model.train(
   #     data='coco.yaml',         # COCO complet (118k images)
   #     epochs=50,                # 50 epochs minimum
   #     imgsz=640,
   #     batch=16,
   #     device=0,
   #     project='runs/detect',
   #     name='yolo11_coco_full'
   # )

.. slide::

8.6. √âvaluation sur le test set
~~~~~~~~~~~~~~~~~~~

.. code-block:: python

   # Charger le meilleur mod√®le
    model = YOLO('runs/detect/yolo11_coco128/weights/best.pt')
    print("‚úÖ Mod√®le charg√© !")
    
    # √âvaluer sur le validation set
    metrics = model.val()

    print(f"üìä mAP@0.5: {metrics.box.map50:.3f}")
    print(f"üìä mAP@0.5:0.95: {metrics.box.map:.3f}")
    print(f"üìä Precision: {metrics.box.mp:.3f}")
    print(f"üìä Recall: {metrics.box.mr:.3f}")

.. note::

   üìà **M√©triques COCO**
   
   - **mAP@0.5** : Pr√©cision moyenne avec seuil IoU=0.5
   - **mAP@0.5:0.95** : Pr√©cision moyenne sur plusieurs seuils (standard COCO)
   - **Objectif** : mAP@0.5:0.95 > 0.40 pour un bon mod√®le


.. slide::

8.7. Inf√©rence et visualisation
~~~~~~~~~~~~~~~~~~~

Une fois le mod√®le entra√Æn√©, vous pouvez l'utiliser pour d√©tecter des objets dans de nouvelles images.

**√âtape 1 : Faire une pr√©diction sur une image**

.. code-block:: python

   # Pr√©diction sur une image
   results = model.predict(
       source='path/to/image.jpg',  # Chemin vers votre image, dossier, vid√©o, ou URL.
       conf=0.5,                    # Seuil de confiance minimum. Le mod√®le ne garde que les d√©tections avec une confiance $$‚â• 50%$$.
       iou=0.45,                    # Seuil NMS (√©limination des doublons). √âlimine les bo√Ætes qui se chevauchent trop (IoU $$ > 45%$$) pour √©viter les doublons.
       show=False,                  # Ne pas afficher automatiquement
       save=False                   # Ne pas sauvegarder automatiquement
   )

.. slide::

**√âtape 2 : Extraire les r√©sultats**

.. code-block:: python

   # R√©cup√©rer les r√©sultats de la premi√®re image
   result = results[0]
   
   # Extraire les informations des d√©tections
   boxes = result.boxes.xyxy.cpu().numpy()    # Coordonn√©es [x1, y1, x2, y2] en pixels
   confs = result.boxes.conf.cpu().numpy()    # Confiances [0-1]
   classes = result.boxes.cls.cpu().numpy()   # IDs des classes d√©tect√©es
   
   print(f"üéØ {len(boxes)} objets d√©tect√©s !")
   
   # Afficher les d√©tails de chaque d√©tection
   for i, (box, conf, cls) in enumerate(zip(boxes, confs, classes)):
       x1, y1, x2, y2 = box
       class_name = model.names[int(cls)]  # Nom de la classe
       print(f"  Objet {i+1}: {class_name} (confiance: {conf:.2f})")

.. slide::

**√âtape 3 : Visualiser les d√©tections**

.. code-block:: python

   from matplotlib import pyplot as plt
   
   # Ultralytics dessine automatiquement les bo√Ætes avec labels
   # La m√©thode ``result.plot()`` dessine automatiquement : les bo√Ætes englobantes avec couleurs par classe, les noms des classes et les scores de confiance.
   img_with_boxes = result.plot()  # Image numpy avec bo√Ætes dessin√©es
   
   plt.figure(figsize=(12, 8))
   plt.imshow(img_with_boxes)
   plt.axis('off')
   plt.title(f'{len(boxes)} objets d√©tect√©s')
   # L'image s'affiche automatiquement dans le notebook

.. slide::

**√âtape 4 : Visualisation sur plusieurs images**

.. code-block:: python

   # Pr√©dire sur un dossier
   results = model.predict(
       source='datasets/coco/images/val2017/',
       conf=0.5,
       save=True,            # Sauvegarder les images annot√©es
       project='runs/detect',
       name='predictions'
   )
   
   print(f"‚úÖ Pr√©dictions sauvegard√©es dans runs/detect/predictions/")


.. slide::

üìñ 9. Entra√Æner YOLO sur votre dataset personnalis√©
-----------

Maintenant, entra√Ænons **YOLO** sur le m√™me dataset personnalis√© que vous avez cr√©√© dans les sections 5, 6 et 7 pour comparer avec ``SimpleBBoxRegressor`` !

**Rappel** : vous avez d√©j√† cr√©√© un dataset avec :

- Des images de votre objet (cube, balle, voiture, etc.)
- Annotations Label Studio au format JSON
- Un Dataset PyTorch ``LabelStudioDetectionDataset`` (section 6)
- Un split train/val/test avec ``random_split`` (section 6)


.. slide::

9.1. Exporter votre dataset au format YOLO
~~~~~~~~~~~~~~~~~~~~~

Label Studio peut exporter directement au format YOLO !

**√âtapes dans Label Studio :**

1. Ouvrez votre projet d'annotation
2. Cliquez sur **"Export"** en haut √† droite
3. Dans la liste des formats, vous verrez plusieurs options avec "YOLO". **Choisissez :**
   
   - ‚úÖ **"YOLO"** (tout court) ‚Üí format standard YOLO
   - ‚ùå Pas "YOLOv5 PyTorch" (format sp√©cifique YOLOv5)
   - ‚ùå Pas "YOLOv8 Detection" (format sp√©cifique YOLOv8)

4. Cliquez sur **"Export"** ‚Üí t√©l√©charge un fichier ZIP

.. note::

   üí° **Pourquoi "YOLO" tout court ?**
   
   Le format **"YOLO"** est le format texte standard compatible avec toutes les versions (YOLOv5, YOLOv8, YOLOv11, etc.). Les formats sp√©cifiques (YOLOv5 PyTorch, YOLOv8 Detection) sont pour des structures de projet particuli√®res.

**Contenu du ZIP :**

.. code-block:: text

   export_yolo.zip
   ‚îú‚îÄ‚îÄ classes.txt          # Liste des classes (ex: "cube")
   ‚îú‚îÄ‚îÄ notes.json           # M√©tadonn√©es (optionnel)
   ‚îî‚îÄ‚îÄ labels/              # Fichiers .txt au format YOLO
       ‚îú‚îÄ‚îÄ ad2a7904-image1.txt
       ‚îú‚îÄ‚îÄ caed06ef-image2.txt
       ‚îî‚îÄ‚îÄ ...

‚ö†Ô∏è **Probl√®me** : Les images ne sont **pas incluses** dans l'export, il faut les ajouter manuellement.


.. slide::

9.2. Nettoyer les labels YOLO
~~~~~~~~~~~~~~~~~~~~~~~~

L'export Label Studio contient les **labels** (fichiers ``.txt``) mais pas les **images**. De plus, Label Studio ajoute des **pr√©fixes UUID** aux noms de fichiers (ex: ``ad2a7904-frame_000001.txt``). Ce code nettoie les noms pour qu'ils correspondent √† vos images :

.. code-block:: python

   import shutil
   from pathlib import Path
   
   def remove_uuid_prefix(filename):
       """Enl√®ve le pr√©fixe UUID de Label Studio.
       Ex: 'ad2a7904-frame_000001.jpg' ‚Üí 'frame_000001.jpg'
       """
       if '-' in filename:
           return '-'.join(filename.split('-')[1:])  # Garde tout apr√®s le premier '-'
       return filename
   
   # Dossiers
   yolo_export = Path('export_yolo')           # ADAPTEZ : votre export d√©compress√© Label Studio 
   output_dir = Path('my_dataset_yolo')        # ADAPTEZ : nom du dossier de sortie
   
   # Cr√©er la structure
   output_dir.mkdir(exist_ok=True)
   (output_dir / 'labels').mkdir(exist_ok=True)
   
   # Copier et renommer les labels (enlever UUID)
   num_labels = 0
   for label_file in (yolo_export / 'labels').glob('*.txt'):
       clean_name = remove_uuid_prefix(label_file.name)
       shutil.copy(label_file, output_dir / 'labels' / clean_name)
       print(f"  {label_file.name} ‚Üí {clean_name}")
       num_labels += 1
   
   # Copier classes.txt
   shutil.copy(yolo_export / 'classes.txt', output_dir / 'classes.txt')
   
   print(f"\n‚úÖ {num_labels} labels nettoy√©s dans : {output_dir / 'labels'}")

üí° **Pas besoin de copier les images !** On va pointer vers le dossier existant dans le fichier YAML (√©tape suivante).

.. slide::

9.3. Organiser le dataset pour YOLO
~~~~~~~~~~~~~~~~~~~~~~~~~~~~

üéØ **Structure requise par YOLO**

YOLO a besoin d'une structure sp√©cifique :

1. Dossier ``images/`` contenant les images
2. Dossier ``labels/`` contenant les labels (m√™mes noms que les images mais en .txt)
3. Fichiers train.txt, val.txt, test.txt listant les chemins des images

.. code-block:: python

   import torch
   import shutil
   from pathlib import Path
   from torch.utils.data import random_split
   
   def create_yolo_dataset(images_dir, labels_dir, output_dir, seed=42):
       """
       Pr√©pare le dataset pour YOLO avec la structure attendue.
       
       Args:
           images_dir: Dossier source des images (ex: 'data/cube_frames')
           labels_dir: Dossier source des labels (ex: 'my_dataset_yolo/labels')
           output_dir: Dossier de sortie (ex: 'data_yolo')
           seed: Seed pour reproductibilit√© (d√©faut: 42)
       """
       images_dir = Path(images_dir)
       labels_dir = Path(labels_dir)
       output_dir = Path(output_dir)
       
       # 1. Cr√©er la structure YOLO
       images_out = output_dir / 'images'
       labels_out = output_dir / 'labels'
       images_out.mkdir(parents=True, exist_ok=True)
       labels_out.mkdir(parents=True, exist_ok=True)
       
       # 2. Copier les images et labels
       image_files = sorted(list(images_dir.glob('*.jpg')))
       print(f"üìÅ {len(image_files)} images trouv√©es")
       
       for img_file in image_files:
           # Copier l'image
           shutil.copy(img_file, images_out / img_file.name)
           
           # Copier le label correspondant
           lbl_file = labels_dir / f"{img_file.stem}.txt"
           if lbl_file.exists():
               shutil.copy(lbl_file, labels_out / lbl_file.name)
       
       print(f"‚úÖ Fichiers copi√©s dans {output_dir}")
       
       # 3. Split 70/15/15
       train_size = int(0.7 * len(image_files))
       val_size = int(0.15 * len(image_files))
       test_size = len(image_files) - train_size - val_size
       
       train_idx, val_idx, test_idx = random_split(
           range(len(image_files)),
           [train_size, val_size, test_size],
           generator=torch.Generator().manual_seed(seed)
       )
       
       # 4. √âcrire les fichiers .txt avec chemins ABSOLUS
       for indices, name in [(train_idx, 'train'), (val_idx, 'val'), (test_idx, 'test')]:
           with open(output_dir / f'{name}.txt', 'w') as f:
               for idx in indices.indices:
                   img_path = image_files[idx]
                   # Chemin absolu vers l'image dans images/
                   abs_path = (images_out / img_path.name).absolute()
                   f.write(f"{abs_path}\n")
       
       print(f"‚úÖ Split : {train_size} train, {val_size} val, {test_size} test")
       return output_dir
   
   # Utilisation :
   output_path = create_yolo_dataset(
       images_dir='data/cube_frames',
       labels_dir='my_dataset_yolo/labels',
       output_dir='data_yolo',
       seed=42
   )

.. slide::

**Structure finale** :

.. code-block:: text

   data_yolo/
   ‚îú‚îÄ‚îÄ my_dataset.yaml          # Configuration YOLO
   ‚îú‚îÄ‚îÄ train.txt                # Chemins absolus des images train
   ‚îú‚îÄ‚îÄ val.txt                  # Chemins absolus des images val
   ‚îú‚îÄ‚îÄ test.txt                 # Chemins absolus des images test
   ‚îú‚îÄ‚îÄ images/                  # Toutes les images
   ‚îÇ   ‚îú‚îÄ‚îÄ frame_000001.jpg
   ‚îÇ   ‚îú‚îÄ‚îÄ frame_000002.jpg
   ‚îÇ   ‚îî‚îÄ‚îÄ ...
   ‚îî‚îÄ‚îÄ labels/                  # Tous les labels (m√™mes noms que images/)
       ‚îú‚îÄ‚îÄ frame_000001.txt
       ‚îú‚îÄ‚îÄ frame_000002.txt
       ‚îî‚îÄ‚îÄ ...

.. note::

   üí° **Pourquoi seed=42 ?**
   
   - ‚úÖ **M√™me split** que SimpleBBoxRegressor (section 7)
   - ‚úÖ **Comparaison √©quitable** : YOLO et SimpleBBoxRegressor test√©s sur les m√™mes images

.. slide::

9.4. Cr√©er le fichier de configuration YAML
~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Cr√©ez un fichier ``my_dataset.yaml`` dans le dossier ``data_yolo/`` :

.. code-block:: yaml

   # my_dataset.yaml
   
   path: /chemin/absolu/vers/data_yolo  # ADAPTEZ : avec le chemin absolu correct
   train: train.txt
   val: val.txt
   test: test.txt
   
   nc: 1
   names: ['mon_objet']  # ADAPTEZ : avec le nom de votre classe

.. warning::

   ‚ö†Ô∏è **Important : Utiliser des chemins ABSOLUS**
   
   YOLO fonctionne mieux avec des chemins absolus. Dans le YAML, utilisez le chemin complet vers ``data_yolo/``.
   
   Les fichiers ``train.txt``, ``val.txt``, ``test.txt`` contiennent d√©j√† des chemins absolus vers les images.


.. slide::

9.5. Entra√Æner YOLO sur votre dataset
~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. code-block:: python

   from ultralytics import YOLO
   
   # Charger YOLOv11n pr√©-entra√Æn√©
   model = YOLO('yolo11n.pt')
   
   # Entra√Æner sur votre dataset
   results = model.train(
       data='data_yolo/my_dataset.yaml',  # ADAPTEZ : avec le chemin vers votre YAML
       epochs=50,                         # ADAPTEZ : en fonction du batch et de la taille de la base de donn√©es
       imgsz=224,                         # M√™me taille que SimpleBBoxRegressor
       batch=2,                           # ADAPTEZ : en fonction du nombre d'epoch et de la taille de la base de donn√©es
       name='yolo11_my_object',           # ADAPTEZ : avec le nom du dossier de sauvegarde du model
       patience=10,                       # Early stopping
       device=0,                          # GPU (ou 'cpu' si pas de GPU)
       project='runs/detect'              # ADAPTEZ : avec le chemin vers le dossier de sauvegarde du model         
   )
   
   print("‚úÖ Entra√Ænement termin√© !")


.. slide::

9.6. Tester YOLO sur le test set
~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Testons YOLO sur les **15% d'images de test** (jamais vues pendant l'entra√Ænement) :

.. code-block:: python

   from ultralytics import YOLO
   from pathlib import Path
   
   # Charger le meilleur mod√®le entra√Æn√©
   yolo_model = YOLO('runs/detect/yolo11_my_object/weights/best.pt')  # ADAPTEZ le chemin
   
   # 1. √âvaluer sur le test set
   test_metrics = yolo_model.val(
       data='data_yolo/my_dataset.yaml', # ADAPTEZ le chemin
       split='test'
   )
   
   print("üìä M√©triques YOLO sur le TEST SET :")
   print(f"  mAP@0.5     : {test_metrics.box.map50:.3f}")
   print(f"  mAP@0.5:0.95: {test_metrics.box.map:.3f}")
   print(f"  Precision   : {test_metrics.box.mp:.3f}")
   print(f"  Recall      : {test_metrics.box.mr:.3f}")
   
   # 2. Tester sur quelques images du test set
   with open('data_yolo/test.txt', 'r') as f: # ADAPTEZ le chemin
       test_images = [line.strip() for line in f.readlines()[:5]]
   
   for img_path in test_images:
       results = yolo_model.predict(
           source=img_path,
           conf=0.5, # ADAPTEZ : en fonction de la confiance du mod√®le dans ses pr√©dictions
           save=True,
           project='./predictions',  # Dossier principal  # ADAPTEZ le chemin
           name='test_results'        # Sous-dossier
       )
       print(f"‚úÖ Pr√©diction pour {Path(img_path).name}")
   
   print(f"‚úÖ Pr√©dictions sauvegard√©es dans : ./predictions/test_results/") # ADAPTEZ le chemin

.. slide::

üèãÔ∏è Travaux Pratiques 6
--------------------

.. toctree::

    TP_chap6

