.. slide::

Chapitre 6 ‚Äî D√©tection d'objets avec des bo√Ætes englobantes (partie 2)
================

üéØ Objectifs du Chapitre
----------------------

.. important::

   √Ä la fin de ce chapitre, vous saurez : 

   - Comprendre la diff√©rence entre classification et d√©tection d'objets.
   - Extraire des images depuis une vid√©o.
   - Utiliser Label Studio pour annoter des objets avec des bo√Ætes englobantes de mani√®re collaborative.
   - Comprendre et manipuler les formats d'annotations.
   - Cr√©er un dataset PyTorch pour la d√©tection d'objets.
   - Entra√Æner un d√©tecteur custom.
   - Comparer avec YOLO et choisir le bon mod√®le selon le contexte.
   - Effectuer l'inf√©rence sur des images en temps r√©el.

.. slide::

üìñ 6. CNN ultra-simple : r√©gression directe de bo√Æte
----------------------

Pour des cas simples avec **1 seul objet par image**, on peut utiliser une approche beaucoup plus simple que YOLO ou Faster R-CNN : **r√©gression directe des coordonn√©es** de la bo√Æte. Le mod√®le pr√©dit directement 4 nombres : ``(x_center, y_center, width, height)`` normalis√©s dans [0,1].

.. note::

   üí° **Quand utiliser cette approche ?**
   
   ‚úÖ **OUI** : 1 objet par image, objet centr√©, peu de variations (ex: d√©tection de visage, logo)
   
   ‚ùå **NON** : plusieurs objets, abscence de l'objet, objets qui se chevauchent, etc.

.. slide::
    
6.1. Architecture ultra-simple
~~~~~~~~~~~~~~~~~~~

Le mod√®le est constitu√© d'un **backbone CNN** (4 couches Conv2D + MaxPool) suivi d'un **head de r√©gression** (2 couches FC) qui pr√©dit directement les 4 coordonn√©es normalis√©es. Dans l'exemple, l‚Äôentr√©e $$224√ó224$$ est r√©duite **4 fois** par MaxPool(2): $$224‚Üí112‚Üí56‚Üí28‚Üí14$$; la carte de features finale est donc $$14√ó14$$. Si vous changez la taille d‚Äôentr√©e ou le nombre de couches √† stride 2, la taille de la grille changera.

.. code-block:: python

   import torch
   import torch.nn as nn
   import torch.nn.functional as F
   from tqdm import tqdm # Pour les barres de progression

   class SimpleBBoxRegressor(nn.Module):
       """
       CNN ultra-simple qui r√©gresse directement UNE bo√Æte par image.
       Sortie : [x_center, y_center, width, height] normalis√©s dans [0,1]
       """
       
       def __init__(self):
           super().__init__()
           
           # Backbone simple : Conv2D + MaxPool (comme chapitre 5)
           self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)
           self.pool1 = nn.MaxPool2d(2)  # 224 -> 112
           
           self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)
           self.pool2 = nn.MaxPool2d(2)  # 112 -> 56
           
           self.conv3 = nn.Conv2d(32, 64, kernel_size=3, padding=1)
           self.pool3 = nn.MaxPool2d(2)  # 56 -> 28
           
           self.conv4 = nn.Conv2d(64, 128, kernel_size=3, padding=1)
           self.pool4 = nn.MaxPool2d(2)  # 28 -> 14
           
           # Apr√®s 4 MaxPool: 224‚Üí112‚Üí56‚Üí28‚Üí14
           # Taille finale: [B, 128, 14, 14]
           
           # Head de r√©gression : 4 sorties (x, y, w, h)
           self.fc1 = nn.Linear(128 * 14 * 14, 128)
           self.fc2 = nn.Linear(128, 4)  # x_center, y_center, width, height
       
       def forward(self, x):
           # Backbone
           x = self.pool1(F.relu(self.conv1(x)))
           x = self.pool2(F.relu(self.conv2(x)))
           x = self.pool3(F.relu(self.conv3(x)))
           x = self.pool4(F.relu(self.conv4(x)))
           
           # Flatten
           x = x.view(x.size(0), -1)  # [B, 128*14*14]
           
           # R√©gression
           x = F.relu(self.fc1(x))
           x = torch.sigmoid(self.fc2(x))  # Sortie dans [0, 1]
           
           return x  # [B, 4] : (x_center, y_center, w, h) normalis√©s


   # Cr√©er le mod√®le
   simple_model = SimpleBBoxRegressor().to(device)
   num_params = sum(p.numel() for p in simple_model.parameters() if p.requires_grad)
   print(f"‚úÖ Mod√®le cr√©√© : {num_params:,} param√®tres")
   print(f"üìä Architecture : Conv2D (16‚Üí32‚Üí64‚Üí128) + Flatten + FC(128‚Üí4)")

.. note::

   üìä **Taille du mod√®le**

   Ce mod√®le a environ **3.3 millions** de param√®tres (principalement dans la premi√®re couche FC ``128*14*14 ‚Üí 128``). C'est bien plus petit que Faster R-CNN (``>40M``) ou YOLO qui sont plus g√©n√©riques.

.. slide::

6.2. Loss et optimiseur
~~~~~~~~~~~~~~~~~~~

**Loss MSE** pour les coordonn√©es normalis√©es (x_center, y_center, width, height) + **pr√©paration des targets**.

.. code-block:: python

   # Loss simple : MSE sur les coordonn√©es
   criterion = nn.MSELoss()
   optimizer = optim.Adam(simple_model.parameters(), lr=1e-3)
   
   # Fonction de pr√©paration des targets
   def prepare_single_box_target(targets, img_size=224):
       """
       Convertit les targets (dict) en format (x_center, y_center, w, h) normalis√©s.
       """
       batch_targets = []
       
       for target in targets:
           # Directement la premi√®re (et unique) bo√Æte
           box = target['boxes'][0]  # [x1, y1, x2, y2]
           
           x1, y1, x2, y2 = box
           x_center = ((x1 + x2) / 2) / img_size
           y_center = ((y1 + y2) / 2) / img_size
           width = (x2 - x1) / img_size
           height = (y2 - y1) / img_size
           
           # Clamp dans [0, 1]
           x_center = torch.clamp(x_center, 0, 1)
           y_center = torch.clamp(y_center, 0, 1)
           width = torch.clamp(width, 0, 1)
           height = torch.clamp(height, 0, 1)
           
           batch_targets.append(torch.tensor([x_center, y_center, width, height], device=device))
       
       return torch.stack(batch_targets)

.. note::

   üìê **Normalisation des coordonn√©es**
   
   - Entr√©e : bo√Ætes en pixels ``[x1, y1, x2, y2]`` dans ``[0, 224]``.
   - Sortie : coordonn√©es normalis√©es ``[x_c, y_c, w, h]`` dans ``[0, 1]``.
   - Le mod√®le pr√©dit directement ces 4 valeurs normalis√©es.

.. slide::

6.3. Entra√Ænement (boucles train/val)
~~~~~~~~~~~~~~~~~~~

Boucles simples d'entra√Ænement et d'√©valuation.

.. code-block:: python
   
   # Fonctions d'entra√Ænement
   def train_simple_epoch(model, criterion, optimizer, loader):
       model.train()
       total_loss = 0
       
       for images, targets in tqdm(loader, desc="Training"):
           images = torch.stack([img.to(device) for img in images])
           
           # Pr√©parer les targets
           batch_targets = prepare_single_box_target(targets)
           
           # Forward
           preds = model(images)
           loss = criterion(preds, batch_targets)
           
           # Backward
           optimizer.zero_grad()
           loss.backward()
           optimizer.step()
           
           total_loss += loss.item() * images.size(0)
       
       return total_loss / len(loader.dataset)
   
   @torch.no_grad()
   def eval_simple_epoch(model, criterion, loader):
       model.eval()
       total_loss = 0
       
       for images, targets in loader:
           images = torch.stack([img.to(device) for img in images])
           batch_targets = prepare_single_box_target(targets)
           
           preds = model(images)
           loss = criterion(preds, batch_targets)
           
           total_loss += loss.item() * images.size(0)
       
       return total_loss / len(loader.dataset)
   
.. slide::

**Lancer l'entra√Ænement** :

.. code-block:: python
   print("\nüöÄ Entra√Ænement du CNN simple...\n")
   
   num_epochs = 20
   best_val = float('inf')
   
   for epoch in range(num_epochs):
       train_loss = train_simple_epoch(simple_model, criterion, optimizer, train_loader)
       val_loss = eval_simple_epoch(simple_model, criterion, val_loader)
       
       print(f"Epoch {epoch+1:02d} | Train: {train_loss:.4f} | Val: {val_loss:.4f}")
       
       if val_loss < best_val:
           best_val = val_loss
           torch.save(simple_model.state_dict(), 'simple_bbox_regressor.pth')
           print("  ‚úÖ Meilleur mod√®le sauvegard√©!")
   
   print("\n‚úÖ Entra√Ænement termin√©!")
   print(f"üìÅ Mod√®le sauvegard√© : simple_bbox_regressor.pth")

.. note::
   
   Avec ce mod√®le simple, vous devriez voir la loss descendre rapidement (√† partir de l'epoch 5). Si la loss ne descend pas, v√©rifiez que vos donn√©es sont bien normalis√©es.

.. slide::
**Visualiser la loss** :

.. code-block:: python

   import matplotlib.pyplot as plt

   # Courbe d'apprentissage
   plt.figure(figsize=(10, 5))
   plt.plot(history['train_loss'], label='Train Loss', marker='o')
   plt.plot(history['val_loss'], label='Val Loss', marker='s')
   plt.xlabel('Epoch')
   plt.ylabel('Loss (MSE)')
   plt.title('Courbe d\'apprentissage')
   plt.legend()
   plt.grid(True, alpha=0.3)
   plt.tight_layout()
   plt.show()

   print(f"üìä Loss finale - Train: {history['train_loss'][-1]:.4f} | Val: {history['val_loss'][-1]:.4f}")

.. slide::

6.4. √âvaluation sur tout le test data
~~~~~~~~~~~~~~~~~~~

Calcul de l'**IoU moyen** (Intersection over Union) sur le test set.

.. code-block:: python

   def calculate_iou(box1, box2):
    """
    Calcule l'IoU (Intersection over Union) entre deux bo√Ætes.
    
    Args:
        box1, box2: tensors ou arrays de forme [x1, y1, x2, y2]
    
    Returns:
        iou: float entre 0 et 1
    """
    # Convertir en numpy si n√©cessaire
    if torch.is_tensor(box1):
        box1 = box1.numpy()
    if torch.is_tensor(box2):
        box2 = box2.numpy()
    
    # Calculer l'intersection
    x1_inter = max(box1[0], box2[0])
    y1_inter = max(box1[1], box2[1])
    x2_inter = min(box1[2], box2[2])
    y2_inter = min(box1[3], box2[3])
    
    # Aire de l'intersection
    inter_width = max(0, x2_inter - x1_inter)
    inter_height = max(0, y2_inter - y1_inter)
    inter_area = inter_width * inter_height
    
    # Aire de chaque bo√Æte
    box1_area = (box1[2] - box1[0]) * (box1[3] - box1[1])
    box2_area = (box2[2] - box2[0]) * (box2[3] - box2[1])
    
    # Aire de l'union
    union_area = box1_area + box2_area - inter_area
    
    # IoU
    if union_area == 0:
        return 0.0
    
    iou = inter_area / union_area
    return iou


   @torch.no_grad()
   def evaluate_all_dataset(model, dataset, img_size=224, iou_threshold=0.5):
      """
      √âvalue le mod√®le sur toutes les images du dataset.
      
      Args:
         model: mod√®le PyTorch
         dataset: dataset PyTorch
         img_size: taille des images (224x224)
         iou_threshold: seuil pour consid√©rer une d√©tection comme correcte
      
      Returns:
         dict avec les m√©triques (IoU moyen, pr√©cision, etc.)
      """
      model.eval()
      
      ious = []
      correct_detections = 0
      total_images = len(dataset)
      
      print(f"üìä √âvaluation sur {total_images} images...\n")
      
      for i in tqdm(range(total_images), desc="√âvaluation"):
         img, target = dataset[i]
         
         # Pr√©diction
         img_batch = img.unsqueeze(0).to(device)
         pred = model(img_batch)[0].cpu()
         
         # Convertir la pr√©diction en format [x1, y1, x2, y2]
         x_c, y_c, w, h = pred.numpy()
         x1_pred = (x_c - w/2) * img_size
         y1_pred = (y_c - h/2) * img_size
         x2_pred = (x_c + w/2) * img_size
         y2_pred = (y_c + h/2) * img_size
         pred_box = np.array([x1_pred, y1_pred, x2_pred, y2_pred])
         
         # Si il y a une ground truth
         if len(target['boxes']) > 0:
               gt_box = target['boxes'][0].numpy()
               
               # Calculer l'IoU
               iou = calculate_iou(gt_box, pred_box)
               ious.append(iou)
               
               # Compter comme correct si IoU > threshold
               if iou >= iou_threshold:
                  correct_detections += 1
      
      # Calculer les m√©triques
      mean_iou = np.mean(ious) if ious else 0.0
      precision = correct_detections / total_images if total_images > 0 else 0.0
      
      results = {
         'mean_iou': mean_iou,
         'precision': precision,
         'correct_detections': correct_detections,
         'total_images': total_images,
         'iou_threshold': iou_threshold,
         'all_ious': ious
      }
      
      return results


   def print_evaluation_results(results):
      """Affiche les r√©sultats d'√©valuation de mani√®re lisible."""
      print("\n" + "="*60)
      print("üìä R√âSULTATS DE L'√âVALUATION")
      print("="*60)
      print(f"\nüìà M√©triques globales :")
      print(f"   ‚Ä¢ IoU moyen            : {results['mean_iou']:.4f} ({results['mean_iou']*100:.2f}%)")
      print(f"   ‚Ä¢ Pr√©cision            : {results['precision']:.4f} ({results['precision']*100:.2f}%)")
      print(f"   ‚Ä¢ Seuil IoU            : {results['iou_threshold']}")
      print(f"\n‚úÖ D√©tections correctes  : {results['correct_detections']} / {results['total_images']}")
      print(f"‚ùå D√©tections incorrectes : {results['total_images'] - results['correct_detections']} / {results['total_images']}")
      
      # Distribution des IoU
      ious = results['all_ious']
      if ious:
         print(f"\nüìä Distribution des IoU :")
         print(f"   ‚Ä¢ Min  : {min(ious):.4f}")
         print(f"   ‚Ä¢ Max  : {max(ious):.4f}")
         print(f"   ‚Ä¢ M√©diane : {np.median(ious):.4f}")
         print(f"   ‚Ä¢ √âcart-type : {np.std(ious):.4f}")
      
      print("="*60 + "\n")


   # √âvaluer sur le test set
   print("üéØ √âvaluation compl√®te du mod√®le sur le test set\n")
   test_results = evaluate_all_dataset(simple_model, test_dataset, img_size=224, iou_threshold=0.5)
   print_evaluation_results(test_results)

.. note::

   üìà **Interpr√©tation de l'IoU**

   - IoU $$> 0.5$$ : Bonne d√©tection
   - IoU $$> 0.75$$ : Tr√®s bonne d√©tection
   - IoU $$> 0.9$$ : D√©tection quasi-parfaite

   Un mod√®le bien entra√Æn√© sur ce dataset simple devrait obtenir un IoU moyen $$> 0.8$$.

.. slide::

6.5. Visualisation
~~~~~~~~~~~~~~~~~~~

Affichage des pr√©dictions sur une grille d'images avec GT (vert) et pr√©dictions (rouge).

.. code-block:: python

   @torch.no_grad()
   def visualize_best_worst_predictions(model, dataset, results, img_size=224, num_samples=4):
      """
      Affiche les meilleures et pires pr√©dictions du mod√®le.
      
      Args:
         model: mod√®le PyTorch
         dataset: dataset PyTorch
         results: r√©sultats de l'√©valuation (dict)
         img_size: taille des images
         num_samples: nombre d'exemples √† afficher pour chaque cat√©gorie
      """
      model.eval()
      
      # Trier les images par IoU
      ious = results['all_ious']
      sorted_indices = np.argsort(ious)
      
      # Indices des meilleures et pires pr√©dictions
      best_indices = sorted_indices[-num_samples:][::-1]  # Les N meilleures
      worst_indices = sorted_indices[:num_samples]  # Les N pires
      
      # Cr√©er la figure
      fig, axes = plt.subplots(2, num_samples, figsize=(20, 10))
      
      # Afficher les meilleures pr√©dictions
      print("‚úÖ MEILLEURES PR√âDICTIONS :")
      for i, idx in enumerate(best_indices):
         img, target = dataset[idx]
         
         # Pr√©diction
         img_batch = img.unsqueeze(0).to(device)
         pred = model(img_batch)[0].cpu()
         
         # Convertir l'image pour affichage
         img_np = img.permute(1, 2, 0).numpy()
         
         ax = axes[0, i]
         ax.imshow(img_np)
         ax.axis('off')
         
         # Dessiner la GT en vert
         if len(target['boxes']) > 0:
               box_gt = target['boxes'][0].numpy()
               x1, y1, x2, y2 = box_gt
               width_gt = x2 - x1
               height_gt = y2 - y1
               
               rect_gt = patches.Rectangle(
                  (x1, y1), width_gt, height_gt,
                  linewidth=2, edgecolor='green', facecolor='none',
                  label='GT'
               )
               ax.add_patch(rect_gt)
         
         # Dessiner la pr√©diction en rouge
         x_c, y_c, w, h = pred.numpy()
         x1_pred = (x_c - w/2) * img_size
         y1_pred = (y_c - h/2) * img_size
         width_pred = w * img_size
         height_pred = h * img_size
         
         rect_pred = patches.Rectangle(
               (x1_pred, y1_pred), width_pred, height_pred,
               linewidth=2, edgecolor='red', facecolor='none',
               linestyle='--', label='Pred'
         )
         ax.add_patch(rect_pred)
         
         iou_val = ious[idx]
         ax.set_title(f'IoU: {iou_val:.3f}', fontsize=12, color='green', fontweight='bold')
         ax.legend(loc='upper right', fontsize=8)
         
         print(f"   Image {idx}: IoU = {iou_val:.4f}")
      
      # Afficher les pires pr√©dictions
      print("\n‚ùå PIRES PR√âDICTIONS :")
      for i, idx in enumerate(worst_indices):
         img, target = dataset[idx]
         
         # Pr√©diction
         img_batch = img.unsqueeze(0).to(device)
         pred = model(img_batch)[0].cpu()
         
         # Convertir l'image pour affichage
         img_np = img.permute(1, 2, 0).numpy()
         
         ax = axes[1, i]
         ax.imshow(img_np)
         ax.axis('off')
         
         # Dessiner la GT en vert
         if len(target['boxes']) > 0:
               box_gt = target['boxes'][0].numpy()
               x1, y1, x2, y2 = box_gt
               width_gt = x2 - x1
               height_gt = y2 - y1
               
               rect_gt = patches.Rectangle(
                  (x1, y1), width_gt, height_gt,
                  linewidth=2, edgecolor='green', facecolor='none',
                  label='GT'
               )
               ax.add_patch(rect_gt)
         
         # Dessiner la pr√©diction en rouge
         x_c, y_c, w, h = pred.numpy()
         x1_pred = (x_c - w/2) * img_size
         y1_pred = (y_c - h/2) * img_size
         width_pred = w * img_size
         height_pred = h * img_size
         
         rect_pred = patches.Rectangle(
               (x1_pred, y1_pred), width_pred, height_pred,
               linewidth=2, edgecolor='red', facecolor='none',
               linestyle='--', label='Pred'
         )
         ax.add_patch(rect_pred)
         
         iou_val = ious[idx]
         ax.set_title(f'IoU: {iou_val:.3f}', fontsize=12, color='red', fontweight='bold')
         ax.legend(loc='upper right', fontsize=8)
         
         print(f"   Image {idx}: IoU = {iou_val:.4f}")
      
      # Titres des lignes
      axes[0, 0].text(-50, img_size/2, '‚úÖ BEST', rotation=90, 
                        fontsize=16, fontweight='bold', color='green',
                        va='center', ha='center')
      axes[1, 0].text(-50, img_size/2, '‚ùå WORST', rotation=90, 
                        fontsize=16, fontweight='bold', color='red',
                        va='center', ha='center')
      
      plt.tight_layout()
      plt.show()


   # Visualiser les meilleures et pires pr√©dictions
   print("üéØ Visualisation des meilleures et pires pr√©dictions\n")
   visualize_best_worst_predictions(simple_model, test_dataset, test_results, num_samples=4)


.. note::

   üé® **L√©gende**
   
   - **Vert** : Ground truth (annotation r√©elle)
   - **Rouge** (pointill√©) : Pr√©diction du mod√®le
   
   Si les bo√Ætes se superposent bien, le mod√®le fonctionne correctement !

.. slide::

üìñ 7. Entra√Ænement avec YOLO sur dataset existant
----------------------

Nous allons maintenant utiliser **YOLOv11** (Ultralytics) pour entra√Æner un d√©tecteur sur un dataset standard. YOLO (You Only Look Once) est un mod√®le utilis√© pour la d√©tection d'objets rapide et efficace, parfait pour la d√©tection en temps r√©el.

7.1. Introduction √† YOLO
~~~~~~~~~~~~~~~~~~~

**YOLO** divise l'image en une **grille** (ex: $$7√ó7$$, $$13√ó13$$, etc.) et pour chaque **cellule** de la grille, pr√©dit :

- **Plusieurs bo√Ætes englobantes candidates** (typiquement 3-9 selon les versions) gr√¢ce aux **anchors**
- Chaque bo√Æte est repr√©sent√©e par : **(x, y, w, h)** relatives au centre de la cellule
- **Objectness** : probabilit√© qu'un objet soit pr√©sent dans cette bo√Æte
- **Classes** : probabilit√©s pour chaque classe (si objet d√©tect√©)

**Avantages de YOLO :**

- ‚úÖ **Rapide** : 30-80 FPS (temps r√©el)
- ‚úÖ **One-stage** : pr√©diction directe
- ‚úÖ **Pr√©cis** : performances sup√©rieures √† Faster R-CNN
- ‚úÖ **Facile √† utiliser** : librairie Ultralytics tr√®s simple

**YOLOv11** est la derni√®re version stable (2024) avec des am√©liorations significatives par rapport √† YOLOv8 (2023) : l'architecture est plus optimis√©e et la pr√©cision est am√©lior√©e tout en √©tant plus rapide. 

.. note::

   üìö **Ressources YOLO**
   
   - Documentation officielle : https://docs.ultralytics.com/
   - GitHub : https://github.com/ultralytics/ultralytics
   - Papier YOLOv11 (2024) : https://arxiv.org/abs/2410.17725
   - Papier YOLOv1 original (2015) : https://arxiv.org/abs/1506.02640

.. slide::

7.2. Concepts cl√©s : Anchors et NMS
~~~~~~~~~~~~~~~~~~~

**C'est quoi un anchor (ancre) ?**

Un anchor est une bo√Æte de r√©f√©rence pr√©d√©finie avec des proportions sp√©cifiques (largeur/hauteur).

**Exemple d'anchors :** 

- Anchor 1 : petit carr√© ($$0.2 √ó 0.2$$ de l'image) ‚Üí pour d√©tecter petits objets

- Anchor 2 : rectangle vertical ($$0.1 √ó 0.3$$) ‚Üí pour personnes debout

- Anchor 3 : rectangle horizontal ($$0.4 √ó 0.2$$) ‚Üí pour voitures

Le mod√®le **ajuste** ces anchors (d√©cale et redimensionne) pour coller aux objets r√©els. C'est plus efficace que de pr√©dire la taille depuis z√©ro !

‚û°Ô∏è **Au total** : Si grille $$13√ó13$$ avec 3 anchors par cellule = $$13√ó13√ó3$$ = **507 bo√Ætes candidates** par image !

.. slide::

**üßπ C'est quoi le NMS (Non-Maximum Suppression) ?**

Probl√®me : Plusieurs bo√Ætes d√©tectent souvent le **m√™me objet** (ex: 5 bo√Ætes qui se chevauchent sur une voiture).

**NMS √©limine les doublons en 3 √©tapes :**

1. **Trier** les bo√Ætes par score de confiance (objectness) d√©croissant

2. **Garder** la bo√Æte avec le meilleur score

3. **Supprimer** toutes les bo√Ætes qui se chevauchent trop (IoU > seuil, ex: 0.5) avec la bo√Æte gard√©e

4. R√©p√©ter pour les bo√Ætes restantes

**Exemple :**

- Avant NMS : 507 bo√Ætes candidates

- Apr√®s NMS : 3-10 d√©tections finales (les meilleures, sans doublons)

Le mod√®le filtre ainsi avec **NMS** pour garder les meilleures d√©tections sans redondance.

.. slide::

7.3. Installation de YOLOv11 (Ultralytics)
~~~~~~~~~~~~~~~~~~~

Installation simple via pip :

.. code-block:: python

   # Installer Ultralytics (inclut YOLOv11)
   !pip install ultralytics
   
   # Imports
   from ultralytics import YOLO
   import torch
   
   print(f"‚úÖ Ultralytics install√© !")
   print(f"üî• PyTorch version: {torch.__version__}")
   print(f"üéÆ CUDA disponible: {torch.cuda.is_available()}")

.. note::

   üí° **Versions compatibles**
   
   - Python ‚â• 3.8
   - PyTorch ‚â• 1.8
   - Ultralytics maintient automatiquement les d√©pendances

.. slide::

7.4. Dataset COCO (Common Objects in Context)
~~~~~~~~~~~~~~~~~~~

**COCO** est le dataset de r√©f√©rence pour la d√©tection d'objets :

- **80 classes** d'objets courants (personne, voiture, chien, etc.)
- **118 000 images** d'entra√Ænement (COCO complet)
- **5 000 images** de validation
- Annotations au format JSON (bo√Ætes + segmentation)

**Pour ce cours, nous utilisons COCO128**, une version r√©duite avec seulement 128 images, car :

- ‚úÖ T√©l√©chargement rapide (6.8 Mo au lieu de ~20 Go)
- ‚úÖ Entra√Ænement rapide (2-3 min au lieu de 6-10h)
- ‚úÖ Parfait pour apprendre et tester

.. note::

   üìä **Classes COCO (extrait)**
   
   0: person, 1: bicycle, 2: car, 3: motorcycle, ... 5: bus, ... 7: truck, ... 15: bird, 16: cat, 17: dog, ... 39: bottle, ... 41: cup, ... 56: chair, ...


.. slide::

7.5. Entra√Ænement YOLOv11 sur COCO128
~~~~~~~~~~~~~~~~~~~

**7.5.1. Choisir et charger le mod√®le**

YOLOv11 propose plusieurs tailles. Nous utilisons **YOLOv11n (Nano)** pour le cours car il est rapide :

.. code-block:: python
   
   # Charger YOLOv11 Nano (le plus rapide)
   model = YOLO('yolo11n.pt')
   
   print(f"‚úÖ Mod√®le YOLOv11n charg√© (3M param√®tres, 80+ FPS)")

.. note::

   üì¶ **Autres mod√®les disponibles** (pour information)
   
   - ``yolo11n.pt`` : Nano (3M params, 80+ FPS) ‚Üê **on utilise celui-ci**
   - ``yolo11s.pt`` : Small (9M params, 60 FPS)
   - ``yolo11m.pt`` : Medium (20M params, 45 FPS)
   - ``yolo11l.pt`` : Large (26M params, 35 FPS)
   - ``yolo11x.pt`` : XLarge (57M params, 30 FPS)

.. slide::

**7.5.2. T√©l√©charger COCO128**

T√©l√©chargez le dataset COCO128 via Ultralytics :

.. code-block:: python

   from ultralytics.data.utils import check_det_dataset
   
   # T√©l√©charger COCO128 (6.8 Mo, 128 images)
   print("üì• T√©l√©chargement de COCO128 (6.8 Mo)...")
   data_dict = check_det_dataset('coco128.yaml', autodownload=True)
   print(f"‚úÖ Dataset t√©l√©charg√© : {data_dict['path']}")

.. note::

   üíæ **COCO128 : 128 images, 80 classes possibles**
   
   - **128 images** : le nombre d'images dans le dataset
   - **80 classes** : les types d'objets que le mod√®le peut d√©tecter (person, car, dog, etc.)
   - **~20-30 classes pr√©sentes** : seulement ces classes apparaissent dans les 128 images
   - Dataset t√©l√©charg√© dans : ``./datasets/coco128/``

.. slide::

**7.5.3. Lancer l'entra√Ænement**

.. code-block:: python

   # Entra√Æner YOLOv11n sur COCO128
   results = model.train(
       data='coco128.yaml',        # COCO128 (128 images)
       epochs=3,                   # 3 epochs pour le cours (rapide)
       imgsz=640,                  # Taille des images
       batch=16,                   # Batch size (ajuster selon votre GPU)
       device=0,                   # GPU 0 (ou 'cpu' sans GPU)
       project='runs/detect',      # Dossier de sortie
       name='yolo11_coco128'       # Nom de l'exp√©rience
   )
   
   print(f"‚úÖ Entra√Ænement termin√© !")
   print(f"üìÅ R√©sultats : runs/detect/yolo11_coco128/")

.. note::

   ‚è±Ô∏è **Temps d'entra√Ænement**
   
   - **COCO128** (128 images, 3 epochs) : ~5-10 minutes sur GPU
   - **COCO complet** (118k images, 50 epochs) : ~5-10 heures sur GPU
   
   Pour ce cours, COCO128 suffit amplement pour comprendre le fonctionnement !

.. slide::

**7.5.4. Visualiser les r√©sultats de l'entra√Ænement**

Ultralytics g√©n√®re automatiquement plusieurs fichiers de r√©sultats dans ``runs/detect/yolo11_coco128/`` :

- **results.png** : graphiques avec toutes les courbes (loss, mAP, etc.)
- **Courbes de loss** (train/val)
- **M√©triques mAP** (mean Average Precision)
- **Exemples de pr√©dictions**

.. code-block:: python

   # Afficher les r√©sultats de l'entra√Ænement
   from IPython.display import Image, display
   
   # Afficher la courbe de loss
   results_path = 'runs/detect/yolo11_coco128/results.png'
   try:
       print(f"üìä Affichage des courbes d'entra√Ænement YOLO\n")
       display(Image(filename=results_path))
       print(f"\n‚úÖ Graphiques charg√©s depuis : {results_path}")
   except FileNotFoundError:
       print(f"‚ö†Ô∏è Fichier non trouv√© : {results_path}")
       print("   Les r√©sultats seront disponibles apr√®s l'entra√Ænement.")

.. slide::

**7.5.5. Pour aller plus loin : COCO complet (optionnel)**

Si vous voulez entra√Æner sur le dataset complet apr√®s avoir test√© avec COCO128 :

.. code-block:: python

   # T√©l√©charger COCO complet (~20 Go, peut prendre 30-60 min)
   # print("üì• T√©l√©chargement de COCO complet (~20 Go)...")
   # data_dict = check_det_dataset('coco.yaml', autodownload=True)
   
   # Entra√Æner sur COCO complet (plusieurs heures)
   # results = model.train(
   #     data='coco.yaml',         # COCO complet (118k images)
   #     epochs=50,                # 50 epochs minimum
   #     imgsz=640,
   #     batch=16,
   #     device=0,
   #     project='runs/detect',
   #     name='yolo11_coco_full'
   # )

.. slide::

7.6. √âvaluation sur le test set
~~~~~~~~~~~~~~~~~~~

.. code-block:: python

   # Charger le meilleur mod√®le
    model = YOLO('runs/detect/yolo11_coco128/weights/best.pt')
    print("‚úÖ Mod√®le charg√© !")
    
    # √âvaluer sur le validation set
    metrics = model.val()

    print(f"üìä mAP@0.5: {metrics.box.map50:.3f}")
    print(f"üìä mAP@0.5:0.95: {metrics.box.map:.3f}")
    print(f"üìä Precision: {metrics.box.mp:.3f}")
    print(f"üìä Recall: {metrics.box.mr:.3f}")

.. note::

   üìà **M√©triques COCO**
   
   - **mAP@0.5** : Pr√©cision moyenne avec seuil IoU=0.5
   - **mAP@0.5:0.95** : Pr√©cision moyenne sur plusieurs seuils (standard COCO)
   - **Objectif** : mAP@0.5:0.95 > 0.40 pour un bon mod√®le


.. slide::

7.7. Inf√©rence et visualisation
~~~~~~~~~~~~~~~~~~~

Une fois le mod√®le entra√Æn√©, vous pouvez l'utiliser pour d√©tecter des objets dans de nouvelles images.

**√âtape 1 : Faire une pr√©diction sur une image**

.. code-block:: python

   # Pr√©diction sur une image
   results = model.predict(
       source='path/to/image.jpg',  # Chemin vers votre image, dossier, vid√©o, ou URL.
       conf=0.5,                    # Seuil de confiance minimum. Le mod√®le ne garde que les d√©tections avec une confiance $$‚â• 50%$$.
       iou=0.45,                    # Seuil NMS (√©limination des doublons). √âlimine les bo√Ætes qui se chevauchent trop (IoU $$ > 45%$$) pour √©viter les doublons.
       show=False,                  # Ne pas afficher automatiquement
       save=False                   # Ne pas sauvegarder automatiquement
   )

.. slide::

**√âtape 2 : Extraire les r√©sultats**

.. code-block:: python

   # R√©cup√©rer les r√©sultats de la premi√®re image
   result = results[0]
   
   # Extraire les informations des d√©tections
   boxes = result.boxes.xyxy.cpu().numpy()    # Coordonn√©es [x1, y1, x2, y2] en pixels
   confs = result.boxes.conf.cpu().numpy()    # Confiances [0-1]
   classes = result.boxes.cls.cpu().numpy()   # IDs des classes d√©tect√©es
   
   print(f"üéØ {len(boxes)} objets d√©tect√©s !")
   
   # Afficher les d√©tails de chaque d√©tection
   for i, (box, conf, cls) in enumerate(zip(boxes, confs, classes)):
       x1, y1, x2, y2 = box
       class_name = model.names[int(cls)]  # Nom de la classe
       print(f"  Objet {i+1}: {class_name} (confiance: {conf:.2f})")

.. slide::

**√âtape 3 : Visualiser les d√©tections**

.. code-block:: python

   from matplotlib import pyplot as plt
   
   # Ultralytics dessine automatiquement les bo√Ætes avec labels
   # La m√©thode ``result.plot()`` dessine automatiquement : les bo√Ætes englobantes avec couleurs par classe, les noms des classes et les scores de confiance.
   img_with_boxes = result.plot()  # Image numpy avec bo√Ætes dessin√©es
   
   plt.figure(figsize=(12, 8))
   plt.imshow(img_with_boxes)
   plt.axis('off')
   plt.title(f'{len(boxes)} objets d√©tect√©s')
   # L'image s'affiche automatiquement dans le notebook

.. slide::

**√âtape 4 : Visualisation sur plusieurs images**

.. code-block:: python

   # Pr√©dire sur un dossier
   results = model.predict(
       source='datasets/coco/images/val2017/',
       conf=0.5,
       save=True,            # Sauvegarder les images annot√©es
       project='runs/detect',
       name='predictions'
   )
   
   print(f"‚úÖ Pr√©dictions sauvegard√©es dans runs/detect/predictions/")


.. slide::

üìñ 8. Entra√Æner YOLO sur votre dataset personnalis√©
-----------

Maintenant, entra√Ænons **YOLO** sur le m√™me dataset personnalis√© que vous avez cr√©√© avec ``SimpleBBoxRegressor`` pour comparer les performances !

**Rappel** : vous avez d√©j√† cr√©√© un dataset avec :

- Des images de votre objet (cube, balle, voiture, etc.)
- Annotations Label Studio au format YOLO export√©es (``images/``, ``labels/``, ``classes.txt``)
- Un Dataset PyTorch ``YOLODetectionDataset``
- Un split train/val/test avec ``random_split`` (seed=42)

.. slide::

8.1. Pr√©parer le dataset pour l'entra√Ænement YOLO
~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Contrairement √† ``SimpleBBoxRegressor`` qui charge les donn√©es via PyTorch Dataset, **YOLO (Ultralytics)** utilise une structure de dossiers sp√©cifique. Nous allons organiser le dataset existant pour YOLO tout en **conservant exactement le m√™me split** que ``SimpleBBoxRegressor``.

**8.1.1. Structure requise par YOLO**

YOLO attend cette organisation :

.. code-block:: text

   data_yolo/
   ‚îú‚îÄ‚îÄ images/
   ‚îÇ   ‚îú‚îÄ‚îÄ train/           # Images d'entra√Ænement
   ‚îÇ   ‚îú‚îÄ‚îÄ val/             # Images de validation
   ‚îÇ   ‚îî‚îÄ‚îÄ test/            # Images de test
   ‚îú‚îÄ‚îÄ labels/
   ‚îÇ   ‚îú‚îÄ‚îÄ train/           # Labels d'entra√Ænement (.txt)
   ‚îÇ   ‚îú‚îÄ‚îÄ val/             # Labels de validation (.txt)
   ‚îÇ   ‚îî‚îÄ‚îÄ test/            # Labels de test (.txt)
   ‚îî‚îÄ‚îÄ dataset.yaml         # Fichier de configuration

.. slide::

**8.1.2. Script pour r√©organiser le dataset**

Cr√©ons une fonction qui r√©utilise **le m√™me split** que ``SimpleBBoxRegressor`` :

.. code-block:: python

   import torch
   import shutil
   from pathlib import Path
   from torch.utils.data import random_split

   def prepare_yolo_from_existing_dataset(
       images_dir, 
       labels_dir, 
       classes_file,
       output_dir='data_yolo',
       seed=42,
       train_ratio=0.70,
       val_ratio=0.15
   ):
       """
       R√©organise un dataset YOLO existant pour l'entra√Ænement YOLO Ultralytics.
       Utilise le M√äME split que SimpleBBoxRegressor (seed=42).
       
       Args:
           images_dir: Dossier contenant toutes les images (ex: 'dataset_yolo/images')
           labels_dir: Dossier contenant tous les labels .txt (ex: 'dataset_yolo/labels')
           classes_file: Fichier classes.txt (ex: 'dataset_yolo/classes.txt')
           output_dir: Dossier de sortie pour la structure YOLO (d√©faut: 'data_yolo')
           seed: Seed pour reproductibilit√© (d√©faut: 42, M√äME que SimpleBBoxRegressor)
           train_ratio: Proportion du train set (d√©faut: 0.70)
           val_ratio: Proportion du val set (d√©faut: 0.15)
       
       Returns:
           Path vers le dossier output_dir, liste des classes
       """
       images_dir = Path(images_dir)
       labels_dir = Path(labels_dir)
       output_dir = Path(output_dir)
       classes_file = Path(classes_file)
       
       print(f"üîÑ Pr√©paration du dataset YOLO depuis : {images_dir}")
       
       # 1. Cr√©er la structure de dossiers pour YOLO
       for split in ['train', 'val', 'test']:
           (output_dir / 'images' / split).mkdir(parents=True, exist_ok=True)
           (output_dir / 'labels' / split).mkdir(parents=True, exist_ok=True)
       
       # 2. R√©cup√©rer toutes les images
       image_files = sorted(list(images_dir.glob('*.jpg')) + 
                           list(images_dir.glob('*.jpeg')) + 
                           list(images_dir.glob('*.png')))
       
       print(f"üìÅ {len(image_files)} images trouv√©es")
       
       if len(image_files) == 0:
           print("‚ùå Aucune image trouv√©e ! V√©rifiez le chemin.")
           return None, []
       
       # 3. Cr√©er le M√äME split que SimpleBBoxRegressor
       total_size = len(image_files)
       train_size = int(train_ratio * total_size)
       val_size = int(val_ratio * total_size)
       test_size = total_size - train_size - val_size
       
       train_indices, val_indices, test_indices = random_split(
           range(len(image_files)),
           [train_size, val_size, test_size],
           generator=torch.Generator().manual_seed(seed)
       )
       
       print(f"üìä Split (seed={seed}) : {train_size} train, {val_size} val, {test_size} test")
       
       # 4. Copier les fichiers dans les bons dossiers
       splits = {
           'train': train_indices.indices,
           'val': val_indices.indices,
           'test': test_indices.indices
       }
       
       for split_name, indices in splits.items():
           print(f"\nüìÇ Pr√©paration du split '{split_name}'...")
           
           copied_count = 0
           for idx in indices:
               img_file = image_files[idx]
               label_file = labels_dir / f"{img_file.stem}.txt"
               
               # Copier l'image
               dest_img = output_dir / 'images' / split_name / img_file.name
               shutil.copy(img_file, dest_img)
               
               # Copier le label correspondant (si existe)
               if label_file.exists():
                   dest_label = output_dir / 'labels' / split_name / label_file.name
                   shutil.copy(label_file, dest_label)
                   copied_count += 1
               else:
                   print(f"   ‚ö†Ô∏è  Label manquant pour : {img_file.name}")
           
           print(f"   ‚úÖ {copied_count} images + labels copi√©s")
       
       # 5. Copier le fichier classes.txt √† la racine
       shutil.copy(classes_file, output_dir / 'classes.txt')
       
       # 6. Charger les classes pour le fichier YAML
       with open(classes_file, 'r') as f:
           classes = [line.strip() for line in f.readlines()]
       
       print(f"\nüìã Classes ({len(classes)}) : {classes}")
       
       print(f"\n‚úÖ Dataset YOLO pr√©par√© dans : {output_dir}")
       print(f"   Structure : images/{{train,val,test}} + labels/{{train,val,test}}")
       
       return output_dir, classes


   # üéØ UTILISATION
   # Adapter ces chemins selon votre export Label Studio
   output_path, classes = prepare_yolo_from_existing_dataset(
       images_dir='dataset_yolo/images',      # ADAPTEZ : Dossier des images export√©es
       labels_dir='dataset_yolo/labels',      # ADAPTEZ : Dossier des labels export√©s
       classes_file='dataset_yolo/classes.txt',  # ADAPTEZ : Fichier classes.txt
       output_dir='data_yolo',                # Dossier de sortie
       seed=42                                # M√äME seed que SimpleBBoxRegressor
   )

.. note::

   üí° **Pourquoi seed=42 ?**
   
   - ‚úÖ **M√™me split** que SimpleBBoxRegressor 
   - ‚úÖ **Comparaison √©quitable** : YOLO et SimpleBBoxRegressor test√©s sur **exactement les m√™mes images**
   - ‚úÖ Les images du test set sont identiques pour les deux mod√®les

.. slide::

8.2. Cr√©er le fichier de configuration YAML
~~~~~~~~~~~~~~~~~~~~~~~~~~~~

YOLO n√©cessite un fichier YAML d√©crivant l'organisation du dataset. Cr√©ons-le automatiquement :

.. code-block:: python

   import yaml
   from pathlib import Path

   def create_yolo_yaml(output_dir, classes, yaml_filename='dataset.yaml'):
       """
       Cr√©e le fichier YAML de configuration pour YOLO.
       
       Args:
           output_dir: Dossier racine du dataset YOLO (ex: 'data_yolo')
           classes: Liste des noms de classes (ex: ['cube', 'bouteille'])
           yaml_filename: Nom du fichier YAML (d√©faut: 'dataset.yaml')
       
       Returns:
           Path vers le fichier YAML cr√©√©
       """
       output_dir = Path(output_dir)
       yaml_path = output_dir / yaml_filename
       
       # Configuration YOLO avec chemins relatifs
       config = {
           'path': str(output_dir.absolute()),  # Chemin absolu vers la racine
           'train': 'images/train',             # Chemin relatif vers images train
           'val': 'images/val',                 # Chemin relatif vers images val
           'test': 'images/test',               # Chemin relatif vers images test
           
           'nc': len(classes),                  # Nombre de classes
           'names': classes                     # Noms des classes
       }
       
       # √âcrire le fichier YAML
       with open(yaml_path, 'w') as f:
           yaml.dump(config, f, default_flow_style=False, sort_keys=False)
       
       print(f"\n‚úÖ Fichier YAML cr√©√© : {yaml_path}")
       print(f"   Contenu :")
       print(f"      - path: {config['path']}")
       print(f"      - train: {config['train']}")
       print(f"      - val: {config['val']}")
       print(f"      - test: {config['test']}")
       print(f"      - nc: {config['nc']}")
       print(f"      - names: {config['names']}")
       
       return yaml_path


   # üéØ UTILISATION (apr√®s avoir pr√©par√© le dataset)
   if output_path and classes:
       yaml_path = create_yolo_yaml(output_path, classes)
       print(f"\nüéØ Fichier de configuration pr√™t : {yaml_path}")
   else:
       print("‚ùå Erreur : dataset non pr√©par√© correctement")

**Exemple de fichier YAML g√©n√©r√©** (``data_yolo/dataset.yaml``) :

.. code-block:: yaml

   path: /chemin/absolu/vers/data_yolo
   train: images/train
   val: images/val
   test: images/test
   nc: 1
   names:
   - cube

.. note::

   üí° **Structure du fichier YAML**
   
   - ``path`` : Chemin absolu vers la racine du dataset
   - ``train/val/test`` : Chemins **relatifs** vers les dossiers d'images
   - ``nc`` : Nombre de classes (calcul√© automatiquement)
   - ``names`` : Liste des noms de classes (depuis ``classes.txt``)

.. slide::

8.3. Entra√Æner YOLO sur votre dataset
~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Maintenant que le dataset est organis√© et le fichier YAML cr√©√©, lan√ßons l'entra√Ænement YOLO :

.. code-block:: python

   from ultralytics import YOLO
   
   # Charger YOLOv11n pr√©-entra√Æn√©
   model = YOLO('yolo11n.pt')
   
   # Entra√Æner sur votre dataset
   results = model.train(
       data=str(yaml_path),               # Chemin vers le YAML cr√©√© automatiquement
       epochs=50,                         # ADAPTEZ : en fonction du batch et de la taille de la base de donn√©es
       imgsz=224,                         # M√™me taille que SimpleBBoxRegressor
       batch=2,                           # ADAPTEZ : en fonction du nombre d'epoch et de la taille de la base de donn√©es
       name='yolo11_my_object',           # ADAPTEZ : avec le nom du dossier de sauvegarde du model
       patience=10,                       # Early stopping
       device=0,                          # GPU (ou 'cpu' si pas de GPU)
       project='runs/detect'              # ADAPTEZ : avec le chemin vers le dossier de sauvegarde du model         
   )
   
   print("‚úÖ Entra√Ænement termin√© !")


.. slide::

8.4. Tester YOLO sur le test set
~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Testons YOLO sur les **15% d'images de test** (jamais vues pendant l'entra√Ænement) - les **m√™mes images** que celles utilis√©es pour tester ``SimpleBBoxRegressor`` :

.. code-block:: python

   from ultralytics import YOLO
   from pathlib import Path
   import os
   
   # Charger le meilleur mod√®le entra√Æn√©
   yolo_model = YOLO('runs/detect/yolo11_my_object/weights/best.pt')  # ADAPTEZ le chemin
   
   # 1. √âvaluer sur le test set
   print("üéØ √âvaluation de YOLO sur le TEST SET...\n")
   test_metrics = yolo_model.val(
       data=str(yaml_path),  # Utiliser le YAML cr√©√© automatiquement
       split='test'
   )
   
   print("\nüìä M√©triques YOLO sur le TEST SET :")
   print(f"  mAP@0.5     : {test_metrics.box.map50:.3f}")
   print(f"  mAP@0.5:0.95: {test_metrics.box.map:.3f}")
   print(f"  Precision   : {test_metrics.box.mp:.3f}")
   print(f"  Recall      : {test_metrics.box.mr:.3f}")
   
   # 2. Pr√©dire sur quelques images du test set pour visualisation
   test_images_dir = output_path / 'images' / 'test'
   test_images = sorted(list(test_images_dir.glob('*.jpg')))[:5]  # Prendre 5 images
   
   print(f"\nüì∏ Pr√©diction sur {len(test_images)} images de test...")
   
   for img_path in test_images:
       results = yolo_model.predict(
           source=str(img_path),
           conf=0.5,                   # ADAPTEZ : seuil de confiance
           save=True,
           project='runs/detect',
           name='yolo_test_predictions'
       )
       print(f"   ‚úÖ {img_path.name}")
   
   print(f"\n‚úÖ Pr√©dictions sauvegard√©es dans : runs/detect/yolo_test_predictions/")

.. note::

   üìä **Interpr√©tation des m√©triques YOLO**
   
   - **mAP@0.5** : Pr√©cision moyenne avec IoU ‚â• 0.5 (m√©trique principale)
   - **mAP@0.5:0.95** : Pr√©cision moyenne sur plusieurs seuils (plus stricte)
   - **Precision** : Proportion de d√©tections correctes parmi toutes les d√©tections
   - **Recall** : Proportion d'objets r√©els d√©tect√©s

.. slide::

8.5. Comparaison SimpleBBoxRegressor vs YOLO
~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Maintenant que nous avons entra√Æn√© et test√© les deux mod√®les sur **exactement le m√™me dataset** (m√™me split avec seed=42), comparons leurs performances :

.. code-block:: python

   import pandas as pd
   import matplotlib.pyplot as plt

   def compare_models(simple_results, yolo_metrics):
       """
       Compare les performances de SimpleBBoxRegressor et YOLO.
       
       Args:
           simple_results: dict des r√©sultats de SimpleBBoxRegressor (section 7.4)
           yolo_metrics: m√©triques YOLO retourn√©es par model.val()
       """
       print("\n" + "="*70)
       print("üìä COMPARAISON SimpleBBoxRegressor vs YOLO")
       print("="*70)
       
       # Cr√©er un tableau comparatif
       comparison = pd.DataFrame({
           'Mod√®le': ['SimpleBBoxRegressor', 'YOLOv11n'],
           'IoU Moyen': [
               simple_results['mean_iou'],
               yolo_metrics.box.map50  # mAP@0.5 est comparable √† l'IoU
           ],
           'Pr√©cision': [
               simple_results['precision'],
               yolo_metrics.box.mp
           ],
           'Taille (param√®tres)': [
               '~3.3M',
               '~2.6M'
           ],
           'Vitesse (relative)': [
               'Rapide',
               'Tr√®s rapide'
           ]
       })
       
       print("\n" + comparison.to_string(index=False))
       
       # Graphique comparatif
       fig, axes = plt.subplots(1, 2, figsize=(14, 5))
       
       # Graphique 1 : IoU / mAP
       ax1 = axes[0]
       models = ['SimpleBBox\nRegressor', 'YOLOv11n']
       scores = [simple_results['mean_iou'], yolo_metrics.box.map50]
       colors = ['#3498db', '#e74c3c']
       
       bars1 = ax1.bar(models, scores, color=colors, alpha=0.7, edgecolor='black', linewidth=2)
       ax1.set_ylabel('Score', fontsize=12, fontweight='bold')
       ax1.set_title('IoU Moyen / mAP@0.5', fontsize=14, fontweight='bold')
       ax1.set_ylim(0, 1)
       ax1.grid(axis='y', alpha=0.3)
       
       # Ajouter les valeurs sur les barres
       for bar, score in zip(bars1, scores):
           height = bar.get_height()
           ax1.text(bar.get_x() + bar.get_width()/2., height,
                   f'{score:.3f}',
                   ha='center', va='bottom', fontweight='bold', fontsize=11)
       
       # Graphique 2 : Pr√©cision
       ax2 = axes[1]
       precisions = [simple_results['precision'], yolo_metrics.box.mp]
       
       bars2 = ax2.bar(models, precisions, color=colors, alpha=0.7, edgecolor='black', linewidth=2)
       ax2.set_ylabel('Score', fontsize=12, fontweight='bold')
       ax2.set_title('Pr√©cision', fontsize=14, fontweight='bold')
       ax2.set_ylim(0, 1)
       ax2.grid(axis='y', alpha=0.3)
       
       # Ajouter les valeurs sur les barres
       for bar, prec in zip(bars2, precisions):
           height = bar.get_height()
           ax2.text(bar.get_x() + bar.get_width()/2., height,
                   f'{prec:.3f}',
                   ha='center', va='bottom', fontweight='bold', fontsize=11)
       
       plt.tight_layout()
       plt.show()
       
       # Analyse
       print("\nüìà ANALYSE :")
       
       if simple_results['mean_iou'] > yolo_metrics.box.map50:
           diff = (simple_results['mean_iou'] - yolo_metrics.box.map50) * 100
           print(f"   ‚úÖ SimpleBBoxRegressor gagne en IoU (+{diff:.1f}%)")
       else:
           diff = (yolo_metrics.box.map50 - simple_results['mean_iou']) * 100
           print(f"   ‚úÖ YOLO gagne en mAP (+{diff:.1f}%)")
       
       print("\nüí° RECOMMANDATIONS :")
       print("   ‚Ä¢ SimpleBBoxRegressor : Parfait pour 1 seul objet, rapide, simple √† comprendre")
       print("   ‚Ä¢ YOLO : Meilleur pour plusieurs objets, plus robuste, plus g√©n√©rique")
       print("   ‚Ä¢ Pour ce dataset (1 objet) : les deux sont comparables !")
       
       print("="*70 + "\n")


   # üéØ UTILISATION
   # Comparer les r√©sultats (utilisez les variables des sections pr√©c√©dentes)
   compare_models(test_results, test_metrics)

.. warning::

   ‚ö†Ô∏è **Pr√©requis pour la comparaison**
   
   Assurez-vous d'avoir ex√©cut√© :
   
   1. Section 6.4 : ``test_results = evaluate_all_dataset(simple_model, test_dataset)``
   2. Section 8.4 : ``test_metrics = yolo_model.val(data=str(yaml_path), split='test')``

.. note::

   üìä **Quand utiliser quel mod√®le ?**
   
   **SimpleBBoxRegressor** :
   
   - ‚úÖ **1 seul objet** par image
   - ‚úÖ Objet **centr√©** et toujours pr√©sent
   - ‚úÖ **Apprentissage** : comprendre les bases de la r√©gression de bbox
   - ‚úÖ Dataset **simple** et contr√¥l√©
   
   **YOLO** :
   
   - ‚úÖ **Plusieurs objets** par image
   - ‚úÖ Objets **multiples** de classes diff√©rentes
   - ‚úÖ **Production** : applications r√©elles, temps r√©el
   - ‚úÖ **Robustesse** : g√®re les cas complexes (occlusions, variations)
   - ‚úÖ Dataset **r√©el** avec variabilit√©



