.. slide::
RÃ©sumÃ© des concepts clÃ©s du chapitre 2
======================================

.. slide::
ğŸ“– 1. Perceptron simple
-----------------------

- Formule : $$y = \sigma(Wx + b)$$
- Les **poids** ($$w_i$$) mesurent lâ€™importance des entrÃ©es, le **biais** ($$b$$) dÃ©place la frontiÃ¨re de dÃ©cision.
- **Mise Ã  jour des paramÃ¨tres** via la descente de gradient.
- Limite : ne rÃ©sout que les problÃ¨mes **linÃ©airement sÃ©parables** (ex. porte logique ET).
- Exemple : **XOR** nâ€™est pas linÃ©airement sÃ©parable â†’ besoin dâ€™un MLP.

.. slide::
ğŸ“– 2. Fonctions dâ€™activation
----------------------------

Introduisent la **non-linÃ©aritÃ©** et influencent la convergence :

- **SigmoÃ¯de** : sortie [0,1], utile pour la classification binaire (probabilitÃ©).    
- **Tanh** : sortie [-1,1], centrÃ©e, parfois plus stable que sigmoÃ¯de.  
- **ReLU** : rapide, efficace pour les couches cachÃ©es, Ã©vite vanishing gradient.  
- **Softmax** : pour la classification multi-classes, renvoie une distribution de probabilitÃ©s.

ğŸ‘‰ En rÃ©sumÃ© :  
- Pour une sortie **binaire**, on utilise la **sigmoÃ¯de** car elle produit une probabilitÃ© entre 0 et 1.  
- Pour une sortie **multi-classes**, on utilise la **softmax** qui renvoie une distribution de probabilitÃ©s.  
- Pour les **couches cachÃ©es**, on privilÃ©gie **ReLU** (ou Tanh) car elles favorisent une meilleure convergence.  


.. slide::
ğŸ“– 3. Epochs, Batchs, ItÃ©rations
--------------------------------

- **ItÃ©ration** = une mise Ã  jour aprÃ¨s un batch.
- **Batch** = sous-ensemble des donnÃ©es.
- **Epoch** = passage complet sur toutes les donnÃ©es.

âš ï¸ Plusieurs epochs sont nÃ©cessaires pour que le modÃ¨le **converge**.


.. slide::
4. Normalisation vs. Standardisation
~~~~~~~~~~~~~~~~~~

- **Objectif** : prÃ©parer les donnÃ©es pour que le modÃ¨le apprenne plus efficacement et converge plus rapidement en Ã©vitant qu'une variable domine les autres. 

- **Normalisation** : ramÃ¨ne les valeurs dans [0,1].
- **Standardisation** : centre autour de 0 et rÃ©duit par lâ€™Ã©cart-type. 

- **Limites de la normalisation** :
  - DÃ©pend des valeurs min et max : les valeurs extrÃªmes ou aberrantes peuvent fausser lâ€™Ã©chelle.
  - Ne centre pas les donnÃ©es : la moyenne nâ€™est pas proche de 0, ralentissant la convergence.
  - Ne standardise pas lâ€™Ã©cart-type : certaines variables peuvent dominer le gradient.

- **Avantages de la standardisation** :
  - Centrage et rÃ©duction des variables â†’ moyenne proche de 0 et Ã©cart-type proche de 1.
  - Plus robuste aux valeurs aberrantes.
  - AccÃ©lÃ¨re et stabilise la convergence du modÃ¨le.


.. slide::
ğŸ“– 5. Perceptron multi-couches (MLP)
------------------------------------

- Un **MLP** = plusieurs couches linÃ©aires + fonctions dâ€™activation.
- **Couches** :
  - EntrÃ©e (features),
  - CachÃ©es (non-linÃ©aritÃ©),
  - Sortie (prÃ©diction finale).

- Construction en PyTorch :
  - Avec ``nn.Sequential`` (rapide).
  - Avec une **classe** (plus flexible).

- Exemple : rÃ©solution du **XOR** possible avec au moins une couche cachÃ©e.

.. slide::
ğŸ“– 6. Broadcasting
------------------

- Permet Ã  PyTorch de faire des calculs avec des tenseurs de dimensions diffÃ©rentes sans passer par des boucles explicites.

.. slide::
ğŸ“– 7. Observer la loss
----------------------

- La **loss** mesure lâ€™erreur du modÃ¨le Ã  chaque Ã©tape.  
- Si la loss **diminue et se stabilise** â†’ le modÃ¨le converge.  
- Si la loss reste **Ã©levÃ©e ou diverge** â†’ le modÃ¨le nâ€™apprend pas correctement.  
- En observant le graphique de la loss, on peut choisir le **nombre dâ€™epochs suffisant** :  
  quand la courbe se stabilise, il est inutile de continuer lâ€™entraÃ®nement pour Ã©viter le surapprentissage.  
- **Early stopping** : arrÃªter automatiquement si la loss ne sâ€™amÃ©liore plus.

.. slide::
ğŸ“– 8. Inspection & Profiling
----------------------------

- **torch-summary** : affiche lâ€™architecture, les dimensions et le nombre de paramÃ¨tres.
- **torch.autograd.profiler** : mesure temps et mÃ©moire des opÃ©rations â†’ utile pour optimiser.

